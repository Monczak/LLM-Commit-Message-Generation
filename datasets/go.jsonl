{"diff_id": 7, "repo": "stackexchange/dnscontrol", "sha": "2fa4f0ece76f02b82f577f615a57e425ee921293", "time": "12.03.2017 16:02:30", "diff": "mmm a / main.go <nl> ppp b / main.go <nl>@@ -173,8 +173,9 @@ func main() { <nl> if !shouldrun { <nl> statusLbl = \"(skipping)\" <nl> } <nl> - fmt.Printf(\"----- DNS Provider: %s%s\\n\", prov, statusLbl) <nl> + fmt.Printf(\"----- DNS Provider: %s... %s\", prov, statusLbl) <nl> if !shouldrun { <nl> + fmt.Println() <nl> continue <nl> } <nl> dsp, ok := dsps[prov] <nl> @@ -183,11 +184,17 @@ func main() { <nl> } <nl> corrections, err := dsp.GetDomainCorrections(dc) <nl> if err != nil { <nl> + fmt.Println(\"ERROR\") <nl> anyErrors = true <nl> fmt.Printf(\"Error getting corrections: %s\\n\", err) <nl> continue DomainLoop <nl> } <nl> totalCorrections += len(corrections) <nl> + plural := \"s\" <nl> + if len(corrections) == 1 { <nl> + plural = \"\" <nl> + } <nl> + fmt.Printf(\"%d correction%s\\n\", len(corrections), plural) <nl> anyErrors = printOrRunCorrections(corrections, command) || anyErrors <nl> } <nl> if run := shouldRunProvider(domain.Registrar, domain); !run { <nl> ", "msg": "Printing correction count for each provider as we go."}
{"diff_id": 10, "repo": "stackexchange/dnscontrol", "sha": "ece9a7aa7dd870df8286f8f22f23805451c5d72d", "time": "16.03.2017 17:05:03", "diff": "mmm a / main.go <nl> ppp b / main.go <nl>@@ -69,13 +69,13 @@ func main() { <nl> } <nl> if flag.NArg() != 1 { <nl> - fmt.Println(\"Usage: dnscontrol [options] cmd\") <nl> - fmt.Println(\" cmd:\") <nl> - fmt.Println(\" preview: Show changed that would happen.\") <nl> - fmt.Println(\" push: Make changes for real.\") <nl> - fmt.Println(\" version: Print program version string.\") <nl> - fmt.Println(\" print: Print compiled data.\") <nl> - fmt.Println(\"\") <nl> + fmt.Println(`Usage: dnscontrol [options] cmd <nl> + cmd: <nl> + preview: Show changed that would happen. <nl> + push: Make changes for real. <nl> + version: Print program version string. <nl> + print: Print compiled data. <nl> + `) <nl> flag.PrintDefaults() <nl> return <nl> } <nl> ", "msg": "Redundant use of multiple fmt.Println commands\nChanged multiple fmt.Println(\"\") commands to one using backticks"}
{"diff_id": 13, "repo": "stackexchange/dnscontrol", "sha": "3d08eb69af97789b23c1a18d71f385fe64da5d6f", "time": "17.04.2017 15:16:29", "diff": "mmm a / providers/providers.go <nl> ppp b / providers/providers.go <nl>@@ -34,6 +34,21 @@ var registrarTypes = map[string]RegistrarInitializer{} <nl> type DspInitializer func(map[string]string, json.RawMessage) (DNSServiceProvider, error) <nl> var dspTypes = map[string]DspInitializer{} <nl> +var dspCapabilities = map[string]Capability{} <nl> + <nl> +//Capability is a bitmasked set of \"features\" that a provider supports. Only use constants from this package. <nl> +type Capability uint32 <nl> + <nl> +const ( <nl> + // CanUseAlias indicates the provider support ALIAS records (or flattened CNAMES). Up to the provider to translate them to the appropriate record type. <nl> + CanUseAlias Capability = 1 << iota <nl> + // CanUsePTR indicates the provider can handle PTR records <nl> + CanUsePTR <nl> +) <nl> + <nl> +func ProviderHasCabability(pType string, cap Capability) bool { <nl> + return dspCapabilities[pType]&cap != 0 <nl> +} <nl> //RegisterRegistrarType adds a registrar type to the registry by providing a suitable initialization function. <nl> func RegisterRegistrarType(name string, init RegistrarInitializer) { <nl> @@ -44,11 +59,16 @@ func RegisterRegistrarType(name string, init RegistrarInitializer) { <nl> } <nl> //RegisterDomainServiceProviderType adds a dsp to the registry with the given initialization function. <nl> -func RegisterDomainServiceProviderType(name string, init DspInitializer) { <nl> +func RegisterDomainServiceProviderType(name string, init DspInitializer, caps ...Capability) { <nl> if _, ok := dspTypes[name]; ok { <nl> log.Fatalf(\"Cannot register registrar type %s multiple times\", name) <nl> } <nl> + var abilities Capability <nl> + for _, c := range caps { <nl> + abilities |= c <nl> + } <nl> dspTypes[name] = init <nl> + dspCapabilities[name] = abilities <nl> } <nl> func createRegistrar(rType string, config map[string]string) (Registrar, error) { <nl> ", "msg": "simple facility for registering provider capabilities"}
{"diff_id": 46, "repo": "stackexchange/dnscontrol", "sha": "d7bdc03b54dde832230a9295122e95d1f4df6572", "time": "15.09.2017 13:36:49", "diff": "mmm a / commands/previewPush.go <nl> ppp b / commands/previewPush.go <nl>@@ -4,7 +4,6 @@ import ( <nl> \"fmt\" <nl> \"log\" <nl> \"os\" <nl> - \"time\" <nl> \"github.com/StackExchange/dnscontrol/models\" <nl> \"github.com/StackExchange/dnscontrol/pkg/nameservers\" <nl> @@ -32,18 +31,12 @@ type PreviewArgs struct { <nl> GetDNSConfigArgs <nl> GetCredentialsArgs <nl> FilterArgs <nl> - Delay int <nl> } <nl> func (args *PreviewArgs) flags() []cli.Flag { <nl> flags := args.GetDNSConfigArgs.flags() <nl> flags = append(flags, args.GetCredentialsArgs.flags()...) <nl> flags = append(flags, args.FilterArgs.flags()...) <nl> - flags = append(flags, cli.IntFlag{ <nl> - Name: \"d\", <nl> - Destination: &args.Delay, <nl> - Usage: \"delay between domains to avoid rate limits (in ms)\", <nl> - }) <nl> return flags <nl> } <nl> @@ -161,9 +154,6 @@ DomainLoop: <nl> } <nl> totalCorrections += len(corrections) <nl> anyErrors = printOrRunCorrections(corrections, out, push, interactive) || anyErrors <nl> - if args.Delay != 0 { <nl> - time.Sleep(time.Duration(args.Delay) * time.Millisecond) <nl> - } <nl> } <nl> if os.Getenv(\"TEAMCITY_VERSION\") != \"\" { <nl> fmt.Fprintf(os.Stderr, \"##teamcity[buildStatus status='SUCCESS' text='%d corrections']\", totalCorrections) <nl> ", "msg": "remove delay flag. providers should throttle themselves"}
{"diff_id": 65, "repo": "stackexchange/dnscontrol", "sha": "b0f86bc007f56eed7e4ab36f851c5dd5de7cbc35", "time": "25.07.2018 12:59:04", "diff": "mmm a / providers/providers.go <nl> ppp b / providers/providers.go <nl>@@ -8,18 +8,18 @@ import ( <nl> \"github.com/pkg/errors\" <nl> ) <nl> -// Registrar is an interface for a domain registrar. It can return a list of needed corrections to be applied in the future. <nl> +// Registrar is an interface for a domain registrar. It can return a list of needed corrections to be applied in the future. Implement this only if the provider is a \"registrar\" (i.e. can update the NS records of the parent to a domain). <nl> type Registrar interface { <nl> models.Registrar <nl> } <nl> -// DNSServiceProvider is able to generate a set of corrections that need to be made to correct records for a domain <nl> +// DNSServiceProvider is able to generate a set of corrections that need to be made to correct records for a domain. Implement this only if the provider is a DNS Service Provider (can update records in a DNS zone). <nl> type DNSServiceProvider interface { <nl> models.DNSProvider <nl> } <nl> // DomainCreator should be implemented by providers that have the ability to add domains to an account. the create-domains command <nl> -// can be run to ensure all domains are present before running preview/push <nl> +// can be run to ensure all domains are present before running preview/push. Implement this only if the provider supoprts the `dnscontrol create-domain` command. <nl> type DomainCreator interface { <nl> EnsureDomainExists(domain string) error <nl> } <nl> ", "msg": "Documentation: Clarify the when to implement certain interfaces"}
{"diff_id": 77, "repo": "stackexchange/dnscontrol", "sha": "be5b588035b47cddb23ca4095ab70ca39b212347", "time": "29.05.2019 14:03:36", "diff": "mmm a / pkg/acme/acme.go <nl> ppp b / pkg/acme/acme.go <nl>@@ -23,6 +23,7 @@ type CertConfig struct { <nl> CertName string `json:\"cert_name\"` <nl> Names []string `json:\"names\"` <nl> UseECC bool `json:\"use_ecc\"` <nl> + MustStaple bool `json:\"must_staple\"` <nl> } <nl> type Client interface { <nl> @@ -103,7 +104,7 @@ func (c *certManager) IssueOrRenewCert(cfg *CertConfig, renewUnder int, verbose <nl> var client *acme.Client <nl> var action = func() (*acme.CertificateResource, error) { <nl> - return client.ObtainCertificate(cfg.Names, true, nil, true) <nl> + return client.ObtainCertificate(cfg.Names, true, nil, cfg.MustStaple) <nl> } <nl> if existing == nil { <nl> @@ -125,7 +126,7 @@ func (c *certManager) IssueOrRenewCert(cfg *CertConfig, renewUnder int, verbose <nl> } else { <nl> log.Println(\"Renewing cert\") <nl> action = func() (*acme.CertificateResource, error) { <nl> - return client.RenewCertificate(*existing, true, true) <nl> + return client.RenewCertificate(*existing, true, cfg.MustStaple) <nl> } <nl> } <nl> } <nl> ", "msg": "add must_staple option to cert. Default false"}
{"diff_id": 94, "repo": "stackexchange/dnscontrol", "sha": "05cedab5a7bdc44d9549856fe16d0ed2628cd817", "time": "18.02.2020 15:24:04", "diff": "mmm a / integrationTest/integration_test.go <nl> ppp b / integrationTest/integration_test.go <nl>@@ -565,12 +565,18 @@ func makeTests(t *testing.T) []*TestCase { <nl> tc(\"Create a 255-byte TXT\", txt(\"foo\", \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")), <nl> ) <nl> - // TXT (empty) <nl> - if *providerToRun != \"CLOUDFLAREAPI\" { <nl> - tests = append(tests, tc(\"Empty\"), <nl> - tc(\"TXT with empty str\", txt(\"foo1\", \"\")), <nl> - ) <nl> - } <nl> + // FUTURE(tal): https://github.com/StackExchange/dnscontrol/issues/598 <nl> + // We decided that handling an empty TXT string is not a <nl> + // requirement. In the future we might make it a \"capability\" to <nl> + // indicate which vendors fully support RFC 1035, which requires <nl> + // that a TXT string can be empty. <nl> + // <nl> + // // TXT (empty) <nl> + // if (provider supports empty txt strings) { <nl> + // tests = append(tests, tc(\"Empty\"), <nl> + // tc(\"TXT with empty str\", txt(\"foo1\", \"\")), <nl> + // ) <nl> + // } <nl> // TXTMulti <nl> if !providers.ProviderHasCapability(*providerToRun, providers.CanUseTXTMulti) { <nl> ", "msg": "Remove test for empty TXT strings."}
{"diff_id": 112, "repo": "stackexchange/dnscontrol", "sha": "42f7568074698956f70b0bef0feff6b7516fdb53", "time": "28.07.2020 20:35:56", "diff": "mmm a / providers/azuredns/azureDnsProvider.go <nl> ppp b / providers/azuredns/azureDnsProvider.go <nl>@@ -518,16 +518,19 @@ func (a *azureDNSProvider) fetchRecordSets(zoneName string) ([]*adns.RecordSet, <nl> var records []*adns.RecordSet <nl> ctx, cancel := context.WithTimeout(context.Background(), 6000*time.Second) <nl> defer cancel() <nl> - recordsIterator, recordsErr := a.recordsClient.ListAllByDNSZoneComplete(ctx, *a.resourceGroup, zoneName, to.Int32Ptr(1000), \"\") <nl> + recordsIterator, recordsErr := a.recordsClient.ListAllByDNSZone(ctx, *a.resourceGroup, zoneName, to.Int32Ptr(1000), \"\") <nl> if recordsErr != nil { <nl> return nil, recordsErr <nl> } <nl> - recordsResult := recordsIterator.Response() <nl> + for recordsIterator.NotDone() { <nl> + recordsResult := recordsIterator.Response() <nl> for _, r := range *recordsResult.Value { <nl> record := r <nl> records = append(records, &record) <nl> } <nl> + recordsIterator.NextWithContext(ctx) <nl> + } <nl> return records, nil <nl> } <nl> ", "msg": "Fix for incomplete results from Azure DNS in fetchRecordSets"}
{"diff_id": 115, "repo": "stackexchange/dnscontrol", "sha": "690f49e0419845f8a03b610a755e126a95d3205b", "time": "11.08.2020 00:36:33", "diff": "mmm a / integrationTest/integration_test.go <nl> ppp b / integrationTest/integration_test.go <nl>@@ -786,7 +786,7 @@ func makeTests(t *testing.T) []*TestGroup { <nl> tc(\"Change Weight\", srv(\"_sip._tcp\", 52, 62, 7, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \"foo4.com.\")), <nl> tc(\"Change Port\", srv(\"_sip._tcp\", 52, 62, 72, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \"foo4.com.\")), <nl> ), <nl> - testgroup(\"SRV w/ null target\", not(\"EXOSCALE\", \"HEXONET\", \"NAMEDOTCOM\"), <nl> + testgroup(\"SRV w/ null target\", requires(providers.CanUseSRV), not(\"EXOSCALE\", \"HEXONET\", \"NAMEDOTCOM\"), <nl> tc(\"Null Target\", srv(\"_sip._tcp\", 52, 62, 72, \"foo.com.\"), srv(\"_sip._tcp\", 15, 65, 75, \".\")), <nl> ), <nl> ", "msg": "fix: only run SRV null target test when supported"}
{"diff_id": 131, "repo": "stackexchange/dnscontrol", "sha": "ab47e92cbf261f4cff901e7e37bc99c22280a927", "time": "02.11.2020 01:34:10", "diff": "mmm a / providers/hetzner/api.go <nl> ppp b / providers/hetzner/api.go <nl>@@ -157,6 +157,7 @@ func (api *hetznerProvider) getZone(name string) (*zone, error) { <nl> } <nl> func (api *hetznerProvider) request(endpoint string, method string, request interface{}, target interface{}) error { <nl> + for { <nl> var requestBody io.Reader <nl> if request != nil { <nl> requestBodySerialised, err := json.Marshal(request) <nl> @@ -171,7 +172,6 @@ func (api *hetznerProvider) request(endpoint string, method string, request inte <nl> } <nl> req.Header.Add(\"Auth-API-Token\", api.apiKey) <nl> - for { <nl> api.requestRateLimiter.beforeRequest() <nl> resp, err := http.DefaultClient.Do(req) <nl> api.requestRateLimiter.afterRequest() <nl> ", "msg": "HETZNER: fix retry of POST/PUT requests -- rebuild request body\nPreviously for any retry the request body was already consumed and\nthe server received an empty body."}
{"diff_id": 151, "repo": "stackexchange/dnscontrol", "sha": "fdd6387aad127eb451c5112f797a0cc962039182", "time": "05.06.2021 05:30:17", "diff": "mmm a / providers/dnsmadeeasy/api.go <nl> ppp b / providers/dnsmadeeasy/api.go <nl>@@ -12,13 +12,13 @@ type dnsMadeEasyProvider struct { <nl> } <nl> func newProvider(apiKey string, secretKey string, sandbox bool, debug bool) *dnsMadeEasyProvider { <nl> - fmt.Println(\"creating DNSMADEEASY provider for sandbox\") <nl> - <nl> baseURL := baseURLV2_0 <nl> if sandbox { <nl> baseURL = sandboxBaseURLV2_0 <nl> } <nl> + fmt.Printf(\"Creating DNSMADEEASY provider for %q\\n\", baseURL) <nl> + <nl> return &dnsMadeEasyProvider{ <nl> restAPI: &dnsMadeEasyRestAPI{ <nl> apiKey: apiKey, <nl> ", "msg": "Show actual URL to use instead of just \"sandbox\"\n* Show actual URL to use instead of just \"sandbox\"\nInstead of stating \"sandbox\", sometimes incorrectly if sandbox is false, just output the actual URL that will be used.\n* Replace Println with Printf"}
{"diff_id": 162, "repo": "stackexchange/dnscontrol", "sha": "45e8622c0a1ad8ac659d842382f94d7c592ab6ed", "time": "17.02.2022 23:16:15", "diff": "mmm a / providers/powerdns/powerdnsProvider.go <nl> ppp b / providers/powerdns/powerdnsProvider.go <nl>@@ -18,6 +18,7 @@ import ( <nl> var features = providers.DocumentationNotes{ <nl> providers.CanUseAlias: providers.Can(\"Needs to be enabled in PowerDNS first\", \"https://doc.powerdns.com/authoritative/guides/alias.html\"), <nl> providers.CanUseCAA: providers.Can(), <nl> + providers.CanUseDS: providers.Can(), <nl> providers.CanUsePTR: providers.Can(), <nl> providers.CanUseSRV: providers.Can(), <nl> providers.CanUseTLSA: providers.Can(), <nl> @@ -263,6 +264,8 @@ func toRecordConfig(domain string, r zones.Record, ttl int, name string, rtype s <nl> return rc, rc.SetTarget(dnsutil.AddOrigin(content, domain)) <nl> case \"CAA\": <nl> return rc, rc.SetTargetCAAString(content) <nl> + case \"DS\": <nl> + return rc, rc.SetTargetDSString(content) <nl> case \"MX\": <nl> return rc, rc.SetTargetMXString(content) <nl> case \"SRV\": <nl> ", "msg": "POWERDNS: Add support for DS records"}
{"diff_id": 170, "repo": "stackexchange/dnscontrol", "sha": "6e802f22579db482e0acca9461fb8898cd2c38d4", "time": "27.04.2022 22:05:08", "diff": "mmm a / pkg/normalize/validate.go <nl> ppp b / pkg/normalize/validate.go <nl>@@ -573,7 +573,7 @@ func checkLabelHasMultipleTTLs(records []*models.RecordConfig) (errs []error) { <nl> for _, r := range records { <nl> label := fmt.Sprintf(\"%s %s\", r.GetLabelFQDN(), r.Type) <nl> - // if we have more records for a given label, append their TTLs here <nl> + // collect the TTLs at this label. <nl> m[label] = append(m[label], r.TTL) <nl> } <nl> ", "msg": "Emit warning in case of label having multiple TTLs\nAn RRSet (=label) consisting of multiple records with different TTLs is\nsomething not supported by most providers, and should be avoided.\nFurthermore it is deprecated in rfc2181#section-5.2\nEmit a warning for now during validation, eventually turning it into a full-blown error.\nFixes"}
{"diff_id": 221, "repo": "stackexchange/dnscontrol", "sha": "8f09e3b03f4d88a010b861962be0010aea0a2c4f", "time": "24.01.2023 00:11:24", "diff": "mmm a / providers/vultr/vultrProvider.go <nl> ppp b / providers/vultr/vultrProvider.go <nl>@@ -7,6 +7,7 @@ import ( <nl> \"fmt\" <nl> \"strings\" <nl> + \"golang.org/x/net/idna\" <nl> \"golang.org/x/oauth2\" <nl> \"github.com/StackExchange/dnscontrol/v3/models\" <nl> @@ -111,6 +112,20 @@ func (api *vultrProvider) GetZoneRecords(domain string) (models.Records, error) <nl> func (api *vultrProvider) GetDomainCorrections(dc *models.DomainConfig) ([]*models.Correction, error) { <nl> dc.Punycode() <nl> + for _, rec := range dc.Records { <nl> + switch rec.Type { // #rtype_variations <nl> + case \"ALIAS\", \"MX\", \"NS\", \"CNAME\", \"PTR\", \"SRV\", \"URL\", \"URL301\", \"FRAME\", \"R53_ALIAS\", \"NS1_URLFWD\", \"AKAMAICDN\", \"CLOUDNS_WR\": <nl> + // These rtypes are hostnames, therefore need to be converted (unlike, for example, an AAAA record) <nl> + t, err := idna.ToUnicode(rec.GetTargetField()) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + rec.SetTarget(t) <nl> + default: <nl> + // Nothing to do. <nl> + } <nl> + } <nl> + <nl> curRecords, err := api.GetZoneRecords(dc.Name) <nl> if err != nil { <nl> return nil, err <nl> @@ -119,10 +134,14 @@ func (api *vultrProvider) GetDomainCorrections(dc *models.DomainConfig) ([]*mode <nl> models.PostProcessRecords(curRecords) <nl> var corrections []*models.Correction <nl> - if !diff2.EnableDiff2 || true { // Remove \"|| true\" when diff2 version arrives <nl> - <nl> + var create, delete, modify diff.Changeset <nl> + if !diff2.EnableDiff2 { <nl> differ := diff.New(dc) <nl> - _, create, delete, modify, err := differ.IncrementalDiff(curRecords) <nl> + _, create, delete, modify, err = differ.IncrementalDiff(curRecords) <nl> + } else { <nl> + differ := diff.NewCompat(dc) <nl> + _, create, delete, modify, err = differ.IncrementalDiff(curRecords) <nl> + } <nl> if err != nil { <nl> return nil, err <nl> } <nl> @@ -161,11 +180,6 @@ func (api *vultrProvider) GetDomainCorrections(dc *models.DomainConfig) ([]*mode <nl> return corrections, nil <nl> } <nl> - // Insert Future diff2 version here. <nl> - <nl> - return corrections, nil <nl> -} <nl> - <nl> // GetNameservers gets the Vultr nameservers for a domain <nl> func (api *vultrProvider) GetNameservers(domain string) ([]*models.Nameserver, error) { <nl> return models.ToNameservers(defaultNS) <nl> @@ -220,6 +234,16 @@ func toRecordConfig(domain string, r govultr.DomainRecord) (*models.RecordConfig <nl> } <nl> rc.SetLabel(r.Name, domain) <nl> + switch rtype := r.Type; rtype { <nl> + case \"ALIAS\", \"MX\", \"NS\", \"CNAME\", \"PTR\", \"SRV\", \"URL\", \"URL301\", \"FRAME\", \"R53_ALIAS\", \"NS1_URLFWD\", \"AKAMAICDN\", \"CLOUDNS_WR\": <nl> + var err error <nl> + data, err = idna.ToUnicode(data) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + default: <nl> + } <nl> + <nl> switch rtype := r.Type; rtype { <nl> case \"CNAME\", \"NS\": <nl> rc.Type = r.Type <nl> @@ -289,6 +313,9 @@ func toVultrRecord(dc *models.DomainConfig, rc *models.RecordConfig, vultrID str <nl> } <nl> switch rtype := rc.Type; rtype { // #rtype_variations <nl> case \"SRV\": <nl> + if data == \"\" { <nl> + data = \".\" <nl> + } <nl> r.Data = fmt.Sprintf(\"%v %v %s\", rc.SrvWeight, rc.SrvPort, data) <nl> case \"CAA\": <nl> r.Data = fmt.Sprintf(`%v %s \"%s\"`, rc.CaaFlag, rc.CaaTag, rc.GetTargetField()) <nl> ", "msg": "VULTR: Adopt diff2 in compatibility mode and fix handling of some integrations tests"}
{"diff_id": 234, "repo": "stackexchange/dnscontrol", "sha": "bbabf661132f8cc29769ff2d2949a1acee1101f3", "time": "01.02.2023 11:12:54", "diff": "mmm a / providers/hostingde/hostingdeProvider.go <nl> ppp b / providers/hostingde/hostingdeProvider.go <nl>@@ -159,7 +159,7 @@ func (hp *hostingdeProvider) GetDomainCorrections(dc *models.DomainConfig) ([]*m <nl> var DnsSecOptions *dnsSecOptions = nil <nl> // ensure that publishKsk is set for domains with AutoDNSSec <nl> - if existingAutoDNSSecEnabled == desiredAutoDNSSecEnabled == true { <nl> + if existingAutoDNSSecEnabled && desiredAutoDNSSecEnabled { <nl> CurrentDnsSecOptions, err := hp.getDNSSECOptions(zone.ZoneConfig.ID) <nl> if err != nil { <nl> return nil, err <nl> ", "msg": "HOSTINGDE: Fix dnssec error resulting from non-go-conformant comparison of three values"}
{"diff_id": 273, "repo": "vmware-tanzu/octant", "sha": "330baeaef79f87e2bbc38b129115b33d22bff12a", "time": "26.07.2019 07:50:19", "diff": "mmm a / internal/modules/clusteroverview/port_forward_describer.go <nl> ppp b / internal/modules/clusteroverview/port_forward_describer.go <nl>@@ -7,7 +7,6 @@ package clusteroverview <nl> import ( <nl> \"context\" <nl> - \"fmt\" <nl> \"github.com/vmware/octant/internal/describer\" <nl> \"github.com/vmware/octant/internal/portforward\" <nl> @@ -42,8 +41,8 @@ func (d *PortForwardListDescriber) Describe(ctx context.Context, prefix, namespa <nl> pfRow := component.TableRow{ <nl> \"Name\": nameLink, <nl> - \"Namespace\": component.NewText(namespace), <nl> - \"Ports\": describePortForwardPorts(pf), <nl> + \"Namespace\": component.NewText(t.Namespace), <nl> + \"Ports\": component.NewPorts(describePortForwardPorts(pf)), <nl> \"Age\": component.NewTimestamp(pf.CreatedAt), <nl> } <nl> tbl.Add(pfRow) <nl> @@ -59,17 +58,24 @@ func (d *PortForwardListDescriber) PathFilters() []describer.PathFilter { <nl> return []describer.PathFilter{*filter} <nl> } <nl> -func describePortForwardPorts(pf portforward.State) component.Component { <nl> - lst := component.NewList(\"\", nil) <nl> +func describePortForwardPorts(pf portforward.State) []component.Port { <nl> + var list []component.Port <nl> + apiVersion, kind := pf.Target.GVK.ToAPIVersionAndKind() <nl> + pfs := component.PortForwardState{} <nl> for _, p := range pf.Ports { <nl> - portStr := fmt.Sprintf(\"%d -> %d\", p.Local, p.Remote) <nl> - item := component.NewPortForwardDeleter( <nl> - portStr, <nl> - pf.ID, <nl> - component.NewPortForwardPorts(p.Local, p.Remote), <nl> - ) <nl> - lst.Add(item) <nl> + pfs.ID = pf.ID <nl> + pfs.Port = int(p.Local) <nl> + pfs.IsForwarded = true <nl> + <nl> + port := component.NewPort( <nl> + pf.Target.Namespace, <nl> + apiVersion, <nl> + kind, <nl> + pf.Target.Name, <nl> + int(p.Remote), <nl> + string(\"TCP\"), pfs) <nl> + list = append(list, *port) <nl> } <nl> - return lst <nl> + return list <nl> } <nl> ", "msg": "Use port component to describe fowarded ports"}
{"diff_id": 276, "repo": "vmware-tanzu/octant", "sha": "7805f9895476e1d2926fc6787d12441ebcf5ce34", "time": "09.08.2019 07:08:09", "diff": "mmm a / internal/kubeconfig/kubeconfig.go <nl> ppp b / internal/kubeconfig/kubeconfig.go <nl>@@ -7,8 +7,7 @@ package kubeconfig <nl> import ( <nl> \"github.com/spf13/afero\" <nl> - \"k8s.io/apimachinery/pkg/util/yaml\" <nl> - clientcmdapiv1 \"k8s.io/client-go/tools/clientcmd/api/v1\" <nl> + \"k8s.io/client-go/tools/clientcmd\" <nl> ) <nl> //go:generate mockgen -destination=./fake/mock_loader.go -package=fake github.com/vmware/octant/internal/kubeconfig Loader <nl> @@ -54,31 +53,24 @@ func NewFSLoader(options ...FSLoaderOpt) *FSLoader { <nl> // Load loads a kube config contexts from a file. <nl> func (l *FSLoader) Load(filename string) (*KubeConfig, error) { <nl> - var rawConfig *clientcmdapiv1.Config <nl> - <nl> - f, err := l.AppFS.Open(filename) <nl> + data, err := afero.ReadFile(l.AppFS, filename) <nl> if err != nil { <nl> return nil, err <nl> } <nl> - defer func() { <nl> - if cErr := f.Close(); cErr != nil && err == nil { <nl> - err = cErr <nl> - } <nl> - }() <nl> - <nl> - if err := yaml.NewYAMLToJSONDecoder(f).Decode(&rawConfig); err != nil { <nl> + config, err := clientcmd.Load(data) <nl> + if err != nil { <nl> return nil, err <nl> } <nl> var list []Context <nl> - for _, kubeContext := range rawConfig.Contexts { <nl> - list = append(list, Context{Name: kubeContext.Name}) <nl> + for name := range config.Contexts { <nl> + list = append(list, Context{Name: name}) <nl> } <nl> return &KubeConfig{ <nl> Contexts: list, <nl> - CurrentContext: rawConfig.CurrentContext, <nl> + CurrentContext: config.CurrentContext, <nl> }, nil <nl> } <nl> ", "msg": "use client-go clientcmd to parse kube config\nSwitches context finder to use client-go's tools rather than parsing\nYAML directly."}
{"diff_id": 296, "repo": "vmware-tanzu/octant", "sha": "57539f6a8f0619ce35f981bfc771b0abccbb1052", "time": "09.04.2020 15:43:11", "diff": "mmm a / internal/terminal/manager.go <nl> ppp b / internal/terminal/manager.go <nl>@@ -155,7 +155,7 @@ func (tm *manager) Create(ctx context.Context, logger log.Logger, key store.Key, <nl> err = rc.Stream(opts) <nl> if err != nil { <nl> t.SetExitMessage(fmt.Sprintf(\"%s\", err)) <nl> - logger.Errorf(\"streaming: %+v\", err) <nl> + logger.Debugf(\"streaming: %+v\", err) <nl> } <nl> t.Stop() <nl> logger.Debugf(\"stopping stream command\") <nl> ", "msg": "error logging should be debug to avoid spam"}
{"diff_id": 316, "repo": "vmware-tanzu/octant", "sha": "0ffc463d4b663eb3234cac851792920cf1eab8f5", "time": "23.11.2020 18:06:42", "diff": "mmm a / pkg/dash/dash.go <nl> ppp b / pkg/dash/dash.go <nl>@@ -124,35 +124,30 @@ func NewRunner(ctx context.Context, logger log.Logger, options Options) (*Runner <nl> } <nl> func (r *Runner) Start(ctx context.Context, logger log.Logger, options Options, startupCh, shutdownCh chan bool) error { <nl> - kubeConfigPath := ocontext.KubeConfigChFrom(ctx) <nl> go func() { <nl> if err := r.dash.Run(ctx, startupCh); err != nil { <nl> logger.Debugf(\"running dashboard service: %v\", err) <nl> } <nl> - return <nl> }() <nl> if !r.apiCreated { <nl> go func() { <nl> - if r.dash != nil { <nl> - var err error <nl> - if options.KubeConfig, err = ValidateKubeConfig(logger, options.KubeConfig, r.fs); err != nil { <nl> logger.Infof(\"waiting for kube config ...\") <nl> - options.KubeConfig = <-kubeConfigPath <nl> - } <nl> + options.KubeConfig = <-ocontext.KubeConfigChFrom(ctx) <nl> - if options.KubeConfig != \"\" { <nl> - logger.Debugf(\"Loading configuration: %v\", options.KubeConfig) <nl> - r.startAPIService(ctx, logger, options) <nl> - return <nl> - } else { <nl> + if options.KubeConfig == \"\" { <nl> logger.Errorf(\"unexpected empty kube config\") <nl> return <nl> } <nl> + logger.Debugf(\"Loading configuration: %v\", options.KubeConfig) <nl> + apiService, pluginService, err := r.initAPI(ctx, logger, options) <nl> + if err != nil { <nl> + logger.Errorf(\"cannot create api: %v\", err) <nl> } <nl> + r.dash.apiHandler = apiService <nl> + r.dash.pluginService = pluginService <nl> + r.startAPIService(ctx, logger) <nl> }() <nl> - } else { <nl> - r.startAPIService(ctx, logger, options) <nl> } <nl> <-ctx.Done() <nl> @@ -563,16 +558,7 @@ func ValidateKubeConfig(logger log.Logger, kubeConfig string, fs afero.Fs) (stri <nl> return \"\", fmt.Errorf(\"no kubeconfig found\") <nl> } <nl> -func (r *Runner) startAPIService(ctx context.Context, logger log.Logger, options Options) { <nl> - if r.apiCreated == false { <nl> - apiService, pluginService, err := r.initAPI(ctx, logger, options) <nl> - if err != nil { <nl> - logger.Errorf(\"cannot create api: %v\", err) <nl> - } <nl> - r.dash.apiHandler = apiService <nl> - r.dash.pluginService = pluginService <nl> - } <nl> - <nl> +func (r *Runner) startAPIService(ctx context.Context, logger log.Logger) { <nl> hf := octant.NewHandlerFactory( <nl> octant.BackendHandler(r.dash.apiHandler.Handler), <nl> octant.FrontendURL(viper.GetString(\"proxy-frontend\"))) <nl> ", "msg": "inline the \"loading\" branch of startAPIService\nAlso the nil check in Start seems to be redundant. Honestly that \"unexpected\nempty kubeconfig\" line could maybe be removed as I'm pretty sure it's dead code."}
{"diff_id": 338, "repo": "percona/percona-backup-mongodb", "sha": "2d946fc195095f1d461b11cde671160c3ed3f949", "time": "09.07.2018 13:11:44", "diff": "mmm a / internal/oplog/oplog_tail.go <nl> ppp b / internal/oplog/oplog_tail.go <nl>@@ -127,15 +127,27 @@ func (ot *OplogTail) tail() { <nl> } <nl> } <nl> -func BsonTimestampNow() bson.MongoTimestamp { <nl> - return bson.MongoTimestamp(bson.Now().UnixNano()) <nl> +func (ot *OplogTail) getOplogTailTimestamp(session *mgo.Session) (bson.MongoTimestamp, error) { <nl> + oplog := &mdbstructs.Oplog{} <nl> + err := session.Find(nil).Limit(1).One(oplog) <nl> + if err != nil { <nl> + return bson.MongoTimestamp(0), err <nl> + } <nl> + return oplog.Timestamp, nil <nl> } <nl> -func (ot *OplogTail) tailQuery() bson.M { <nl> +func (ot *OplogTail) tailQuery(session *mgo.Session) (bson.M, error) { <nl> + query := bson.M{\"op\": bson.M{\"$ne\": mdbstructs.OperationNoop}} <nl> if ot.lastOplogEntry != nil { <nl> - return bson.M{\"ts\": bson.M{\"$gt\": ot.lastOplogEntry.Timestamp}, \"op\": bson.M{\"$ne\": mdbstructs.OperationNoop}} <nl> + query[\"ts\"] = bson.M{\"$gt\": ot.lastOplogEntry.Timestamp} <nl> + } else { <nl> + oplogTailTs, err := ot.getOplogTailTimestamp(session) <nl> + if err != nil { <nl> + query[\"ts\"] = bson.M{\"$gte\": bson.MongoTimestamp(0)} <nl> + } <nl> + query[\"ts\"] = bson.M{\"$gte\": oplogTailTs} <nl> } <nl> - return bson.M{\"ts\": bson.M{\"$gte\": BsonTimestampNow()}, \"op\": bson.M{\"$ne\": mdbstructs.OperationNoop}} <nl> + return query, nil <nl> } <nl> func determineOplogCollectionName(session *mgo.Session) (string, error) { <nl> ", "msg": "Query the oplog to find the timestamp we should tail from (instead of using BsonTimestampNow())"}
{"diff_id": 342, "repo": "percona/percona-backup-mongodb", "sha": "69c0dc3dc2a4473e5f43b65356e76e82ed35cfc6", "time": "09.07.2018 13:48:28", "diff": "mmm a / internal/oplog/oplog_tail.go <nl> ppp b / internal/oplog/oplog_tail.go <nl>@@ -114,7 +114,13 @@ func (ot *OplogTail) tail() { <nl> } <nl> result := bson.Raw{} <nl> if iter.Next(&result) { <nl> + oplog := mdbstructs.Oplog{} <nl> + err := result.Unmarshal(&oplog) <nl> + if err != nil { <nl> + continue <nl> + } <nl> ot.dataChan <- result.Data <nl> + ot.lastOplogEntry = oplog <nl> continue <nl> } <nl> if iter.Timeout() { <nl> ", "msg": "Store last-tailed oplog entry to the tailer can resume properly"}
{"diff_id": 398, "repo": "percona/percona-backup-mongodb", "sha": "2ba9ca25e91ddfd75698eb54819d541aadcf3bed", "time": "05.06.2019 18:52:16", "diff": "mmm a / cli/pbm-agent/main.go <nl> ppp b / cli/pbm-agent/main.go <nl>@@ -141,10 +141,6 @@ func main() { <nl> } <nl> } <nl> - if di.Username == \"\" || di.Password == \"\" { <nl> - log.Fatalf(\"Not enough data for authentication. Please set correctly either `dns` or `username`/`password` options\") <nl> - } <nl> - <nl> // Test the connection to the MongoDB server before starting the agent. <nl> // We don't want to wait until backup/restore start to know there is an error with the <nl> // connection options <nl> ", "msg": "Remove login/pass checks to work with the no-auth-enabled mongo"}
{"diff_id": 401, "repo": "percona/percona-backup-mongodb", "sha": "8114b8f8471ff19cba6acbb23bd7c8e8ea649c65", "time": "06.06.2019 21:46:02", "diff": "mmm a / grpc/client/client.go <nl> ppp b / grpc/client/client.go <nl>@@ -159,7 +159,7 @@ const ( <nl> balancerStopRetries = 3 <nl> balancerStopTimeout = 30 * time.Second <nl> dbReconnectInterval = 30 * time.Second <nl> - dbPingInterval = 60 * time.Second <nl> + dbPingInterval = 1 * time.Second <nl> isdbgrid = \"isdbgrid\" <nl> typeFilesystem = \"filesystem\" <nl> typeS3 = \"s3\" <nl> @@ -216,18 +216,30 @@ func NewClient(inctx context.Context, in InputOptions) (*Client, error) { <nl> } <nl> func (c *Client) Start() error { <nl> + if err := c.dbConnect(); err != nil { <nl> + return errors.Wrap(err, \"cannot connect to the database\") <nl> + } <nl> + <nl> + go c.dbWatchdog() <nl> + <nl> + if err := c.newCoordinatorStream(); err != nil { <nl> + return errors.Wrap(err, \"cannot connect to the coordinator\") <nl> + } <nl> + <nl> + return nil <nl> +} <nl> + <nl> +func (c *Client) newCoordinatorStream() error { <nl> var err error <nl> + c.ctx, c.cancelFunc = context.WithCancel(context.Background()) <nl> + <nl> c.grpcClient = pb.NewMessagesClient(c.grpcClientConn) <nl> c.stream, err = c.grpcClient.MessagesChat(c.ctx) <nl> if err != nil { <nl> return errors.Wrap(err, \"cannot connect to the gRPC server\") <nl> } <nl> - if err := c.dbConnect(); err != nil { <nl> - return errors.Wrap(err, \"cannot connect to the database\") <nl> - } <nl> - <nl> if err := c.updateClientInfo(); err != nil { <nl> return errors.Wrap(err, \"cannot get MongoDB status information\") <nl> } <nl> @@ -238,7 +250,6 @@ func (c *Client) Start() error { <nl> // start listening server messages <nl> go c.processIncommingServerMessages() <nl> - go c.dbWatchdog() <nl> return nil <nl> } <nl> @@ -514,17 +525,27 @@ func (c *Client) dbWatchdog() { <nl> for { <nl> select { <nl> case <-time.After(dbPingInterval): <nl> - if c.mgoSession == nil { <nl> + if c.mgoSession == nil || c.mgoSession.Ping() != nil { <nl> + if c.isRunning() { <nl> + c.logger.Infof(\"Closing grpc connection with coordinator: %v\", <nl> + c.Stop(), <nl> + ) <nl> + } <nl> + <nl> c.dbReconnect() <nl> continue <nl> } <nl> - if err := c.mgoSession.Ping(); err != nil { <nl> - c.dbReconnect() <nl> + <nl> + if !c.isRunning() { <nl> + c.lock.Lock() <nl> + c.running = true <nl> + c.lock.Unlock() <nl> + c.logger.Infof(\"Open grpc connection with coordinator: %v\", <nl> + c.newCoordinatorStream(), <nl> + ) <nl> } <nl> case <-c.dbReconnectChan: <nl> c.dbReconnect() <nl> - case <-c.ctx.Done(): <nl> - return <nl> } <nl> } <nl> } <nl> ", "msg": "Unregister the agent in case of mongo loss\n... and re-register back again when mongo gets back"}
{"diff_id": 431, "repo": "percona/percona-backup-mongodb", "sha": "15ae52bdf6768b979c301cac1a296bcd3dc41063", "time": "29.11.2019 17:06:21", "diff": "mmm a / pbm/pbm.go <nl> ppp b / pbm/pbm.go <nl>@@ -119,7 +119,7 @@ func New(ctx context.Context, uri, appName string) (*PBM, error) { <nl> curi, err := url.Parse(uri) <nl> if err != nil { <nl> - return nil, errors.Wrap(err, \"parse mongo-uri\") <nl> + return nil, errors.Wrapf(err, \"parse mongo-uri '%s'\", uri) <nl> } <nl> // Preserving `replicaSet` parameter will causes an error while connecting to the ConfigServer (mismatched replicaset names) <nl> @@ -129,7 +129,7 @@ func New(ctx context.Context, uri, appName string) (*PBM, error) { <nl> curi.Host = chost[1] <nl> pbm.Conn, err = connect(ctx, curi.String(), appName) <nl> if err != nil { <nl> - return nil, errors.Wrap(err, \"create mongo connection to configsvr\") <nl> + return nil, errors.Wrapf(err, \"create mongo connection to configsvr with connection string '%s'\", curi) <nl> } <nl> return pbm, errors.Wrap(pbm.setupNewDB(), \"setup a new backups db\") <nl> ", "msg": "Add more context to connection errors"}
{"diff_id": 434, "repo": "percona/percona-backup-mongodb", "sha": "c2a5beae1c987eaf591ac1999aa42f764675fd5d", "time": "07.01.2020 19:44:21", "diff": "mmm a / cmd/pbm/backup.go <nl> ppp b / cmd/pbm/backup.go <nl>@@ -57,7 +57,10 @@ func backup(cn *pbm.PBM, bcpName, compression string) (string, error) { <nl> return \"\", err <nl> } <nl> - storeString := \"s3://\" <nl> + storeString := \"\" <nl> + switch stg.Type { <nl> + case pbm.StorageS3: <nl> + storeString = \"s3://\" <nl> if stg.S3.EndpointURL != \"\" { <nl> storeString += stg.S3.EndpointURL + \"/\" <nl> } <nl> @@ -65,6 +68,9 @@ func backup(cn *pbm.PBM, bcpName, compression string) (string, error) { <nl> if stg.S3.Prefix != \"\" { <nl> storeString += \"/\" + stg.S3.Prefix <nl> } <nl> + case pbm.StorageFilesystem: <nl> + storeString = stg.Filesystem.Path <nl> + } <nl> return storeString, nil <nl> } <nl> ", "msg": "ctl: Fix Filesystem store output for the backup command"}
{"diff_id": 437, "repo": "percona/percona-backup-mongodb", "sha": "ef4d5c4bb36325df6645381a5d0d39161de4ebd8", "time": "18.02.2020 15:22:33", "diff": "mmm a / e2e-tests/pkg/pbm/docker.go <nl> ppp b / e2e-tests/pkg/pbm/docker.go <nl>@@ -19,7 +19,7 @@ type Docker struct { <nl> } <nl> func NewDocker(ctx context.Context, host string) (*Docker, error) { <nl> - cn, err := docker.NewClient(host, \"1.40\", nil, nil) <nl> + cn, err := docker.NewClient(host, \"1.39\", nil, nil) <nl> if err != nil { <nl> return nil, errors.Wrap(err, \"docker client\") <nl> } <nl> ", "msg": "Tests: downgrade the docker-api version to fit the CI env"}
{"diff_id": 462, "repo": "percona/percona-backup-mongodb", "sha": "38e956c90abf6ecc1c67a9c76cbee54c440d2385", "time": "05.10.2020 12:35:24", "diff": "mmm a / cmd/pbm-speed-test/main.go <nl> ppp b / cmd/pbm-speed-test/main.go <nl>@@ -24,7 +24,7 @@ func main() { <nl> sampleSizeF = tCmd.Flag(\"size-gb\", \"Set data size in GB. Default 1\").Short('s').Float64() <nl> compressType = tCmd.Flag(\"compression\", \"Compression type <none>/<gzip>/<snappy>/<lz4>/<s2>/<pgzip>\"). <nl> - Default(string(pbm.CompressionTypeGZIP)). <nl> + Default(string(pbm.CompressionTypeS2)). <nl> Enum(string(pbm.CompressionTypeNone), string(pbm.CompressionTypeGZIP), <nl> string(pbm.CompressionTypeSNAPPY), string(pbm.CompressionTypeLZ4), <nl> string(pbm.CompressionTypeS2), string(pbm.CompressionTypePGZIP), <nl> ", "msg": "align default compression for speed-test with pbm"}
{"diff_id": 470, "repo": "percona/percona-backup-mongodb", "sha": "cd26b7f1e07a0d7b9a6d88f6f38a274c7c05ff59", "time": "23.12.2020 21:21:38", "diff": "mmm a / agent/pitr.go <nl> ppp b / agent/pitr.go <nl>@@ -76,10 +76,9 @@ func (a *Agent) PITR() { <nl> // wee need epoch just to log pitr err with an extra context <nl> // so not much care if we get it or not <nl> ep, _ := a.pbm.GetEpoch() <nl> - a.log.Error(string(pbm.CmdPITR), \"\", \"\", ep.TS(), \"%v\", err) <nl> + a.log.Error(string(pbm.CmdPITR), \"\", \"\", ep.TS(), \"init: %v\", err) <nl> - // penalty to the failed node to give priority to other nodes <nl> - // on the next try <nl> + // penalty to the failed node so healthy nodes would have priority on next try <nl> wait *= 2 <nl> } <nl> @@ -172,13 +171,6 @@ func (a *Agent) pitr() (err error) { <nl> } <nl> go func() { <nl> - defer func() { <nl> - err := lock.Release() <nl> - if err != nil { <nl> - l.Error(\"release lock: %v\", err) <nl> - } <nl> - }() <nl> - <nl> ctx, cancel := context.WithCancel(context.Background()) <nl> w := make(chan struct{}) <nl> @@ -187,14 +179,25 @@ func (a *Agent) pitr() (err error) { <nl> wakeup: w, <nl> }) <nl> - err := ibcp.Stream(ctx, ep, w, stg, pbm.CompressionTypeS2) <nl> - if err != nil { <nl> - switch err.(type) { <nl> + streamErr := ibcp.Stream(ctx, ep, w, stg, pbm.CompressionTypeS2) <nl> + if streamErr != nil { <nl> + switch streamErr.(type) { <nl> case pitr.ErrOpMoved: <nl> - l.Info(\"streaming oplog: %v\", err) <nl> + l.Info(\"streaming oplog: %v\", streamErr) <nl> default: <nl> - l.Error(\"streaming oplog: %v\", err) <nl> + l.Error(\"streaming oplog: %v\", streamErr) <nl> + } <nl> + } <nl> + <nl> + if err := lock.Release(); err != nil { <nl> + l.Error(\"release lock: %v\", err) <nl> } <nl> + <nl> + // Penalty to the failed node so healthy nodes would have priority on next try. <nl> + // But lock has to be released first. Otherwise, healthy nodes would wait for the lock release <nl> + // and the penalty won't have any sense. <nl> + if streamErr != nil { <nl> + time.Sleep(pitrCheckPeriod * 2) <nl> } <nl> a.unsetPitr() <nl> ", "msg": "fix: PITR error doesn't disappear even after PITR slicer switches to another node\nFixed penalty to the failed node so healthy nodes would have priority on next try."}
{"diff_id": 482, "repo": "percona/percona-backup-mongodb", "sha": "f56cfb0b675623bb0b68a3b41ff17a9cfe58b9c4", "time": "22.12.2021 13:40:57", "diff": "mmm a / pbm/storage/s3/s3.go <nl> ppp b / pbm/storage/s3/s3.go <nl>package s3 <nl> import ( <nl> + \"crypto/tls\" <nl> \"fmt\" <nl> \"io\" <nl> \"net/http\" <nl> @@ -47,6 +48,10 @@ type Conf struct { <nl> UploadPartSize int `bson:\"uploadPartSize,omitempty\" json:\"uploadPartSize,omitempty\" yaml:\"uploadPartSize,omitempty\"` <nl> MaxUploadParts int `bson:\"maxUploadParts,omitempty\" json:\"maxUploadParts,omitempty\" yaml:\"maxUploadParts,omitempty\"` <nl> StorageClass string `bson:\"storageClass,omitempty\" json:\"storageClass,omitempty\" yaml:\"storageClass,omitempty\"` <nl> + <nl> + // InsecureSkipTLSVerify disables client verification of the server's <nl> + // certificate chain and host name <nl> + InsecureSkipTLSVerify bool `bson:\"insecureSkipTLSVerify\" json:\"insecureSkipTLSVerify\" yaml:\"insecureSkipTLSVerify\"` <nl> } <nl> type AWSsse struct { <nl> @@ -236,7 +241,6 @@ func (s *S3) List(prefix, suffix string) ([]storage.FileInfo, error) { <nl> } <nl> return true <nl> }) <nl> - <nl> if err != nil { <nl> return nil, errors.Wrap(err, \"get backup list\") <nl> } <nl> @@ -334,7 +338,6 @@ func (pr *partReader) writeNext(w io.Writer) (n int64, err error) { <nl> Key: aws.String(path.Join(pr.opts.Prefix, pr.fname)), <nl> Range: aws.String(fmt.Sprintf(\"bytes=%d-%d\", pr.n, pr.n+downloadChuckSize-1)), <nl> }) <nl> - <nl> if err != nil { <nl> // if object size is undefined, we would read <nl> // until HTTP code 416 (Requested Range Not Satisfiable) <nl> @@ -465,7 +468,6 @@ func (s *S3) Delete(name string) error { <nl> Bucket: aws.String(s.opts.Bucket), <nl> Key: aws.String(path.Join(s.opts.Prefix, name)), <nl> }) <nl> - <nl> if err != nil { <nl> if aerr, ok := err.(awserr.Error); ok { <nl> switch aerr.Code() { <nl> @@ -506,10 +508,20 @@ func (s *S3) session() (*session.Session, error) { <nl> Client: ec2metadata.New(session.New()), <nl> }) <nl> + httpClient := http.DefaultClient <nl> + if s.opts.InsecureSkipTLSVerify { <nl> + httpClient = &http.Client{ <nl> + Transport: &http.Transport{ <nl> + TLSClientConfig: &tls.Config{InsecureSkipVerify: true}, <nl> + }, <nl> + } <nl> + } <nl> + <nl> return session.NewSession(&aws.Config{ <nl> Region: aws.String(s.opts.Region), <nl> Endpoint: aws.String(s.opts.EndpointURL), <nl> Credentials: credentials.NewChainCredentials(providers), <nl> S3ForcePathStyle: aws.Bool(true), <nl> + HTTPClient: httpClient, <nl> }) <nl> } <nl> ", "msg": "option to skip TLS verification for object storage"}
{"diff_id": 502, "repo": "percona/percona-backup-mongodb", "sha": "604444d91b63e70aadd420d2eecce161da6830f9", "time": "07.09.2022 18:08:16", "diff": "mmm a / cli/backup.go <nl> ppp b / cli/backup.go <nl>@@ -219,6 +219,7 @@ type bcpReplDesc struct { <nl> LastWriteTS string `json:\"last_write_ts\" yaml:\"last_write_ts\"` <nl> LastTransitionTS string `json:\"last_transition_ts\" yaml:\"last_transition_ts\"` <nl> IsConfigSvr *bool `json:\"configsvr,omitempty\" yaml:\"configsvr,omitempty\"` <nl> + SecurityOpts *pbm.MongodOptsSec `json:\"security,omitempty\" yaml:\"security,omitempty\"` <nl> Error *string `json:\"error,omitempty\" yaml:\"error,omitempty\"` <nl> } <nl> @@ -276,6 +277,9 @@ func describeBackup(cn *pbm.PBM, b *descBcp) (fmt.Stringer, error) { <nl> if r.Error != \"\" { <nl> rv.Replsets[i].Error = &r.Error <nl> } <nl> + if r.MongodOpts.Security != nil { <nl> + rv.Replsets[i].SecurityOpts = r.MongodOpts.Security <nl> + } <nl> } <nl> return rv, err <nl> ", "msg": "show security opts in descibe-backup\nIf there were any security (data-at-rest enc) options ob the backup node,\nthese will be displayed the describe-backup output"}
{"diff_id": 503, "repo": "percona/percona-backup-mongodb", "sha": "7746e6f5e1c3e68710b9c1bb4731c444d3eb6c79", "time": "08.09.2022 16:22:56", "diff": "mmm a / cli/restore.go <nl> ppp b / cli/restore.go <nl>@@ -162,7 +162,7 @@ func waitRestore(cn *pbm.PBM, m *pbm.RestoreMeta) error { <nl> } <nl> switch rmeta.Status { <nl> - case pbm.StatusDone: <nl> + case pbm.StatusDone, pbm.StatusPartlyDone: <nl> return nil <nl> case pbm.StatusError: <nl> return errRestoreFailed{fmt.Sprintf(\"operation failed with: %s\", rmeta.Error)} <nl> ", "msg": "phys restore wait for partlyDone\nFor physical restore any of the `done` or `partlyDone` statuses is a success."}
{"diff_id": 513, "repo": "percona/percona-backup-mongodb", "sha": "282a215eea61ab02e58b68c812a4e4cd9cc11a76", "time": "06.12.2022 16:45:15", "diff": "mmm a / e2e-tests/pkg/pbm/pbm_ctl.go <nl> ppp b / e2e-tests/pkg/pbm/pbm_ctl.go <nl>@@ -223,7 +223,7 @@ func (c *Ctl) CheckRestore(bcpName string, waitFor time.Duration) error { <nl> case pbm.StatusDone: <nl> return nil <nl> case pbm.StatusError: <nl> - errors.Errorf(\"failed with %s\", r.Error) <nl> + return errors.Errorf(\"failed with %s\", r.Error) <nl> } <nl> } <nl> } <nl> ", "msg": "Fix CheckRestore function in e2e-tests\nAdd missing return statement if restore error has been found.\nWithout a return statement restore is not recognized as a failure until\nit times out (by default 25 minutes)."}
{"diff_id": 515, "repo": "percona/percona-backup-mongodb", "sha": "b3f725a8d1203e7534b0e9ba97c1f5d674a30a51", "time": "12.12.2022 15:15:53", "diff": "mmm a / pbm/restore/physical.go <nl> ppp b / pbm/restore/physical.go <nl>@@ -1032,7 +1032,7 @@ func (r *PhysRestore) startMongo(opts ...string) error { <nl> const hbFrameSec = 60 * 2 <nl> func (r *PhysRestore) init(name string, opid pbm.OPID, l *log.Event) (err error) { <nl> - r.stg, err = r.cn.GetStorage(r.log) <nl> + r.stg, err = r.cn.GetStorage(l) <nl> if err != nil { <nl> return errors.Wrap(err, \"get storage\") <nl> } <nl> ", "msg": "fix panic during phys restore S3 retries\nPhysical restore wasn't properly initialising the logger for storage.\nThat caused panics when S3 storage tried to log some actions."}
{"diff_id": 518, "repo": "percona/percona-backup-mongodb", "sha": "02c6cf5646782bed8d38ed6e95e796dcacc1f49c", "time": "21.12.2022 17:32:34", "diff": "mmm a / agent/snapshot.go <nl> ppp b / agent/snapshot.go <nl>@@ -126,11 +126,9 @@ func (a *Agent) Backup(cmd *pbm.BackupCmd, opid pbm.OPID, ep pbm.Epoch) { <nl> // try backup anyway <nl> l.Warning(\"define source backup: %v\", err) <nl> } else { <nl> + c = make(map[string]float64) <nl> for _, rs := range src.Replsets { <nl> - if rs.Name == nodeInfo.SetName { <nl> - c = map[string]float64{rs.Node: srcHostMultiplier} <nl> - break <nl> - } <nl> + c[rs.Node] = srcHostMultiplier <nl> } <nl> } <nl> } <nl> ", "msg": "fix incr backups voting for all rs\nApply upvote for all replica sets, not primary (config server) only"}
{"diff_id": 520, "repo": "percona/percona-backup-mongodb", "sha": "4ce130008c8ce4693333d51215b2cc511ce3f710", "time": "22.12.2022 12:53:32", "diff": "mmm a / cli/backup.go <nl> ppp b / cli/backup.go <nl>@@ -282,16 +282,20 @@ func describeBackup(cn *pbm.PBM, b *descBcp) (fmt.Stringer, error) { <nl> } <nl> if bcp.Size == 0 { <nl> + switch bcp.Status { <nl> + case pbm.StatusDone, pbm.StatusCancelled, pbm.StatusError: <nl> stg, err := cn.GetStorage(cn.Logger().NewEvent(\"\", \"\", \"\", primitive.Timestamp{})) <nl> if err != nil { <nl> return nil, errors.WithMessage(err, \"get storage\") <nl> } <nl> rv.Size, err = getLegacySnapshotSize(bcp, stg) <nl> - if err != nil { <nl> + if errors.Is(err, errMissedFile) && bcp.Status != pbm.StatusDone { <nl> + // canceled/failed backup can be incomplete. ignore <nl> return nil, errors.WithMessage(err, \"get snapshot size\") <nl> } <nl> } <nl> + } <nl> rv.Replsets = make([]bcpReplDesc, len(bcp.Replsets)) <nl> for i, r := range bcp.Replsets { <nl> ", "msg": "describe-backup: ignore missed file for running backup"}
{"diff_id": 524, "repo": "percona/percona-backup-mongodb", "sha": "85601830a0686a96e487360a7f29822a6072c4f5", "time": "03.02.2023 12:11:18", "diff": "mmm a / pbm/backup/physical.go <nl> ppp b / pbm/backup/physical.go <nl>@@ -181,6 +181,21 @@ func (b *Backup) doPhysical(ctx context.Context, bcp *pbm.BackupCmd, opid pbm.OP <nl> } <nl> } <nl> currOpts = append(currOpts, bson.E{\"srcBackupName\", pbm.BackupCursorName(src.Name)}) <nl> + } else { <nl> + // We don't need any previous incremental backup history if <nl> + // this is a base backup. So we can flush it to free up resources. <nl> + l.Debug(\"flush incremental backup history\") <nl> + cr, err := NewBackupCursor(b.node, l, bson.D{ <nl> + {\"disableIncrementalBackup\", true}, <nl> + }).create(ctx, cursorCreateRetries) <nl> + if err != nil { <nl> + l.Warning(\"flush incremental backup history error: %v\", err) <nl> + } else { <nl> + err = cr.Close(ctx) <nl> + if err != nil { <nl> + l.Warning(\"close cursor disableIncrementalBackup: %v\", err) <nl> + } <nl> + } <nl> } <nl> } <nl> cursor := NewBackupCursor(b.node, l, currOpts) <nl> ", "msg": "flush incremental backup history\nWe don't need any previous incremental backup history when we start a\nbase backup. So we can flush it to free up resources."}
{"diff_id": 538, "repo": "nats-io/natscli", "sha": "ba61a27afe59c5df1c38556e7152c5deb9f12a08", "time": "05.05.2020 11:12:24", "diff": "mmm a / nats/main.go <nl> ppp b / nats/main.go <nl>@@ -31,14 +31,14 @@ func main() { <nl> ncli.Version(version) <nl> ncli.HelpFlag.Short('h') <nl> - ncli.Flag(\"server\", \"NATS servers\").Short('s').Default(\"localhost:4222\").Envar(\"NATS_URL\").StringVar(&servers) <nl> - ncli.Flag(\"user\", \"Username of Token\").Envar(\"NATS_USER\").StringVar(&username) <nl> - ncli.Flag(\"password\", \"Password\").Envar(\"NATS_PASSWORD\").StringVar(&password) <nl> - ncli.Flag(\"creds\", \"User credentials\").Envar(\"NATS_CREDS\").StringVar(&creds) <nl> - ncli.Flag(\"tlscert\", \"TLS public certificate\").Envar(\"NATS_CERT\").ExistingFileVar(&tlsCert) <nl> - ncli.Flag(\"tlskey\", \"TLS private key\").Envar(\"NATS_KEY\").ExistingFileVar(&tlsCert) <nl> - ncli.Flag(\"tlsca\", \"TLS certificate authority chain\").Envar(\"NATS_CA\").ExistingFileVar(&tlsCA) <nl> - ncli.Flag(\"timeout\", \"Time to wait on responses from NATS\").Default(\"2s\").Envar(\"NATS_TIMEOUT\").DurationVar(&timeout) <nl> + ncli.Flag(\"server\", \"NATS servers\").Short('s').Default(\"localhost:4222\").Envar(\"NATS_URL\").PlaceHolder(\"NATS_URL\").StringVar(&servers) <nl> + ncli.Flag(\"user\", \"Username of Token\").Envar(\"NATS_USER\").PlaceHolder(\"NATS_USER\").StringVar(&username) <nl> + ncli.Flag(\"password\", \"Password\").Envar(\"NATS_PASSWORD\").PlaceHolder(\"NATS_PASSWORD\").StringVar(&password) <nl> + ncli.Flag(\"creds\", \"User credentials\").Envar(\"NATS_CREDS\").PlaceHolder(\"NATS_CREDS\").StringVar(&creds) <nl> + ncli.Flag(\"tlscert\", \"TLS public certificate\").Envar(\"NATS_CERT\").PlaceHolder(\"NATS_CERT\").ExistingFileVar(&tlsCert) <nl> + ncli.Flag(\"tlskey\", \"TLS private key\").Envar(\"NATS_KEY\").PlaceHolder(\"NATS_KEY\").ExistingFileVar(&tlsCert) <nl> + ncli.Flag(\"tlsca\", \"TLS certificate authority chain\").Envar(\"NATS_CA\").PlaceHolder(\"NATS_CA\").ExistingFileVar(&tlsCA) <nl> + ncli.Flag(\"timeout\", \"Time to wait on responses from NATS\").Default(\"2s\").Envar(\"NATS_TIMEOUT\").PlaceHolder(\"NATS_TIMEOUT\").DurationVar(&timeout) <nl> ncli.Flag(\"trace\", \"Trace the JetStream JSON API interactions\").BoolVar(&trace) <nl> log.SetFlags(log.Ltime) <nl> ", "msg": "surface environment vars in help output\nWhile kingpin does not support showing the variables in --help output,\nthis at least gives some hints at them, not perfect but its what we got"}
{"diff_id": 552, "repo": "nats-io/natscli", "sha": "1b56879cf2161ff8a64fd330ffd785c64e653ca8", "time": "17.08.2020 13:48:52", "diff": "mmm a / nats/stream_command.go <nl> ppp b / nats/stream_command.go <nl>@@ -210,7 +210,7 @@ func (c *streamCmd) restoreAction(pc *kingpin.ParseContext) (err error) { <nl> } <nl> fmt.Println() <nl> - fmt.Printf(\"Restored stream %q in %v\\n\", c.stream, fp.EndTime().Sub(fp.StartTime())) <nl> + fmt.Printf(\"Restored stream %q in %v\\n\", c.stream, fp.EndTime().Sub(fp.StartTime()).Round(time.Second)) <nl> fmt.Println() <nl> stream, err := jsm.LoadStream(c.stream) <nl> @@ -296,7 +296,7 @@ func (c *streamCmd) backupAction(_ *kingpin.ParseContext) (err error) { <nl> kingpin.FatalIfError(err, \"snapshot failed\") <nl> fmt.Println() <nl> - fmt.Printf(\"Received %s compressed data in %d chunks for stream %q in %v, %s uncompressed \\n\", humanize.IBytes(fp.BytesReceived()), fp.ChunksReceived(), c.stream, fp.EndTime().Sub(fp.StartTime()), humanize.IBytes(fp.BlockBytesReceived())) <nl> + fmt.Printf(\"Received %s compressed data in %d chunks for stream %q in %v, %s uncompressed \\n\", humanize.IBytes(fp.BytesReceived()), fp.ChunksReceived(), c.stream, fp.EndTime().Sub(fp.StartTime()).Round(time.Second), humanize.IBytes(fp.BlockBytesReceived())) <nl> return nil <nl> } <nl> ", "msg": "round some durations to the nearest second"}
{"diff_id": 571, "repo": "nats-io/natscli", "sha": "d3e8dca62b7db99e8f5c31fd63ee3b49ad0fdb93", "time": "09.12.2020 16:21:52", "diff": "mmm a / jsonschema.go <nl> ppp b / jsonschema.go <nl>@@ -25,7 +25,7 @@ type SchemaValidator struct{} <nl> func (v SchemaValidator) ValidateStruct(data interface{}, schemaType string) (ok bool, errs []string) { <nl> s, err := api.Schema(schemaType) <nl> if err != nil { <nl> - return false, []string{\"unknown schema type %s\", schemaType} <nl> + return false, []string{fmt.Sprintf(\"unknown schema type %s\", schemaType)} <nl> } <nl> ls := gojsonschema.NewBytesLoader(s) <nl> ", "msg": "Fix error formatting (missing a call to Sprintf)\nJust a minor thing I noticed while skimming over the code..."}
{"diff_id": 597, "repo": "nats-io/natscli", "sha": "48c944a5fb52d43e2ce4ec5df5725a48419ac69d", "time": "23.04.2021 09:06:31", "diff": "mmm a / nats/nats_test.go <nl> ppp b / nats/nats_test.go <nl>@@ -570,8 +570,8 @@ func TestCLIConsumerCopy(t *testing.T) { <nl> t.Fatalf(\"Expected pull1 to be pull-based, got %v\", pull1.Configuration()) <nl> } <nl> - if pull1.MaxAckPending() != 0 { <nl> - t.Fatalf(\"Expected pull1 to have 0 Ack outstanding, got %v\", pull1.MaxAckPending()) <nl> + if pull1.MaxAckPending() != 20000 { <nl> + t.Fatalf(\"Expected pull1 to have 20000 Ack outstanding, got %v\", pull1.MaxAckPending()) <nl> } <nl> } <nl> ", "msg": "update tests for new JS defaults"}
{"diff_id": 605, "repo": "nats-io/natscli", "sha": "63bc3f0ccfd1012abf2aedbf82033840140b205c", "time": "17.05.2021 13:44:01", "diff": "mmm a / nats/server_report_command.go <nl> ppp b / nats/server_report_command.go <nl>@@ -18,6 +18,7 @@ import ( <nl> \"fmt\" <nl> \"log\" <nl> \"sort\" <nl> + \"strings\" <nl> \"github.com/dustin/go-humanize\" <nl> \"github.com/fatih/color\" <nl> @@ -41,12 +42,13 @@ type SrvReportCmd struct { <nl> type srvReportAccountInfo struct { <nl> Account string `json:\"account\"` <nl> Connections int `json:\"connections\"` <nl> - ConnInfo []*server.ConnInfo `json:\"connection_info\"` <nl> + ConnInfo []connInfo `json:\"connection_info\"` <nl> InMsgs int64 `json:\"in_msgs\"` <nl> OutMsgs int64 `json:\"out_msgs\"` <nl> InBytes int64 `json:\"in_bytes\"` <nl> OutBytes int64 `json:\"out_bytes\"` <nl> Subs int `json:\"subscriptions\"` <nl> + Server []*server.ServerInfo `json:\"server\"` <nl> } <nl> func configureServerReportCommand(srv *kingpin.CmdClause) { <nl> @@ -285,7 +287,7 @@ func (c *SrvReportCmd) reportAccount(_ *kingpin.ParseContext) error { <nl> if c.account != \"\" { <nl> accounts := c.accountInfo(connz) <nl> if len(accounts) != 1 { <nl> - return fmt.Errorf(\"received results for multiple accounts\") <nl> + return fmt.Errorf(\"received results for multiple accounts, expected %v\", c.account) <nl> } <nl> account, ok := accounts[c.account] <nl> @@ -339,10 +341,15 @@ func (c *SrvReportCmd) reportAccount(_ *kingpin.ParseContext) error { <nl> } <nl> table := newTableWriter(fmt.Sprintf(\"%d Accounts Overview\", len(accounts))) <nl> - table.AddHeaders(\"Account\", \"Connections\", \"In Msgs\", \"Out Msgs\", \"In Bytes\", \"Out Bytes\", \"Subs\") <nl> + table.AddHeaders(\"Account\", \"Server / Cluster\", \"Connections\", \"In Msgs\", \"Out Msgs\", \"In Bytes\", \"Out Bytes\", \"Subs\") <nl> for _, acct := range accounts { <nl> - table.AddRow(acct.Account, humanize.Comma(int64(acct.Connections)), humanize.Comma(acct.InMsgs), humanize.Comma(acct.OutMsgs), humanize.IBytes(uint64(acct.InBytes)), humanize.IBytes(uint64(acct.OutBytes)), humanize.Comma(int64(acct.Subs))) <nl> + server := []string{} <nl> + for _, s := range acct.Server { <nl> + server = append(server, fmt.Sprintf(\"%s / %s\", s.Name, s.Cluster)) <nl> + } <nl> + <nl> + table.AddRow(acct.Account, strings.Join(server, \", \"), humanize.Comma(int64(acct.Connections)), humanize.Comma(acct.InMsgs), humanize.Comma(acct.OutMsgs), humanize.IBytes(uint64(acct.InBytes)), humanize.IBytes(uint64(acct.OutBytes)), humanize.Comma(int64(acct.Subs))) <nl> } <nl> fmt.Print(table.Render()) <nl> @@ -350,30 +357,47 @@ func (c *SrvReportCmd) reportAccount(_ *kingpin.ParseContext) error { <nl> return nil <nl> } <nl> -func (c *SrvReportCmd) accountInfo(connz []*server.Connz) map[string]*srvReportAccountInfo { <nl> +func (c *SrvReportCmd) accountInfo(connz connzList) map[string]*srvReportAccountInfo { <nl> result := make(map[string]*srvReportAccountInfo) <nl> for _, conn := range connz { <nl> - for _, info := range conn.Conns { <nl> + for _, info := range conn.Connz.Conns { <nl> account, ok := result[info.Account] <nl> if !ok { <nl> result[info.Account] = &srvReportAccountInfo{Account: info.Account} <nl> account = result[info.Account] <nl> } <nl> - account.ConnInfo = append(account.ConnInfo, info) <nl> + account.ConnInfo = append(account.ConnInfo, connInfo{info, conn.ServerInfo}) <nl> account.Connections++ <nl> account.InBytes += info.InBytes <nl> account.OutBytes += info.OutBytes <nl> account.InMsgs += info.InMsgs <nl> account.OutMsgs += info.OutMsgs <nl> account.Subs += len(info.Subs) <nl> + <nl> + // make sure we only store one server info per unique server <nl> + found := false <nl> + for _, s := range account.Server { <nl> + if s.ID == conn.ServerInfo.ID { <nl> + found = true <nl> + break <nl> + } <nl> + } <nl> + if !found { <nl> + account.Server = append(account.Server, conn.ServerInfo) <nl> + } <nl> } <nl> } <nl> return result <nl> } <nl> +type connInfo struct { <nl> + *server.ConnInfo <nl> + Info *server.ServerInfo `json:\"server\"` <nl> +} <nl> + <nl> func (c *SrvReportCmd) reportConnections(_ *kingpin.ParseContext) error { <nl> nc, _, err := prepareHelper(\"\", natsOpts()...) <nl> if err != nil { <nl> @@ -393,11 +417,7 @@ func (c *SrvReportCmd) reportConnections(_ *kingpin.ParseContext) error { <nl> return fmt.Errorf(\"did not get results from any servers\") <nl> } <nl> - var conns []*server.ConnInfo <nl> - <nl> - for _, conn := range connz { <nl> - conns = append(conns, conn.Conns...) <nl> - } <nl> + conns := connz.flatConnInfo() <nl> c.sortConnections(conns) <nl> @@ -424,7 +444,7 @@ func (c *SrvReportCmd) boolReverse(v bool) bool { <nl> return v <nl> } <nl> -func (c *SrvReportCmd) sortConnections(conns []*server.ConnInfo) { <nl> +func (c *SrvReportCmd) sortConnections(conns []connInfo) { <nl> sort.Slice(conns, func(i int, j int) bool { <nl> switch c.sort { <nl> case \"in-bytes\": <nl> @@ -445,9 +465,9 @@ func (c *SrvReportCmd) sortConnections(conns []*server.ConnInfo) { <nl> }) <nl> } <nl> -func (c *SrvReportCmd) renderConnections(report []*server.ConnInfo) { <nl> +func (c *SrvReportCmd) renderConnections(report []connInfo) { <nl> table := newTableWriter(fmt.Sprintf(\"%d Connections Overview\", len(report))) <nl> - table.AddHeaders(\"CID\", \"Name\", \"IP\", \"Account\", \"Uptime\", \"In Msgs\", \"Out Msgs\", \"In Bytes\", \"Out Bytes\", \"Subs\") <nl> + table.AddHeaders(\"CID\", \"Name\", \"Server\", \"Cluster\", \"IP\", \"Account\", \"Uptime\", \"In Msgs\", \"Out Msgs\", \"In Bytes\", \"Out Bytes\", \"Subs\") <nl> if c.json { <nl> printJSON(report) <nl> @@ -472,31 +492,92 @@ func (c *SrvReportCmd) renderConnections(report []*server.ConnInfo) { <nl> iBytes += info.InBytes <nl> subs += info.NumSubs <nl> - table.AddRow(info.Cid, name, info.IP, info.Account, info.Uptime, humanize.Comma(info.InMsgs), humanize.Comma(info.OutMsgs), humanize.IBytes(uint64(info.InBytes)), humanize.IBytes(uint64(info.OutBytes)), len(info.Subs)) <nl> + srvName := info.Info.Name <nl> + cluster := info.Info.Cluster <nl> + <nl> + acc := info.Account[0:10] + \"..\" + info.Account[46:] <nl> + <nl> + table.AddRow(info.Cid, name, srvName, cluster, info.IP, acc, info.Uptime, humanize.Comma(info.InMsgs), humanize.Comma(info.OutMsgs), humanize.IBytes(uint64(info.InBytes)), humanize.IBytes(uint64(info.OutBytes)), len(info.Subs)) <nl> } <nl> if len(report) > 1 { <nl> table.AddSeparator() <nl> - table.AddRow(\"\", \"\", \"\", \"\", \"\", humanize.Comma(iMsgs), humanize.Comma(oMsgs), humanize.IBytes(uint64(iBytes)), humanize.IBytes(uint64(oBytes)), humanize.Comma(int64(subs))) <nl> + table.AddRow(\"\", \"\", \"\", \"\", \"\", \"\", \"\", humanize.Comma(iMsgs), humanize.Comma(oMsgs), humanize.IBytes(uint64(iBytes)), humanize.IBytes(uint64(oBytes)), humanize.Comma(int64(subs))) <nl> } <nl> fmt.Print(table.Render()) <nl> } <nl> +type connz struct { <nl> + Connz *server.Connz <nl> + ServerInfo *server.ServerInfo <nl> +} <nl> + <nl> +type connzList []connz <nl> + <nl> +func (c connzList) flatConnInfo() []connInfo { <nl> + var conns []connInfo <nl> + for _, conn := range c { <nl> + for _, c := range conn.Connz.Conns { <nl> + conns = append(conns, connInfo{c, conn.ServerInfo}) <nl> + } <nl> + } <nl> + return conns <nl> +} <nl> + <nl> +func parseConnzResp(resp []byte) (connz, error) { <nl> + reqresp := map[string]json.RawMessage{} <nl> + <nl> + err := json.Unmarshal(resp, &reqresp) <nl> + if err != nil { <nl> + return connz{}, err <nl> + } <nl> + <nl> + errresp, ok := reqresp[\"error\"] <nl> + if ok { <nl> + return connz{}, fmt.Errorf(\"invalid response received: %#v\", errresp) <nl> + } <nl> + <nl> + data, ok := reqresp[\"data\"] <nl> + if !ok { <nl> + return connz{}, fmt.Errorf(\"no data received in response: %#v\", reqresp) <nl> + } <nl> + <nl> + c := connz{ <nl> + Connz: &server.Connz{}, <nl> + ServerInfo: &server.ServerInfo{}, <nl> + } <nl> + <nl> + s, ok := reqresp[\"server\"] <nl> + if !ok { <nl> + return connz{}, fmt.Errorf(\"no server data received in response: %#v\", reqresp) <nl> + } <nl> + err = json.Unmarshal(s, c.ServerInfo) <nl> + if err != nil { <nl> + return connz{}, err <nl> + } <nl> + <nl> + err = json.Unmarshal(data, c.Connz) <nl> + if err != nil { <nl> + return connz{}, err <nl> + } <nl> + return c, nil <nl> +} <nl> + <nl> // recursively fetches connz from the fleet till all paged results is complete <nl> -func (c *SrvReportCmd) getConnz(current []*server.Connz, nc *nats.Conn, level int) (result []*server.Connz, all bool, err error) { <nl> +func (c *SrvReportCmd) getConnz(current []connz, nc *nats.Conn, level int) (result connzList, all bool, err error) { <nl> // warn every 5 levels that we're still recursing... <nl> if level != 0 && level%5 == 0 { <nl> log.Printf(\"Recursing into %d servers to resolve all pages of connection info\", len(current)) <nl> } <nl> if current == nil { <nl> - current = []*server.Connz{} <nl> + current = []connz{} <nl> } <nl> // get the initial result from all nodes as one req and then recursively fetch all that has more pages <nl> if len(current) == 0 { <nl> - var initial []*server.Connz <nl> + var initial []connz <nl> req := &server.ConnzEventOptions{ <nl> ConnzOptions: server.ConnzOptions{ <nl> @@ -513,30 +594,11 @@ func (c *SrvReportCmd) getConnz(current []*server.Connz, nc *nats.Conn, level in <nl> } <nl> for _, c := range res { <nl> - reqresp := map[string]json.RawMessage{} <nl> - <nl> - err = json.Unmarshal(c, &reqresp) <nl> + co, err := parseConnzResp(c) <nl> if err != nil { <nl> return nil, false, err <nl> } <nl> - <nl> - errresp, ok := reqresp[\"error\"] <nl> - if ok { <nl> - return nil, false, fmt.Errorf(\"invalid response received: %#v\", errresp) <nl> - } <nl> - <nl> - data, ok := reqresp[\"data\"] <nl> - if !ok { <nl> - return nil, false, fmt.Errorf(\"no data received in response: %#v\", reqresp) <nl> - } <nl> - <nl> - connz := &server.Connz{} <nl> - err = json.Unmarshal(data, &connz) <nl> - if err != nil { <nl> - return nil, false, err <nl> - } <nl> - <nl> - initial = append(initial, connz) <nl> + initial = append(initial, co) <nl> } <nl> // recursively fetch... <nl> @@ -550,9 +612,9 @@ func (c *SrvReportCmd) getConnz(current []*server.Connz, nc *nats.Conn, level in <nl> } <nl> // find ones in the current list that's incomplete <nl> - var incomplete []*server.Connz <nl> + var incomplete []connz <nl> for _, conn := range current { <nl> - if (conn.Offset+1)*conn.Limit < conn.Total { <nl> + if (conn.Connz.Offset+1)*conn.Connz.Limit < conn.Connz.Total { <nl> incomplete = append(incomplete, conn) <nl> } <nl> } <nl> @@ -587,11 +649,11 @@ func (c *SrvReportCmd) getConnz(current []*server.Connz, nc *nats.Conn, level in <nl> SubscriptionsDetail: false, <nl> Account: c.account, <nl> Username: true, <nl> - Offset: conn.Offset + 1, <nl> + Offset: conn.Connz.Offset + 1, <nl> }, <nl> EventFilterOptions: c.reqFilter(), <nl> } <nl> - res, err := c.doReq(req, fmt.Sprintf(\"$SYS.REQ.SERVER.%s.CONNZ\", conn.ID), nc) <nl> + res, err := c.doReq(req, fmt.Sprintf(\"$SYS.REQ.SERVER.%s.CONNZ\", conn.Connz.ID), nc) <nl> if err == nats.ErrNoResponders { <nl> return nil, false, fmt.Errorf(\"server request failed, ensure the account used has system privileges and appropriate permissions\") <nl> } else if err != nil { <nl> @@ -599,13 +661,11 @@ func (c *SrvReportCmd) getConnz(current []*server.Connz, nc *nats.Conn, level in <nl> } <nl> for _, c := range res { <nl> - connz := &server.Connz{} <nl> - err = json.Unmarshal(c, connz) <nl> + co, err := parseConnzResp(c) <nl> if err != nil { <nl> return nil, false, err <nl> } <nl> - <nl> - result = append(result, connz) <nl> + result = append(result, co) <nl> } <nl> } <nl> ", "msg": "[added] display of server/cluster name to  server report connections/accounts\nWhen -j (json output) is provided server info will be included.\nFor accounts, we collect all server/cluster the account is present in."}
{"diff_id": 633, "repo": "nats-io/natscli", "sha": "76158853501d50ac17f90181456433816258f0a5", "time": "14.10.2021 15:03:07", "diff": "mmm a / nats/object_command.go <nl> ppp b / nats/object_command.go <nl>@@ -81,6 +81,7 @@ NOTE: This is an experimental feature. <nl> get.Arg(\"file\", \"The file to retrieve\").Required().StringVar(&c.file) <nl> get.Flag(\"output\", \"Override the output file name\").Short('O').StringVar(&c.overrideName) <nl> get.Flag(\"no-progress\", \"Disables progress bars\").Default(\"false\").BoolVar(&c.noProgress) <nl> + get.Flag(\"force\", \"Act without confirmation\").Short('f').BoolVar(&c.force) <nl> info := obj.Command(\"info\", \"Get information about a bucket or object\").Action(c.infoAction) <nl> info.Arg(\"bucket\", \"The bucket to act on\").Required().StringVar(&c.bucket) <nl> ", "msg": "fix force for obj get"}
{"diff_id": 657, "repo": "nats-io/natscli", "sha": "e33868d61e55548e79f84fea9a9eb5ae7c049c26", "time": "04.03.2022 13:46:31", "diff": "mmm a / cli/stream_command.go <nl> ppp b / cli/stream_command.go <nl>@@ -1714,7 +1714,7 @@ func (c *streamCmd) prepareConfig(pc *kingpin.ParseContext) api.StreamConfig { <nl> var maxAge time.Duration <nl> if c.maxBytesLimit == 0 { <nl> - c.maxBytesLimit, err = askOneBytes(\"Message size limit\", \"-1\", \"Defines the combined size of all messages in a Stream, when exceeded oldest messages are removed, -1 for unlimited. Settable using --max-bytes\") <nl> + c.maxBytesLimit, err = askOneBytes(\"Stream size limit\", \"-1\", \"Defines the combined size of all messages in a Stream, when exceeded messages are removed or new ones are rejected, -1 for unlimited. Settable using --max-bytes\") <nl> kingpin.FatalIfError(err, \"invalid input\") <nl> if c.maxBytesLimit <= 0 { <nl> ", "msg": "wording tweaks for max stream size question"}
{"diff_id": 679, "repo": "nats-io/natscli", "sha": "22310b49066120c974872eae9fa16c8d426d1a48", "time": "17.04.2022 09:24:15", "diff": "mmm a / cli/stream_command.go <nl> ppp b / cli/stream_command.go <nl>@@ -254,6 +254,8 @@ func configureStreamCommand(app commandHost) { <nl> strRestore.Arg(\"file\", \"The directory holding the backup to restore\").Required().ExistingDirVar(&c.backupDirectory) <nl> strRestore.Flag(\"progress\", \"Enables or disables progress reporting using a progress bar\").Default(\"true\").BoolVar(&c.showProgress) <nl> strRestore.Flag(\"config\", \"Load a different configuration when restoring the stream\").ExistingFileVar(&c.inputFile) <nl> + strRestore.Flag(\"cluster\", \"Place the stream in a specific cluster\").StringVar(&c.placementCluster) <nl> + strRestore.Flag(\"tag\", \"Place the stream on servers that has specific tags (pass multiple times)\").StringsVar(&c.placementTags) <nl> strSeal := str.Command(\"seal\", \"Seals a stream preventing further updates\").Action(c.sealAction) <nl> strSeal.Arg(\"stream\", \"The name of the Stream to seal\").Required().StringVar(&c.stream) <nl> @@ -631,6 +633,8 @@ func (c *streamCmd) restoreAction(_ *kingpin.ParseContext) error { <nl> err = json.Unmarshal(bmj, &bm) <nl> kingpin.FatalIfError(err, \"restore failed\") <nl> + var cfg *api.StreamConfig <nl> + <nl> known, err := mgr.IsKnownStream(bm.Config.Name) <nl> kingpin.FatalIfError(err, \"Could not check if the stream already exist\") <nl> if known { <nl> @@ -674,10 +678,19 @@ func (c *streamCmd) restoreAction(_ *kingpin.ParseContext) error { <nl> if bm.Config.Name != cfg.Name { <nl> return fmt.Errorf(\"stream names may not be changed during restore\") <nl> } <nl> + } else { <nl> + cfg = &bm.Config <nl> + } <nl> - opts = append(opts, jsm.RestoreConfiguration(*cfg)) <nl> + if c.placementCluster != \"\" || len(c.placementTags) > 0 { <nl> + cfg.Placement = &api.Placement{ <nl> + Cluster: c.placementCluster, <nl> + Tags: c.placementTags, <nl> + } <nl> } <nl> + opts = append(opts, jsm.RestoreConfiguration(*cfg)) <nl> + <nl> fmt.Printf(\"Starting restore of Stream %q from file %q\\n\\n\", bm.Config.Name, c.backupDirectory) <nl> fp, _, err := mgr.RestoreSnapshotFromDirectory(ctx, bm.Config.Name, c.backupDirectory, opts...) <nl> ", "msg": "allow placement to be adjusted on restore"}
{"diff_id": 715, "repo": "nats-io/natscli", "sha": "97976937d5ec447a8134a8f29d9f39dbdc3da346", "time": "23.07.2022 21:48:10", "diff": "mmm a / cli/sub_command.go <nl> ppp b / cli/sub_command.go <nl>@@ -48,7 +48,7 @@ type subCmd struct { <nl> headersOnly bool <nl> stream string <nl> jetStream bool <nl> - excludeSubjets string <nl> + excludeSubjets []string <nl> } <nl> func configureSubCommand(app commandHost) { <nl> @@ -73,7 +73,7 @@ func configureSubCommand(app commandHost) { <nl> act.Flag(\"since\", \"Delivers messages received since a duration (requires JetStream)\").PlaceHolder(\"DURATION\").StringVar(&c.deliverSince) <nl> act.Flag(\"last-per-subject\", \"Deliver the most recent messages for each subject in the Stream (requires JetStream)\").UnNegatableBoolVar(&c.deliverLastPerSubject) <nl> act.Flag(\"stream\", \"Subscribe to a specific stream (required JetStream)\").PlaceHolder(\"STREAM\").StringVar(&c.stream) <nl> - act.Flag(\"exclude-subjects\", \"Subjects for which corresponding messages will be excluded and therefore not shown in the output\").StringVar(&c.excludeSubjets) <nl> + act.Flag(\"exclude-subjects\", \"Subjects for which corresponding messages will be excluded and therefore not shown in the output\").StringsVar(&c.excludeSubjets) <nl> } <nl> func init() { <nl> @@ -111,7 +111,7 @@ func (c *subCmd) subscribe(p *fisk.ParseContext) error { <nl> mu = sync.Mutex{} <nl> dump = c.dump != \"\" <nl> ctr = uint(0) <nl> - excludeSubjects = splitString(c.excludeSubjets) <nl> + excludeSubjects = splitCLISubjects(c.excludeSubjets) <nl> ctx, cancel = context.WithCancel(ctx) <nl> replySub *nats.Subscription <nl> ", "msg": "Add support for array of excluded subjects"}
{"diff_id": 727, "repo": "nats-io/natscli", "sha": "3fdb894bd732f57f33276d178458f291585ee36c", "time": "26.10.2022 15:18:28", "diff": "mmm a / cli/server_request_command.go <nl> ppp b / cli/server_request_command.go <nl>@@ -174,13 +174,18 @@ func (c *SrvRequestCmd) jsz(_ *fisk.ParseContext) error { <nl> } <nl> func (c *SrvRequestCmd) reqFilter() server.EventFilterOptions { <nl> - return server.EventFilterOptions{ <nl> + opt := server.EventFilterOptions{ <nl> Name: c.name, <nl> Host: c.host, <nl> Cluster: c.cluster, <nl> Tags: c.tags, <nl> Domain: opts.Config.JSDomain(), <nl> } <nl> + if opts.Config != nil { <nl> + opt.Domain = opts.Config.JSDomain() <nl> + } <nl> + <nl> + return opt <nl> } <nl> func (c *SrvRequestCmd) accountz(_ *fisk.ParseContext) error { <nl> ", "msg": "fix nil pointer issue when constructing server filters"}
{"diff_id": 729, "repo": "nats-io/natscli", "sha": "867a79dcd87fa046e88926836748c51d28237ece", "time": "28.10.2022 09:35:09", "diff": "mmm a / cli/reply_command.go <nl> ppp b / cli/reply_command.go <nl>@@ -52,6 +52,14 @@ This will request the weather for london when invoked as: <nl> nats request weather.london '' <nl> +Use {{.Request}} to access the request body within the --command <nl> + <nl> +The command gets also spawned with two ENVs: <nl> + NATS_REQUEST_SUBJECT <nl> + NATS_REQUEST_BODY <nl> + <nl> + nats reply 'echo' --command=\"printenv NATS_REQUEST_BODY\" <nl> + <nl> The body and Header values of the messages may use Go templates to create unique messages. <nl> nats reply test \"Message {{Count}} @ {{Time}}\" <nl> ", "msg": "add some help comments to use request body aand subject within reply --command"}
{"diff_id": 743, "repo": "nats-io/natscli", "sha": "56b9beb9b69501431b79cf77f1b1422b003446fd", "time": "09.12.2022 18:23:01", "diff": "mmm a / cli/server_request_command.go <nl> ppp b / cli/server_request_command.go <nl>@@ -66,8 +66,8 @@ func configureServerRequestCommand(srv *fisk.CmdClause) { <nl> subz := req.Command(\"subscriptions\", \"Show subscription information\").Alias(\"sub\").Alias(\"subsz\").Action(c.subs) <nl> subz.Arg(\"wait\", \"Wait for a certain number of responses\").Uint32Var(&c.waitFor) <nl> subz.Flag(\"detail\", \"Include detail about all subscriptions\").UnNegatableBoolVar(&c.detail) <nl> - subz.Flag(\"filter-account\", \"Filter on a specific account\").StringVar(&c.accountFilter) <nl> - subz.Flag(\"subject\", \"Filter based on subscriptions matching this subject\").StringVar(&c.subjectFilter) <nl> + subz.Flag(\"filter-account\", \"Filter on a specific account\").PlaceHolder(\"ACCOUNT\").StringVar(&c.accountFilter) <nl> + subz.Flag(\"filter-subject\", \"Filter based on subscriptions matching this subject\").PlaceHolder(\"SUBJECT\").StringVar(&c.subjectFilter) <nl> varz := req.Command(\"variables\", \"Show runtime variables\").Alias(\"var\").Alias(\"varz\").Action(c.varz) <nl> varz.Arg(\"wait\", \"Wait for a certain number of responses\").Uint32Var(&c.waitFor) <nl> @@ -76,11 +76,11 @@ func configureServerRequestCommand(srv *fisk.CmdClause) { <nl> connz.Arg(\"wait\", \"Wait for a certain number of responses\").Uint32Var(&c.waitFor) <nl> connz.Flag(\"sort\", \"Sort by a specific property\").Default(\"cid\").EnumVar(&c.sortOpt, \"cid\", \"start\", \"subs\", \"pending\", \"msgs_to\", \"msgs_from\", \"bytes_to\", \"bytes_from\", \"last\", \"idle\", \"uptime\", \"stop\", \"reason\") <nl> connz.Flag(\"subscriptions\", \"Show subscriptions\").UnNegatableBoolVar(&c.detail) <nl> - connz.Flag(\"filter-cid\", \"Filter on a specific CID\").Uint64Var(&c.cidFilter) <nl> - connz.Flag(\"filter-state\", \"Filter on a specific account state (open, closed, all)\").Default(\"open\").EnumVar(&c.stateFilter, \"open\", \"closed\", \"all\") <nl> - connz.Flag(\"filter-user\", \"Filter on a specific username\").StringVar(&c.userFilter) <nl> - connz.Flag(\"filter-account\", \"Filter on a specific account\").StringVar(&c.accountFilter) <nl> - connz.Flag(\"filter-subject\", \"Limits responses only to those connections with matching subscription interest\").StringVar(&c.subjectFilter) <nl> + connz.Flag(\"filter-cid\", \"Filter on a specific CID\").PlaceHolder(\"CID\").Uint64Var(&c.cidFilter) <nl> + connz.Flag(\"filter-state\", \"Filter on a specific account state (open, closed, all)\").PlaceHolder(\"STATE\").Default(\"open\").EnumVar(&c.stateFilter, \"open\", \"closed\", \"all\") <nl> + connz.Flag(\"filter-user\", \"Filter on a specific username\").PlaceHolder(\"USER\").StringVar(&c.userFilter) <nl> + connz.Flag(\"filter-account\", \"Filter on a specific account\").PlaceHolder(\"ACCOUNT\").StringVar(&c.accountFilter) <nl> + connz.Flag(\"filter-subject\", \"Limits responses only to those connections with matching subscription interest\").PlaceHolder(\"SUBJECT\").StringVar(&c.subjectFilter) <nl> routez := req.Command(\"routes\", \"Show route details\").Alias(\"route\").Alias(\"routez\").Action(c.routez) <nl> routez.Arg(\"wait\", \"Wait for a certain number of responses\").Uint32Var(&c.waitFor) <nl> @@ -88,8 +88,8 @@ func configureServerRequestCommand(srv *fisk.CmdClause) { <nl> gwyz := req.Command(\"gateways\", \"Show gateway details\").Alias(\"gateway\").Alias(\"gwy\").Alias(\"gatewayz\").Action(c.gwyz) <nl> gwyz.Arg(\"wait\", \"Wait for a certain number of responses\").Uint32Var(&c.waitFor) <nl> - gwyz.Arg(\"filter-name\", \"Filter results on gateway name\").StringVar(&c.nameFilter) <nl> - gwyz.Flag(\"filter-account\", \"Show only a certain account in account detail\").StringVar(&c.accountFilter) <nl> + gwyz.Arg(\"filter-name\", \"Filter results on gateway name\").PlaceHolder(\"NAME\").StringVar(&c.nameFilter) <nl> + gwyz.Flag(\"filter-account\", \"Show only a certain account in account detail\").PlaceHolder(\"ACCOUNT\").StringVar(&c.accountFilter) <nl> gwyz.Flag(\"accounts\", \"Show account detail\").UnNegatableBoolVar(&c.detail) <nl> leafz := req.Command(\"leafnodes\", \"Show leafnode details\").Alias(\"leaf\").Alias(\"leafz\").Action(c.leafz) <nl> ", "msg": "Improve help layout using better placeholders"}
{"diff_id": 746, "repo": "nats-io/natscli", "sha": "852ca7c26c0d64d0fe8425da3932f05fc95e765d", "time": "06.01.2023 13:03:03", "diff": "mmm a / cli/server_report_command.go <nl> ppp b / cli/server_report_command.go <nl>@@ -116,6 +116,7 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> bytes uint64 <nl> msgs uint64 <nl> cluster *server.MetaClusterInfo <nl> + expectedClusterSize int <nl> ) <nl> renderDomain := false <nl> @@ -211,10 +212,15 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> msgs += js.Data.Messages <nl> leader := \"\" <nl> - if js.Data.Meta != nil && js.Data.Meta.Leader == js.Server.Name { <nl> + if js.Data.Meta != nil { <nl> + if js.Data.Meta.Leader == js.Server.Name { <nl> leader = \"*\" <nl> cluster = js.Data.Meta <nl> } <nl> + if expectedClusterSize < js.Data.Meta.Size { <nl> + expectedClusterSize = js.Data.Meta.Size <nl> + } <nl> + } <nl> row := []any{cNames[i] + leader, js.Server.Cluster} <nl> if renderDomain { <nl> @@ -244,7 +250,11 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> fmt.Print(table.Render()) <nl> fmt.Println() <nl> - if cluster != nil { <nl> + if len(jszResponses) > 0 && cluster == nil { <nl> + fmt.Println() <nl> + fmt.Printf(\"WARNING: No cluster meta leader found. The cluster expects %d nodes but only %d responded. JetStream operation require at least %d up nodes.\", expectedClusterSize, len(jszResponses), expectedClusterSize/2+1) <nl> + fmt.Println() <nl> + } else { <nl> cluster.Replicas = append(cluster.Replicas, &server.PeerInfo{ <nl> Name: cluster.Leader, <nl> Current: true, <nl> ", "msg": "report when the cluster is in an impossible leader situation"}
{"diff_id": 748, "repo": "nats-io/natscli", "sha": "c79f5a36c34a2c147c8f5ab56b34010c4263a253", "time": "10.01.2023 20:10:43", "diff": "mmm a / cli/server_check_command.go <nl> ppp b / cli/server_check_command.go <nl>@@ -228,11 +228,7 @@ func (r *result) criticalIfErr(err error, format string, a ...any) bool { <nl> return true <nl> } <nl> -func (r *result) exitCode() int { <nl> - if checkRenderFormat == \"prometheus\" { <nl> - return 0 <nl> - } <nl> - <nl> +func (r *result) nagiosCode() int { <nl> switch r.Status { <nl> case okCheckStatus: <nl> return 0 <nl> @@ -245,6 +241,14 @@ func (r *result) exitCode() int { <nl> } <nl> } <nl> +func (r *result) exitCode() int { <nl> + if checkRenderFormat == \"prometheus\" { <nl> + return 0 <nl> + } <nl> + <nl> + return r.nagiosCode() <nl> +} <nl> + <nl> func (r *result) Exit() { <nl> os.Exit(r.exitCode()) <nl> } <nl> @@ -323,12 +327,12 @@ func (r *result) renderPrometheus() string { <nl> } <nl> status := prometheus.NewGaugeVec(prometheus.GaugeOpts{ <nl> - Name: fmt.Sprintf(\"nats_server_check_%s_status_code\", r.Check), <nl> + Name: prometheus.BuildFQName(opts.PrometheusNamespace, r.Check, \"status_code\"), <nl> Help: fmt.Sprintf(\"Nagios compatible status code for %s\", r.Check), <nl> }, []string{\"item\", \"status\"}) <nl> prometheus.MustRegister(status) <nl> - status.WithLabelValues(sname, string(r.Status)).Set(float64(r.exitCode())) <nl> + status.WithLabelValues(sname, string(r.Status)).Set(float64(r.nagiosCode())) <nl> var buf bytes.Buffer <nl> ", "msg": "fix prometheus status_code metric that was always 0\nAlso ensure prom namespace is used for the status metric"}
{"diff_id": 749, "repo": "nats-io/natscli", "sha": "2e6a4fd80b5b2e599909bd9933b5204fcd9f0fbf", "time": "12.01.2023 17:16:02", "diff": "mmm a / cli/stream_command.go <nl> ppp b / cli/stream_command.go <nl>@@ -1008,6 +1008,10 @@ func (c *streamCmd) reportAction(_ *fisk.ParseContext) error { <nl> if source.FilterSubject != \"\" { <nl> edge.Label(source.FilterSubject) <nl> } <nl> + if source.SubjectTransform != \"\" { <nl> + edge2 := dg.Edge(snode, node).Attr(\"color\", \"red\") <nl> + edge2.Label(source.SubjectTransform) <nl> + } <nl> } <nl> } <nl> @@ -1593,7 +1597,13 @@ func (c *streamCmd) renderSource(s *api.StreamSource) string { <nl> parts = append(parts, fmt.Sprintf(\"Start Time: %v\", s.OptStartTime)) <nl> } <nl> if s.FilterSubject != \"\" { <nl> - parts = append(parts, fmt.Sprintf(\"Subject: %s\", s.FilterSubject)) <nl> + parts = append(parts, fmt.Sprintf(\"Subject filter: %s\", s.FilterSubject)) <nl> + } <nl> + if s.SubjectTransform != \"\" { <nl> + if s.FilterSubject == \"\" { <nl> + parts = append(parts, fmt.Sprintf(\"Subject filter: %s\", \">\")) <nl> + } <nl> + parts = append(parts, fmt.Sprintf(\"Subject transform: %s\", s.SubjectTransform)) <nl> } <nl> if s.External != nil { <nl> if s.External.ApiPrefix != \"\" { <nl> @@ -1674,6 +1684,20 @@ func (c *streamCmd) showStreamInfo(info *api.StreamInfo) { <nl> showSource := func(s *api.StreamSourceInfo) { <nl> fmt.Printf(\" Stream Name: %s\\n\", s.Name) <nl> + <nl> + if s.SubjectTransform != \"\" { <nl> + if s.FilterSubject == \"\" { <nl> + fmt.Printf(\" Subject Filter: %s\\n\", \">\") <nl> + } else { <nl> + fmt.Printf(\" Subject Filter: %s\\n\", s.FilterSubject) <nl> + } <nl> + fmt.Printf(\" Subject Transform: %s \\n\", s.SubjectTransform) <nl> + } else { <nl> + if s.FilterSubject != \"\" { <nl> + fmt.Printf(\" Subject Filter: %s\\n\", s.FilterSubject) <nl> + } <nl> + } <nl> + <nl> fmt.Printf(\" Lag: %s\\n\", humanize.Comma(int64(s.Lag))) <nl> if s.Active > 0 && s.Active < math.MaxInt64 { <nl> fmt.Printf(\" Last Seen: %v\\n\", humanizeDuration(s.Active)) <nl> @@ -2225,6 +2249,12 @@ func (c *streamCmd) askSource(name string, prefix string) *api.StreamSource { <nl> }, &cfg.FilterSubject) <nl> fisk.FatalIfError(err, \"could not request filter\") <nl> + err = askOne(&survey.Input{ <nl> + Message: fmt.Sprintf(\"%s Subject mapping transform\", prefix), <nl> + Help: \"Map matching subjects according to this destination transform\", <nl> + }, &cfg.SubjectTransform) <nl> + fisk.FatalIfError(err, \"could not request subject mapping destination transform\") <nl> + <nl> ok, err = askConfirmation(fmt.Sprintf(\"Import %q from a different JetStream domain\", name), false) <nl> fisk.FatalIfError(err, \"Could not request source details\") <nl> if ok { <nl> ", "msg": "Add support for stream source subject transforms\n(requires companion jsm.go changes)"}
{"diff_id": 753, "repo": "nats-io/natscli", "sha": "167558f24488e6d651d0e3c9cde80bd64c4216ca", "time": "27.01.2023 13:27:18", "diff": "mmm a / cli/server_check_command.go <nl> ppp b / cli/server_check_command.go <nl>@@ -103,71 +103,71 @@ func configureServerCheckCommand(srv *fisk.CmdClause) { <nl> conn := check.Command(\"connection\", \"Checks basic server connection\").Alias(\"conn\").Default().Action(c.checkConnection) <nl> conn.Flag(\"connect-warn\", \"Warning threshold to allow for establishing connections\").Default(\"500ms\").PlaceHolder(\"DURATION\").DurationVar(&c.connectWarning) <nl> - conn.Flag(\"connect-Critical\", \"Critical threshold to allow for establishing connections\").Default(\"1s\").PlaceHolder(\"DURATION\").DurationVar(&c.connectCritical) <nl> + conn.Flag(\"connect-critical\", \"Critical threshold to allow for establishing connections\").Default(\"1s\").PlaceHolder(\"DURATION\").DurationVar(&c.connectCritical) <nl> conn.Flag(\"rtt-warn\", \"Warning threshold to allow for server RTT\").Default(\"500ms\").PlaceHolder(\"DURATION\").DurationVar(&c.rttWarning) <nl> - conn.Flag(\"rtt-Critical\", \"Critical threshold to allow for server RTT\").Default(\"1s\").PlaceHolder(\"DURATION\").DurationVar(&c.rttCritical) <nl> + conn.Flag(\"rtt-critical\", \"Critical threshold to allow for server RTT\").Default(\"1s\").PlaceHolder(\"DURATION\").DurationVar(&c.rttCritical) <nl> conn.Flag(\"req-warn\", \"Warning threshold to allow for full round trip test\").PlaceHolder(\"DURATION\").Default(\"500ms\").DurationVar(&c.reqWarning) <nl> - conn.Flag(\"req-Critical\", \"Critical threshold to allow for full round trip test\").PlaceHolder(\"DURATION\").Default(\"1s\").DurationVar(&c.reqCritical) <nl> + conn.Flag(\"req-critical\", \"Critical threshold to allow for full round trip test\").PlaceHolder(\"DURATION\").Default(\"1s\").DurationVar(&c.reqCritical) <nl> stream := check.Command(\"stream\", \"Checks the health of mirrored streams, streams with sources or clustered streams\").Action(c.checkStream) <nl> stream.Flag(\"stream\", \"The streams to check\").Required().StringVar(&c.sourcesStream) <nl> - stream.Flag(\"lag-Critical\", \"Critical threshold to allow for lag on any source or mirror\").PlaceHolder(\"MSGS\").Uint64Var(&c.sourcesLagCritical) <nl> - stream.Flag(\"seen-Critical\", \"Critical threshold for how long ago the source or mirror should have been seen\").PlaceHolder(\"DURATION\").DurationVar(&c.sourcesSeenCritical) <nl> + stream.Flag(\"lag-critical\", \"Critical threshold to allow for lag on any source or mirror\").PlaceHolder(\"MSGS\").Uint64Var(&c.sourcesLagCritical) <nl> + stream.Flag(\"seen-critical\", \"Critical threshold for how long ago the source or mirror should have been seen\").PlaceHolder(\"DURATION\").DurationVar(&c.sourcesSeenCritical) <nl> stream.Flag(\"min-sources\", \"Minimum number of sources to expect\").PlaceHolder(\"SOURCES\").Default(\"1\").IntVar(&c.sourcesMinSources) <nl> stream.Flag(\"max-sources\", \"Maximum number of sources to expect\").PlaceHolder(\"SOURCES\").Default(\"1\").IntVar(&c.sourcesMaxSources) <nl> stream.Flag(\"peer-expect\", \"Number of cluster replicas to expect\").Required().PlaceHolder(\"SERVERS\").IntVar(&c.raftExpect) <nl> - stream.Flag(\"peer-lag-Critical\", \"Critical threshold to allow for cluster peer lag\").PlaceHolder(\"OPS\").Uint64Var(&c.raftLagCritical) <nl> - stream.Flag(\"peer-seen-Critical\", \"Critical threshold for how long ago a cluster peer should have been seen\").PlaceHolder(\"DURATION\").Default(\"10s\").DurationVar(&c.raftSeenCritical) <nl> + stream.Flag(\"peer-lag-critical\", \"Critical threshold to allow for cluster peer lag\").PlaceHolder(\"OPS\").Uint64Var(&c.raftLagCritical) <nl> + stream.Flag(\"peer-seen-critical\", \"Critical threshold for how long ago a cluster peer should have been seen\").PlaceHolder(\"DURATION\").Default(\"10s\").DurationVar(&c.raftSeenCritical) <nl> stream.Flag(\"msgs-warn\", \"Warn if there are fewer than this many messages in the stream\").PlaceHolder(\"MSGS\").Uint64Var(&c.sourcesMessagesWarn) <nl> - stream.Flag(\"msgs-Critical\", \"Critical if there are fewer than this many messages in the stream\").PlaceHolder(\"MSGS\").Uint64Var(&c.sourcesMessagesCrit) <nl> + stream.Flag(\"msgs-critical\", \"Critical if there are fewer than this many messages in the stream\").PlaceHolder(\"MSGS\").Uint64Var(&c.sourcesMessagesCrit) <nl> stream.Flag(\"subjects-warn\", \"Critical threshold for subjects in the stream\").PlaceHolder(\"SUBJECTS\").Default(\"-1\").IntVar(&c.subjectsWarn) <nl> - stream.Flag(\"subjects-Critical\", \"Warning threshold for subjects in the stream\").PlaceHolder(\"SUBJECTS\").Default(\"-1\").IntVar(&c.subjectsCrit) <nl> + stream.Flag(\"subjects-critical\", \"Warning threshold for subjects in the stream\").PlaceHolder(\"SUBJECTS\").Default(\"-1\").IntVar(&c.subjectsCrit) <nl> msg := check.Command(\"message\", \"Checks properties of a message stored in a stream\").Action(c.checkMsg) <nl> msg.Flag(\"stream\", \"The streams to check\").Required().StringVar(&c.sourcesStream) <nl> msg.Flag(\"subject\", \"The subject to fetch a message from\").Default(\">\").StringVar(&c.msgSubject) <nl> msg.Flag(\"age-warn\", \"Warning threshold for message age as a duration\").PlaceHolder(\"DURATION\").DurationVar(&c.msgAgeWarn) <nl> - msg.Flag(\"age-Critical\", \"Critical threshold for message age as a duration\").PlaceHolder(\"DURATION\").DurationVar(&c.msgAgeCrit) <nl> + msg.Flag(\"age-critical\", \"Critical threshold for message age as a duration\").PlaceHolder(\"DURATION\").DurationVar(&c.msgAgeCrit) <nl> msg.Flag(\"content\", \"Regular expression to check the content against\").PlaceHolder(\"REGEX\").RegexpVar(&c.msgRegexp) <nl> msg.Flag(\"body-timestamp\", \"Use message body as a unix timestamp instead of message metadata\").UnNegatableBoolVar(&c.msgBodyAsTs) <nl> meta := check.Command(\"meta\", \"Check JetStream cluster state\").Alias(\"raft\").Action(c.checkRaft) <nl> meta.Flag(\"expect\", \"Number of servers to expect\").Required().PlaceHolder(\"SERVERS\").IntVar(&c.raftExpect) <nl> - meta.Flag(\"lag-Critical\", \"Critical threshold to allow for lag\").PlaceHolder(\"OPS\").Required().Uint64Var(&c.raftLagCritical) <nl> - meta.Flag(\"seen-Critical\", \"Critical threshold for how long ago a peer should have been seen\").Required().PlaceHolder(\"DURATION\").DurationVar(&c.raftSeenCritical) <nl> + meta.Flag(\"lag-critical\", \"Critical threshold to allow for lag\").PlaceHolder(\"OPS\").Required().Uint64Var(&c.raftLagCritical) <nl> + meta.Flag(\"seen-critical\", \"Critical threshold for how long ago a peer should have been seen\").Required().PlaceHolder(\"DURATION\").DurationVar(&c.raftSeenCritical) <nl> js := check.Command(\"jetstream\", \"Check JetStream account state\").Alias(\"js\").Action(c.checkJS) <nl> js.Flag(\"mem-warn\", \"Warning threshold for memory storage, in percent\").Default(\"75\").IntVar(&c.jsMemWarn) <nl> - js.Flag(\"mem-Critical\", \"Critical threshold for memory storage, in percent\").Default(\"90\").IntVar(&c.jsMemCritical) <nl> + js.Flag(\"mem-critical\", \"Critical threshold for memory storage, in percent\").Default(\"90\").IntVar(&c.jsMemCritical) <nl> js.Flag(\"store-warn\", \"Warning threshold for disk storage, in percent\").Default(\"75\").IntVar(&c.jsStoreWarn) <nl> - js.Flag(\"store-Critical\", \"Critical threshold for memory storage, in percent\").Default(\"90\").IntVar(&c.jsStoreCritical) <nl> + js.Flag(\"store-critical\", \"Critical threshold for memory storage, in percent\").Default(\"90\").IntVar(&c.jsStoreCritical) <nl> js.Flag(\"streams-warn\", \"Warning threshold for number of streams used, in percent\").Default(\"-1\").IntVar(&c.jsStreamsWarn) <nl> - js.Flag(\"streams-Critical\", \"Critical threshold for number of streams used, in percent\").Default(\"-1\").IntVar(&c.jsStreamsCritical) <nl> + js.Flag(\"streams-critical\", \"Critical threshold for number of streams used, in percent\").Default(\"-1\").IntVar(&c.jsStreamsCritical) <nl> js.Flag(\"consumers-warn\", \"Warning threshold for number of consumers used, in percent\").Default(\"-1\").IntVar(&c.jsConsumersWarn) <nl> - js.Flag(\"consumers-Critical\", \"Critical threshold for number of consumers used, in percent\").Default(\"-1\").IntVar(&c.jsConsumersCritical) <nl> + js.Flag(\"consumers-critical\", \"Critical threshold for number of consumers used, in percent\").Default(\"-1\").IntVar(&c.jsConsumersCritical) <nl> js.Flag(\"replicas\", \"Checks if all streams have healthy replicas\").Default(\"true\").BoolVar(&c.jsReplicas) <nl> - js.Flag(\"replica-seen-Critical\", \"Critical threshold for when a stream replica should have been seen, as a duration\").Default(\"5s\").DurationVar(&c.jsReplicaSeenCritical) <nl> - js.Flag(\"replica-lag-Critical\", \"Critical threshold for how many operations behind a peer can be\").Default(\"200\").Uint64Var(&c.jsReplicaLagCritical) <nl> + js.Flag(\"replica-seen-critical\", \"Critical threshold for when a stream replica should have been seen, as a duration\").Default(\"5s\").DurationVar(&c.jsReplicaSeenCritical) <nl> + js.Flag(\"replica-lag-critical\", \"Critical threshold for how many operations behind a peer can be\").Default(\"200\").Uint64Var(&c.jsReplicaLagCritical) <nl> serv := check.Command(\"server\", \"Checks a NATS Server health\").Action(c.checkSrv) <nl> serv.Flag(\"name\", \"Server name to require in the result\").Required().StringVar(&c.srvName) <nl> serv.Flag(\"cpu-warn\", \"Warning threshold for CPU usage, in percent\").IntVar(&c.srvCPUWarn) <nl> - serv.Flag(\"cpu-Critical\", \"Critical threshold for CPU usage, in percent\").IntVar(&c.srvCPUCrit) <nl> + serv.Flag(\"cpu-critical\", \"Critical threshold for CPU usage, in percent\").IntVar(&c.srvCPUCrit) <nl> serv.Flag(\"mem-warn\", \"Warning threshold for Memory usage, in percent\").IntVar(&c.srvMemWarn) <nl> - serv.Flag(\"mem-Critical\", \"Critical threshold Memory CPU usage, in percent\").IntVar(&c.srvMemCrit) <nl> + serv.Flag(\"mem-critical\", \"Critical threshold Memory CPU usage, in percent\").IntVar(&c.srvMemCrit) <nl> serv.Flag(\"conn-warn\", \"Warning threshold for connections, supports inversion\").IntVar(&c.srvConnWarn) <nl> - serv.Flag(\"conn-Critical\", \"Critical threshold for connections, supports inversion\").IntVar(&c.srvConnCrit) <nl> + serv.Flag(\"conn-critical\", \"Critical threshold for connections, supports inversion\").IntVar(&c.srvConnCrit) <nl> serv.Flag(\"subs-warn\", \"Warning threshold for number of active subscriptions, supports inversion\").IntVar(&c.srvSubsWarn) <nl> - serv.Flag(\"subs-Critical\", \"Critical threshold for number of active subscriptions, supports inversion\").IntVar(&c.srvSubCrit) <nl> + serv.Flag(\"subs-critical\", \"Critical threshold for number of active subscriptions, supports inversion\").IntVar(&c.srvSubCrit) <nl> serv.Flag(\"uptime-warn\", \"Warning threshold for server uptime as duration\").DurationVar(&c.srvUptimeWarn) <nl> - serv.Flag(\"uptime-Critical\", \"Critical threshold for server uptime as duration\").DurationVar(&c.srvUptimeCrit) <nl> + serv.Flag(\"uptime-critical\", \"Critical threshold for server uptime as duration\").DurationVar(&c.srvUptimeCrit) <nl> serv.Flag(\"auth-required\", \"Checks that authentication is enabled\").UnNegatableBoolVar(&c.srvAuthRequire) <nl> serv.Flag(\"tls-required\", \"Checks that TLS is required\").UnNegatableBoolVar(&c.srvTLSRequired) <nl> serv.Flag(\"js-required\", \"Checks that JetStream is enabled\").UnNegatableBoolVar(&c.srvJSRequired) <nl> kv := check.Command(\"kv\", \"Checks a NATS KV Bucket\").Action(c.checkKV) <nl> kv.Flag(\"bucket\", \"Checks a specific bucket\").Required().StringVar(&c.kvBucket) <nl> - kv.Flag(\"values-Critical\", \"Critical threshold for number of values in the bucket\").Default(\"-1\").IntVar(&c.kvValuesCrit) <nl> + kv.Flag(\"values-critical\", \"Critical threshold for number of values in the bucket\").Default(\"-1\").IntVar(&c.kvValuesCrit) <nl> kv.Flag(\"values-warn\", \"Warning threshold for number of values in the bucket\").Default(\"-1\").IntVar(&c.kvValuesWarn) <nl> kv.Flag(\"key\", \"Requires a key to have any non-delete value set\").StringVar(&c.kvKey) <nl> } <nl> ", "msg": "revert unintended changes to server check flags"}
{"diff_id": 756, "repo": "nats-io/natscli", "sha": "da475b26c15afcced444406646132a9045f3e8f2", "time": "28.01.2023 12:43:54", "diff": "mmm a / cli/server_report_command.go <nl> ppp b / cli/server_report_command.go <nl>@@ -83,7 +83,15 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> return err <nl> } <nl> - req := &server.JszEventOptions{JSzOptions: server.JSzOptions{Account: c.account}, EventFilterOptions: c.reqFilter()} <nl> + jszOpts := server.JSzOptions{} <nl> + if c.account != \"\" { <nl> + jszOpts.Account = c.account <nl> + jszOpts.Streams = true <nl> + jszOpts.Consumer = true <nl> + jszOpts.Limit = 10000 <nl> + } <nl> + <nl> + req := &server.JszEventOptions{JSzOptions: jszOpts, EventFilterOptions: c.reqFilter()} <nl> res, err := doReq(req, \"$SYS.REQ.SERVER.PING.JSZ\", c.waitFor, nc) <nl> if err != nil { <nl> return err <nl> @@ -94,16 +102,6 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> Server server.ServerInfo `json:\"server\"` <nl> } <nl> - // works around 2.7.0 breaking chances <nl> - type jszrCompact struct { <nl> - Data struct { <nl> - Streams int `json:\"total_streams,omitempty\"` <nl> - Consumers int `json:\"total_consumers,omitempty\"` <nl> - Messages uint64 `json:\"total_messages,omitempty\"` <nl> - Bytes uint64 `json:\"total_message_bytes,omitempty\"` <nl> - } `json:\"data\"` <nl> - } <nl> - <nl> var ( <nl> names []string <nl> jszResponses []*jszr <nl> @@ -128,19 +126,6 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> return err <nl> } <nl> - // we may have a pre 2.7.0 machine and will try get data with old struct names, if all of these are <nl> - // 0 it might be that they are 0 or that we had data in the old format, so we try parse the old <nl> - // and set what is in there. If it's not an old server 0s will stay 0s, otherwise we pull in old format values <nl> - if response.Data.Streams == 0 && response.Data.Consumers == 0 && response.Data.Messages == 0 && response.Data.Bytes == 0 { <nl> - cresp := jszrCompact{} <nl> - if json.Unmarshal(r, &cresp) == nil { <nl> - response.Data.Streams = cresp.Data.Streams <nl> - response.Data.Consumers = cresp.Data.Consumers <nl> - response.Data.Messages = cresp.Data.Messages <nl> - response.Data.Bytes = cresp.Data.Bytes <nl> - } <nl> - } <nl> - <nl> if response.Data.Config.Domain != \"\" { <nl> renderDomain = true <nl> } <nl> @@ -177,7 +162,7 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> return fmt.Errorf(\"no results received, ensure the account used has system privileges and appropriate permissions\") <nl> } <nl> - // here so its after the sort <nl> + // here so it's after the sort <nl> for _, js := range jszResponses { <nl> names = append(names, js.Server.Name) <nl> } <nl> @@ -202,14 +187,48 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> } <nl> for i, js := range jszResponses { <nl> - apiErr += js.Data.JetStreamStats.API.Errors <nl> - apiTotal += js.Data.JetStreamStats.API.Total <nl> - memory += js.Data.JetStreamStats.Memory <nl> - store += js.Data.JetStreamStats.Store <nl> + jss := js.Data.JetStreamStats <nl> + var acc *server.AccountDetail <nl> + var doAccountStats bool <nl> + <nl> + if c.account != \"\" && len(js.Data.AccountDetails) == 1 { <nl> + acc = js.Data.AccountDetails[0] <nl> + jss = acc.JetStreamStats <nl> + doAccountStats = true <nl> + } <nl> + <nl> + apiErr += jss.API.Errors <nl> + apiTotal += jss.API.Total <nl> + memory += jss.Memory <nl> + store += jss.Store <nl> + <nl> + rStreams := 0 <nl> + rConsumers := 0 <nl> + rMessages := uint64(0) <nl> + rBytes := uint64(0) <nl> + <nl> + if doAccountStats { <nl> + rBytes = acc.Memory + acc.Store <nl> + bytes += rBytes <nl> + rStreams = len(acc.Streams) <nl> + streams += rStreams <nl> + <nl> + for _, sd := range acc.Streams { <nl> + consumers += sd.State.Consumers <nl> + rConsumers += sd.State.Consumers <nl> + msgs += sd.State.Msgs <nl> + rMessages += sd.State.Msgs <nl> + } <nl> + } else { <nl> consumers += js.Data.Consumers <nl> + rConsumers = js.Data.Consumers <nl> streams += js.Data.Streams <nl> + rStreams = js.Data.Streams <nl> bytes += js.Data.Bytes <nl> + rBytes = js.Data.Bytes <nl> msgs += js.Data.Messages <nl> + rMessages = js.Data.Messages <nl> + } <nl> leader := \"\" <nl> if js.Data.Meta != nil { <nl> @@ -227,14 +246,14 @@ func (c *SrvReportCmd) reportJetStream(_ *fisk.ParseContext) error { <nl> row = append(row, js.Data.Config.Domain) <nl> } <nl> row = append(row, <nl> - humanize.Comma(int64(js.Data.Streams)), <nl> - humanize.Comma(int64(js.Data.Consumers)), <nl> - humanize.Comma(int64(js.Data.Messages)), <nl> - humanize.IBytes(js.Data.Bytes), <nl> - humanize.IBytes(js.Data.JetStreamStats.Memory), <nl> - humanize.IBytes(js.Data.JetStreamStats.Store), <nl> - humanize.Comma(int64(js.Data.JetStreamStats.API.Total)), <nl> - humanize.Comma(int64(js.Data.JetStreamStats.API.Errors))) <nl> + humanize.Comma(int64(rStreams)), <nl> + humanize.Comma(int64(rConsumers)), <nl> + humanize.Comma(int64(rMessages)), <nl> + humanize.IBytes(rBytes), <nl> + humanize.IBytes(jss.Memory), <nl> + humanize.IBytes(jss.Store), <nl> + humanize.Comma(int64(jss.API.Total)), <nl> + humanize.Comma(int64(jss.API.Errors))) <nl> table.AddRow(row...) <nl> } <nl> ", "msg": "scope report jsz to a specific account when requested"}
{"diff_id": 757, "repo": "nats-io/natscli", "sha": "a2a99348ff9695aadbfef7df2316a266148e6a36", "time": "10.02.2023 16:45:07", "diff": "mmm a / cli/server_account_command.go <nl> ppp b / cli/server_account_command.go <nl>@@ -206,6 +206,7 @@ func (c *srvAccountCommand) renderExport(exp *server.ExtExport) { <nl> } else { <nl> fmt.Printf(\" Accounts:\\n\") <nl> } <nl> + fmt.Println() <nl> var list []string <nl> var cnt int <nl> @@ -228,6 +229,7 @@ func (c *srvAccountCommand) renderExport(exp *server.ExtExport) { <nl> } else { <nl> fmt.Printf(\" Revocations:\\n\") <nl> } <nl> + fmt.Println() <nl> var list []string <nl> var cnt int <nl> ", "msg": "minor ux updates for server account info"}
{"diff_id": 770, "repo": "rqlite/rqlite", "sha": "a61f05985b39d286ba55a7cfcb2de83efcdfd172", "time": "19.02.2017 13:08:46", "diff": "mmm a / cmd/rqlited/main.go <nl> ppp b / cmd/rqlited/main.go <nl>@@ -280,24 +280,31 @@ func main() { <nl> } <nl> func determineJoinAddresses() ([]string, error) { <nl> - advAddr := raftAddr <nl> - if raftAdv != \"\" { <nl> - advAddr = raftAdv <nl> + apiAdv := httpAddr <nl> + if httpAdv != \"\" { <nl> + apiAdv = httpAdv <nl> + } <nl> + <nl> + var addrs []string <nl> + if joinAddr != \"\" { <nl> + // An explicit join address is first priority. <nl> + addrs = append(addrs, joinAddr) <nl> } <nl> - addrs := []string{} <nl> if discoID != \"\" { <nl> c := disco.New(discoURL) <nl> - r, err := c.Register(discoID, advAddr) <nl> + r, err := c.Register(discoID, apiAdv) <nl> if err != nil { <nl> return nil, err <nl> } <nl> - addrs = append(addrs, r.Nodes...) <nl> + for _, a := range r.Nodes { <nl> + if a != apiAdv { <nl> + // Only other nodes can be joined. <nl> + addrs = append(addrs, a) <nl> + } <nl> } <nl> - <nl> - if joinAddr != \"\" { <nl> - addrs = append([]string{joinAddr}, addrs...) <nl> } <nl> + <nl> return addrs, nil <nl> } <nl> ", "msg": "Supply HTTP advertised address to disco service"}
{"diff_id": 774, "repo": "rqlite/rqlite", "sha": "2b6332a21072548bd41e56d5ce5923801d32632c", "time": "22.02.2017 09:54:32", "diff": "mmm a / disco/client.go <nl> ppp b / disco/client.go <nl>@@ -74,7 +74,7 @@ func (c *Client) Register(id, addr string) (*Response, error) { <nl> c.logger.Printf(\"discovery client successfully registered %s at %s\", addr, url) <nl> return r, nil <nl> case http.StatusMovedPermanently: <nl> - url = c.registrationURL(resp.Header.Get(\"location\"), id) <nl> + url = resp.Header.Get(\"location\") <nl> c.logger.Println(\"discovery client redirecting to\", url) <nl> continue <nl> default: <nl> ", "msg": "Fix reference to redirect URL in disco client"}
{"diff_id": 784, "repo": "rqlite/rqlite", "sha": "4b9220c1075f84122b1d3274a73302ee1d22a8f5", "time": "25.02.2017 09:41:24", "diff": "mmm a / cmd/rqlited/main.go <nl> ppp b / cmd/rqlited/main.go <nl>@@ -227,6 +227,8 @@ func main() { <nl> } else { <nl> log.Println(\"node is already member of cluster, ignoring any join requests\") <nl> } <nl> + } else { <nl> + log.Println(\"no join addresses available\") <nl> } <nl> // Publish to the cluster the mapping between this Raft address and API address. <nl> ", "msg": "More join logging"}
{"diff_id": 801, "repo": "rqlite/rqlite", "sha": "780ab93e6e2a77672f7e18c1dedf01f24e0fe31d", "time": "11.06.2017 21:42:50", "diff": "mmm a / tcp/mux_test.go <nl> ppp b / tcp/mux_test.go <nl>@@ -2,6 +2,7 @@ package tcp <nl> import ( <nl> \"bytes\" <nl> + \"crypto/tls\" <nl> \"io\" <nl> \"io/ioutil\" <nl> \"log\" <nl> @@ -176,10 +177,24 @@ func TestTLSMux(t *testing.T) { <nl> key := x509.KeyFile() <nl> defer os.Remove(key) <nl> - _, err := NewTLSMux(tcpListener, nil, cert, key) <nl> + mux, err := NewTLSMux(tcpListener, nil, cert, key) <nl> if err != nil { <nl> t.Fatalf(\"failed to create mux: %s\", err.Error()) <nl> } <nl> + go mux.Serve() <nl> + <nl> + // Verify that the listener is secured. <nl> + conn, err := tls.Dial(\"tcp\", tcpListener.Addr().String(), &tls.Config{ <nl> + InsecureSkipVerify: true, <nl> + }) <nl> + if err != nil { <nl> + t.Fatal(err) <nl> + } <nl> + <nl> + state := conn.ConnectionState() <nl> + if !state.HandshakeComplete { <nl> + t.Fatal(\"connection handshake failed to complete\") <nl> + } <nl> } <nl> func TestTLSMux_Fail(t *testing.T) { <nl> ", "msg": "More unit testing of TLS mux"}
{"diff_id": 814, "repo": "rqlite/rqlite", "sha": "6d644a4f645f3f263c296aa892a177f572a879e4", "time": "29.08.2017 10:10:07", "diff": "mmm a / system_test/helpers.go <nl> ppp b / system_test/helpers.go <nl>@@ -94,6 +94,9 @@ func (n *Node) Status() (string, error) { <nl> if err != nil { <nl> return \"\", err <nl> } <nl> + if resp.StatusCode != 200 { <nl> + return \"\", fmt.Errorf(\"status endpoint returned: %s\", resp.Status) <nl> + } <nl> defer resp.Body.Close() <nl> body, err := ioutil.ReadAll(resp.Body) <nl> if err != nil { <nl> @@ -110,6 +113,9 @@ func (n *Node) Expvar() (string, error) { <nl> if err != nil { <nl> return \"\", err <nl> } <nl> + if resp.StatusCode != 200 { <nl> + return \"\", fmt.Errorf(\"expvar endpoint returned: %s\", resp.Status) <nl> + } <nl> defer resp.Body.Close() <nl> body, err := ioutil.ReadAll(resp.Body) <nl> if err != nil { <nl> @@ -286,6 +292,7 @@ func mustNewNode(enableSingle bool) *Node { <nl> node.RaftAddr = node.Store.Addr().String() <nl> node.Service = httpd.New(\"localhost:0\", node.Store, nil) <nl> + node.Service.Expvar = true <nl> if err := node.Service.Start(); err != nil { <nl> node.Deprovision() <nl> panic(fmt.Sprintf(\"failed to start HTTP server: %s\", err.Error())) <nl> ", "msg": "Enable expvar endpoint for testing"}
{"diff_id": 819, "repo": "rqlite/rqlite", "sha": "0fe34905ffc62fefcda66b5ad44ee5d1c9a06b95", "time": "27.10.2017 13:43:31", "diff": "mmm a / cmd/rqlite/main.go <nl> ppp b / cmd/rqlite/main.go <nl>@@ -133,6 +133,10 @@ func sendRequest(ctx *cli.Context, urlStr string, line string, argv *argT, ret i <nl> } <nl> defer resp.Body.Close() <nl> + if resp.StatusCode == http.StatusUnauthorized { <nl> + return fmt.Errorf(\"unauthorized\") <nl> + } <nl> + <nl> // Check for redirect. <nl> if resp.StatusCode == http.StatusMovedPermanently { <nl> nRedirect++ <nl> @@ -190,6 +194,10 @@ func cliJSON(ctx *cli.Context, cmd, line, url string, argv *argT) error { <nl> } <nl> defer resp.Body.Close() <nl> + if resp.StatusCode == http.StatusUnauthorized { <nl> + return fmt.Errorf(\"unauthorized\") <nl> + } <nl> + <nl> body, err := ioutil.ReadAll(resp.Body) <nl> if err != nil { <nl> return err <nl> ", "msg": "CLI checks for HTTP 401\nThe CLI doesn't yet support passing authentication credentials, but this change means the user will know what is happening."}
{"diff_id": 820, "repo": "rqlite/rqlite", "sha": "9f32f026e28df8abfa470aeb0d96e79c9a1c6cbd", "time": "31.10.2017 10:35:49", "diff": "mmm a / cmd/rqbench/main.go <nl> ppp b / cmd/rqbench/main.go <nl>@@ -15,6 +15,7 @@ import ( <nl> var addr string <nl> var numReqs int <nl> var batchSz int <nl> +var modPrint int <nl> var tx bool <nl> var tp string <nl> @@ -25,6 +26,7 @@ func init() { <nl> flag.StringVar(&addr, \"a\", \"localhost:4001\", \"Node address\") <nl> flag.IntVar(&numReqs, \"n\", 100, \"Number of requests\") <nl> flag.IntVar(&batchSz, \"b\", 1, \"Statements per request\") <nl> + flag.IntVar(&modPrint, \"m\", 0, \"Print progress every m requests\") <nl> flag.BoolVar(&tx, \"x\", false, \"Use explicit transaction per request\") <nl> flag.StringVar(&tp, \"t\", \"http\", \"Transport to use\") <nl> flag.Usage = func() { <nl> @@ -73,11 +75,16 @@ func run(t Tester, n int) (time.Duration, error) { <nl> var dur time.Duration <nl> for i := 0; i < n; i++ { <nl> - if d, err := t.Once(); err != nil { <nl> + d, err := t.Once() <nl> + if err != nil { <nl> return 0, err <nl> } else { <nl> dur += d <nl> } <nl> + <nl> + if modPrint != 0 && i != 0 && i%modPrint == 0 { <nl> + fmt.Printf(\"%d requests completed in %s\\n\", i, d) <nl> + } <nl> } <nl> return dur, nil <nl> } <nl> ", "msg": "More informative output during load testing"}
{"diff_id": 854, "repo": "rqlite/rqlite", "sha": "6e6f12fdd6757e897e39bfcb33410b490a341671", "time": "26.09.2020 21:28:20", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -458,7 +458,12 @@ func (s *Service) handleLoad(w http.ResponseWriter, r *http.Request) { <nl> } <nl> r.Body.Close() <nl> - queries := []string{string(b)} <nl> + queries, err := ParseRequest(b) <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> + return <nl> + } <nl> + <nl> results, err := s.store.ExecuteOrAbort(&store.ExecuteRequest{queries, timings, false}) <nl> if err != nil { <nl> if err == store.ErrNotLeader { <nl> @@ -601,13 +606,19 @@ func (s *Service) handleExecute(w http.ResponseWriter, r *http.Request) { <nl> } <nl> r.Body.Close() <nl> + stmts, err := ParseRequest(b) <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> + return <nl> + } <nl> + <nl> queries := []string{} <nl> if err := json.Unmarshal(b, &queries); err != nil { <nl> http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> } <nl> - results, err := s.store.Execute(&store.ExecuteRequest{queries, timings, isTx}) <nl> + results, err := s.store.Execute(&store.ExecuteRequest{stmts, timings, isTx}) <nl> if err != nil { <nl> if err == store.ErrNotLeader { <nl> leaderAPIAddr := s.LeaderAPIAddr() <nl> @@ -809,29 +820,24 @@ func (s *Service) checkCredentials(r *http.Request) bool { <nl> return ok && s.credentialStore.Check(username, password) <nl> } <nl> -func requestQueries(r *http.Request) ([]string, error) { <nl> +func requestQueries(r *http.Request) ([]store.Statement, error) { <nl> if r.Method == \"GET\" { <nl> query, err := stmtParam(r) <nl> if err != nil || query == \"\" { <nl> return nil, errors.New(\"bad query GET request\") <nl> } <nl> - return []string{query}, nil <nl> + return []store.Statement{ <nl> + store.Statement{query, nil}, <nl> + }, nil <nl> } <nl> - qs := []string{} <nl> b, err := ioutil.ReadAll(r.Body) <nl> if err != nil { <nl> return nil, errors.New(\"bad query POST request\") <nl> } <nl> r.Body.Close() <nl> - if err := json.Unmarshal(b, &qs); err != nil { <nl> - return nil, errors.New(\"bad query POST request\") <nl> - } <nl> - if len(qs) == 0 { <nl> - return nil, errors.New(\"bad query POST request\") <nl> - } <nl> - return qs, nil <nl> + return ParseRequest(b) <nl> } <nl> func writeResponse(w http.ResponseWriter, r *http.Request, j *Response) { <nl> ", "msg": "HTTP API now supports multi-format\nIf now supports an array of strings, as before, and also supports arrays\nof arrays, where each inner array contains a statement, and parameters\nfor binding to that statement."}
{"diff_id": 857, "repo": "rqlite/rqlite", "sha": "1480d8898fb02ef2f7fa864228b08614a1760a0a", "time": "27.09.2020 09:27:48", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -1121,12 +1121,18 @@ func subCommandToStatements(d *databaseSub) []sql.Statement { <nl> stmts := make([]sql.Statement, len(d.Queries)) <nl> for i := range d.Queries { <nl> stmts[i].Query = d.Queries[i] <nl> - stmts[i].Parameters = make([]gosql.Value, len(d.Parameters[i])) <nl> + // Support backwards-compatibility, since previous versions didn't <nl> + // have Parameters in Raft commands. <nl> + if len(d.Parameters) == 0 { <nl> + stmts[i].Parameters = make([]gosql.Value, 0) <nl> + } else { <nl> + stmts[i].Parameters = make([]gosql.Value, len(d.Parameters[i])) <nl> for j := range d.Parameters[i] { <nl> stmts[i].Parameters[j] = d.Parameters[i][j] <nl> } <nl> } <nl> + } <nl> return stmts <nl> } <nl> ", "msg": "Handle Raft commands missing Parameterized values\nThis allows this code to run with Raft logs from previous versions."}
{"diff_id": 863, "repo": "rqlite/rqlite", "sha": "d47c290cda0a2d75c8af8f14536cad6d433424a5", "time": "29.09.2020 09:00:29", "diff": "mmm a / cmd/rqlite/backup.go <nl> ppp b / cmd/rqlite/backup.go <nl>@@ -4,7 +4,6 @@ import ( <nl> \"bytes\" <nl> \"crypto/tls\" <nl> \"fmt\" <nl> - \"io\" <nl> \"io/ioutil\" <nl> \"net/http\" <nl> \"net/url\" <nl> @@ -86,9 +85,9 @@ func dump(ctx *cli.Context, filename string, argv *argT) error { <nl> return nil <nl> } <nl> -func makeRestoreRequest(restoreFile io.Reader) func(string) (*http.Request, error) { <nl> +func makeRestoreRequest(b []byte) func(string) (*http.Request, error) { <nl> return func(urlStr string) (*http.Request, error) { <nl> - req, err := http.NewRequest(\"POST\", urlStr, restoreFile) <nl> + req, err := http.NewRequest(\"POST\", urlStr, bytes.NewReader(b)) <nl> req.Header[\"Content-type\"] = []string{\"text/plain\"} <nl> if err != nil { <nl> return nil, err <nl> @@ -158,8 +157,7 @@ func restore(ctx *cli.Context, filename string, argv *argT) error { <nl> Path: fmt.Sprintf(\"%sdb/load\", argv.Prefix), <nl> RawQuery: queryStr.Encode(), <nl> } <nl> - restoreFileReader := bytes.NewReader(restoreFile) <nl> - response, err := sendRequest(ctx, makeRestoreRequest(restoreFileReader), restoreURL.String(), argv) <nl> + response, err := sendRequest(ctx, makeRestoreRequest(restoreFile), restoreURL.String(), argv) <nl> if err != nil { <nl> return err <nl> } <nl> ", "msg": "Restore request should re-read file every attempt\nIt can now handle HTTP 301 redirects properly."}
{"diff_id": 868, "repo": "rqlite/rqlite", "sha": "c4b4cf2a5197929fe188eacb8dc213f7c558928c", "time": "13.11.2020 09:39:27", "diff": "mmm a / store/store_test.go <nl> ppp b / store/store_test.go <nl>@@ -11,6 +11,7 @@ import ( <nl> \"testing\" <nl> \"time\" <nl> + sql \"github.com/rqlite/rqlite/db\" <nl> \"github.com/rqlite/rqlite/testdata/chinook\" <nl> ) <nl> @@ -121,24 +122,46 @@ func Test_SingleNodeFileExecuteQuery(t *testing.T) { <nl> if err != nil { <nl> t.Fatalf(\"failed to execute on single node: %s\", err.Error()) <nl> } <nl> + <nl> + // Every query should return the same results, so use a function for the check. <nl> + check := func(r []*sql.Rows) { <nl> + if exp, got := `[\"id\",\"name\"]`, asJSON(r[0].Columns); exp != got { <nl> + t.Fatalf(\"unexpected results for query\\nexp: %s\\ngot: %s\", exp, got) <nl> + } <nl> + if exp, got := `[[1,\"fiona\"]]`, asJSON(r[0].Values); exp != got { <nl> + t.Fatalf(\"unexpected results for query\\nexp: %s\\ngot: %s\", exp, got) <nl> + } <nl> + } <nl> + <nl> r, err := s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, false, None, 0}) <nl> if err != nil { <nl> t.Fatalf(\"failed to query single node: %s\", err.Error()) <nl> } <nl> - r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, false, None, 0}) <nl> + check(r) <nl> + <nl> + r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, false, Weak, 0}) <nl> if err != nil { <nl> t.Fatalf(\"failed to query single node: %s\", err.Error()) <nl> } <nl> - r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, false, None, 0}) <nl> + check(r) <nl> + <nl> + r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, false, Strong, 0}) <nl> if err != nil { <nl> t.Fatalf(\"failed to query single node: %s\", err.Error()) <nl> } <nl> - if exp, got := `[\"id\",\"name\"]`, asJSON(r[0].Columns); exp != got { <nl> - t.Fatalf(\"unexpected results for query\\nexp: %s\\ngot: %s\", exp, got) <nl> + check(r) <nl> + <nl> + r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), false, true, None, 0}) <nl> + if err != nil { <nl> + t.Fatalf(\"failed to query single node: %s\", err.Error()) <nl> } <nl> - if exp, got := `[[1,\"fiona\"]]`, asJSON(r[0].Values); exp != got { <nl> - t.Fatalf(\"unexpected results for query\\nexp: %s\\ngot: %s\", exp, got) <nl> + check(r) <nl> + <nl> + r, err = s.Query(&QueryRequest{stmtsFromString(\"SELECT * FROM foo\"), true, false, None, 0}) <nl> + if err != nil { <nl> + t.Fatalf(\"failed to query single node: %s\", err.Error()) <nl> } <nl> + check(r) <nl> } <nl> func Test_SingleNodeExecuteQueryTx(t *testing.T) { <nl> ", "msg": "Perform more testing of queries"}
{"diff_id": 871, "repo": "rqlite/rqlite", "sha": "f96c443a9c25b44d59f771575e2d60766cb6cc6f", "time": "26.11.2020 15:09:00", "diff": "mmm a / cluster/join.go <nl> ppp b / cluster/join.go <nl>@@ -8,7 +8,6 @@ import ( <nl> \"fmt\" <nl> \"io/ioutil\" <nl> \"log\" <nl> - \"net\" <nl> \"net/http\" <nl> \"os\" <nl> \"strings\" <nl> @@ -55,12 +54,6 @@ func join(joinAddr, id, addr string, voter bool, meta map[string]string, tlsConf <nl> return \"\", fmt.Errorf(\"node ID not set\") <nl> } <nl> - // Join using IP address, as that is what Hashicorp Raft works in. <nl> - resv, err := net.ResolveTCPAddr(\"tcp\", addr) <nl> - if err != nil { <nl> - return \"\", err <nl> - } <nl> - <nl> // Check for protocol scheme, and insert default if necessary. <nl> fullAddr := httpd.NormalizeAddr(fmt.Sprintf(\"%s/join\", joinAddr)) <nl> @@ -76,7 +69,7 @@ func join(joinAddr, id, addr string, voter bool, meta map[string]string, tlsConf <nl> for { <nl> b, err := json.Marshal(map[string]interface{}{ <nl> \"id\": id, <nl> - \"addr\": resv.String(), <nl> + \"addr\": addr, <nl> \"voter\": voter, <nl> \"meta\": meta, <nl> }) <nl> ", "msg": "Do not use resolved address in join request\nIt's not clear why this code was put in place in the first place, but it\nmay not be necessary."}
{"diff_id": 874, "repo": "rqlite/rqlite", "sha": "499df692be89ea0e81ed59c4599e572f6790cfdb", "time": "28.12.2020 18:07:07", "diff": "mmm a / cmd/rqlited/main.go <nl> ppp b / cmd/rqlited/main.go <nl>@@ -130,7 +130,7 @@ func init() { <nl> flag.StringVar(&memProfile, \"mem-profile\", \"\", \"Path to file for memory profiling information\") <nl> flag.Usage = func() { <nl> fmt.Fprintf(os.Stderr, \"\\n%s\\n\\n\", desc) <nl> - fmt.Fprintf(os.Stderr, \"Usage: %s [arguments] <data directory>\\n\", name) <nl> + fmt.Fprintf(os.Stderr, \"Usage: %s [flags] <data directory>\\n\", name) <nl> flag.PrintDefaults() <nl> } <nl> } <nl> @@ -145,8 +145,14 @@ func main() { <nl> } <nl> // Ensure the data path is set. <nl> - if flag.NArg() == 0 { <nl> - flag.Usage() <nl> + if flag.NArg() < 1 { <nl> + fmt.Fprintf(os.Stderr, \"fatal: no data directory set\\n\") <nl> + os.Exit(1) <nl> + } <nl> + <nl> + // Ensure no args come after the data directory. <nl> + if flag.NArg() > 1 { <nl> + fmt.Fprintf(os.Stderr, \"fatal: arguments after data directory are not accepted\\n\") <nl> os.Exit(1) <nl> } <nl> ", "msg": "Exit if arguments are passed after data directory\nFixes issue"}
{"diff_id": 875, "repo": "rqlite/rqlite", "sha": "ab2338733c32d89a9c281d543592e784e163e261", "time": "28.12.2020 18:48:17", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>package http <nl> import ( <nl> - \"bytes\" <nl> \"crypto/tls\" <nl> \"crypto/x509\" <nl> \"encoding/json\" <nl> @@ -343,9 +342,7 @@ func (s *Service) handleJoin(w http.ResponseWriter, r *http.Request) { <nl> return <nl> } <nl> - b := bytes.NewBufferString(err.Error()) <nl> - w.WriteHeader(http.StatusInternalServerError) <nl> - w.Write(b.Bytes()) <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> return <nl> } <nl> } <nl> @@ -398,7 +395,7 @@ func (s *Service) handleRemove(w http.ResponseWriter, r *http.Request) { <nl> return <nl> } <nl> - w.WriteHeader(http.StatusInternalServerError) <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> return <nl> } <nl> } <nl> @@ -518,7 +515,7 @@ func (s *Service) handleStatus(w http.ResponseWriter, r *http.Request) { <nl> results, err := s.store.Stats() <nl> if err != nil { <nl> - w.WriteHeader(http.StatusInternalServerError) <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> return <nl> } <nl> @@ -582,7 +579,8 @@ func (s *Service) handleStatus(w http.ResponseWriter, r *http.Request) { <nl> } else { <nl> _, err = w.Write([]byte(b)) <nl> if err != nil { <nl> - w.WriteHeader(http.StatusInternalServerError) <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> + return <nl> } <nl> } <nl> } <nl> ", "msg": "Include more information with HTTP 500"}
{"diff_id": 896, "repo": "rqlite/rqlite", "sha": "33aa4522cfb6e45c86098b47dbfca07376ce1c5d", "time": "31.01.2021 23:25:35", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -125,6 +125,7 @@ type Store struct { <nl> boltStore *raftboltdb.BoltStore // Physical store. <nl> lastIdxOnOpen uint64 // Last index on log when Store opens. <nl> + firstLogAppliedT time.Time // Time first log is applied <nl> appliedOnOpen uint64 // Number of logs applied at open. <nl> openT time.Time // Timestamp when Store opens. <nl> @@ -881,11 +882,16 @@ func (s *Store) Apply(l *raft.Log) interface{} { <nl> if l.Index <= s.lastIdxOnOpen { <nl> s.appliedOnOpen++ <nl> if l.Index == s.lastIdxOnOpen { <nl> - s.logger.Printf(\"%d committed log entries applied in %s\", <nl> - s.appliedOnOpen, time.Since(s.openT)) <nl> + s.logger.Printf(\"%d committed log entries applied in %s, took %s since open\", <nl> + s.appliedOnOpen, time.Since(s.firstLogAppliedT), time.Since(s.openT)) <nl> } <nl> } <nl> }() <nl> + <nl> + if s.firstLogAppliedT.IsZero() { <nl> + s.firstLogAppliedT = time.Now() <nl> + } <nl> + <nl> var c command.Command <nl> if err := legacy.Unmarshal(l.Data, &c); err != nil { <nl> ", "msg": "Better timing from first to last log"}
{"diff_id": 901, "repo": "rqlite/rqlite", "sha": "aa2fec4a1fda6244c22967313197a5e3b438aa6a", "time": "04.02.2021 09:19:51", "diff": "mmm a / store/store_test.go <nl> ppp b / store/store_test.go <nl>@@ -1261,10 +1261,7 @@ func Test_State(t *testing.T) { <nl> } <nl> } <nl> -func mustNewStore(inmem bool) *Store { <nl> - path := mustTempDir() <nl> - defer os.RemoveAll(path) <nl> - <nl> +func mustNewStoreAtPath(path string, inmem bool) *Store { <nl> cfg := NewDBConfig(\"\", inmem) <nl> s := New(mustMockLister(\"localhost:0\"), &StoreConfig{ <nl> DBConf: cfg, <nl> @@ -1277,6 +1274,10 @@ func mustNewStore(inmem bool) *Store { <nl> return s <nl> } <nl> +func mustNewStore(inmem bool) *Store { <nl> + return mustNewStoreAtPath(mustTempDir(), inmem) <nl> +} <nl> + <nl> type mockSnapshotSink struct { <nl> *os.File <nl> } <nl> ", "msg": "More useful store test helper functions"}
{"diff_id": 906, "repo": "rqlite/rqlite", "sha": "1dc156b96577bf352eff27e570742c5a6ee5fe91", "time": "06.02.2021 16:52:18", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -498,6 +498,11 @@ func (s *Store) Stats() (map[string]interface{}, error) { <nl> return nil, err <nl> } <nl> + dirSz, err := dirSize(s.raftDir) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + <nl> status := map[string]interface{}{ <nl> \"node_id\": s.raftID, <nl> \"raft\": raftStats, <nl> @@ -516,6 +521,7 @@ func (s *Store) Stats() (map[string]interface{}, error) { <nl> \"metadata\": s.meta, <nl> \"nodes\": nodes, <nl> \"dir\": s.raftDir, <nl> + \"dir_size\": dirSz, <nl> \"sqlite3\": dbStatus, <nl> \"db_conf\": s.dbConf, <nl> } <nl> @@ -1421,3 +1427,18 @@ func pathExists(p string) bool { <nl> } <nl> return true <nl> } <nl> + <nl> +// dirSize returns the total size of all files in the given directory <nl> +func dirSize(path string) (int64, error) { <nl> + var size int64 <nl> + err := filepath.Walk(path, func(_ string, info os.FileInfo, err error) error { <nl> + if err != nil { <nl> + return err <nl> + } <nl> + if !info.IsDir() { <nl> + size += info.Size() <nl> + } <nl> + return err <nl> + }) <nl> + return size, err <nl> +} <nl> ", "msg": "Add total Raft directory size to Store stats"}
{"diff_id": 907, "repo": "rqlite/rqlite", "sha": "73663ccc58ca62dcec5d9f66d187e2867673ea97", "time": "06.02.2021 19:10:32", "diff": "mmm a / system_test/helpers.go <nl> ppp b / system_test/helpers.go <nl>@@ -54,7 +54,7 @@ func (n *Node) Close(graceful bool) error { <nl> // Deprovision shuts down and removes all resources associated with the node. <nl> func (n *Node) Deprovision() { <nl> - n.Store.Close(false) <nl> + n.Store.Close(true) <nl> n.Service.Close() <nl> os.RemoveAll(n.Dir) <nl> } <nl> ", "msg": "Shutdown gracefully to prevent races\nSee"}
{"diff_id": 912, "repo": "rqlite/rqlite", "sha": "7c885d52b95845a1b78caa919bda591b967be9de", "time": "12.04.2021 15:31:47", "diff": "mmm a / command/marshal.go <nl> ppp b / command/marshal.go <nl>@@ -182,19 +182,22 @@ func UnmarshalSubCommand(c *Command, m proto.Message) error { <nl> if c.Compressed { <nl> gz, err := gzip.NewReader(bytes.NewReader(b)) <nl> if err != nil { <nl> - return err <nl> + fmt.Errorf(\"unmarshal sub gzip NewReader: %s\", err) <nl> } <nl> ub, err := ioutil.ReadAll(gz) <nl> if err != nil { <nl> - return err <nl> + fmt.Errorf(\"unmarshal sub gzip ReadAll: %s\", err) <nl> } <nl> if err := gz.Close(); err != nil { <nl> - return err <nl> + fmt.Errorf(\"unmarshal sub gzip Close: %s\", err) <nl> } <nl> b = ub <nl> } <nl> - return proto.Unmarshal(b, m) <nl> + if err := proto.Unmarshal(b, m); err != nil { <nl> + return fmt.Errorf(\"proto unmarshal: %s\", err) <nl> + } <nl> + return nil <nl> } <nl> ", "msg": "Better error messages during unmarshal"}
{"diff_id": 913, "repo": "rqlite/rqlite", "sha": "1df87281308c2073570fd8356723009b2a17945d", "time": "13.04.2021 08:59:44", "diff": "mmm a / command/marshal.go <nl> ppp b / command/marshal.go <nl>@@ -28,8 +28,6 @@ type RequestMarshaler struct { <nl> BatchThreshold int <nl> SizeThreshold int <nl> ForceCompression bool <nl> - <nl> - gz *gzip.Writer <nl> } <nl> const ( <nl> @@ -58,15 +56,9 @@ func init() { <nl> // NewRequestMarshaler returns an initialized RequestMarshaler. <nl> func NewRequestMarshaler() *RequestMarshaler { <nl> - w, err := gzip.NewWriterLevel(nil, gzip.BestCompression) <nl> - if err != nil { <nl> - panic(fmt.Sprintf(\"failed to create GZIP writer: %s\", err.Error())) <nl> - } <nl> - <nl> return &RequestMarshaler{ <nl> BatchThreshold: defaultBatchThreshold, <nl> SizeThreshold: defaultSizeThreshold, <nl> - gz: w, <nl> } <nl> } <nl> @@ -76,6 +68,11 @@ func (m *RequestMarshaler) Marshal(r Requester) ([]byte, bool, error) { <nl> stats.Add(numRequests, 0) <nl> compress := false <nl> + gzw, err := gzip.NewWriterLevel(nil, gzip.BestCompression) <nl> + if err != nil { <nl> + panic(fmt.Sprintf(\"failed to create GZIP writer: %s\", err.Error())) <nl> + } <nl> + <nl> stmts := r.GetRequest().GetStatements() <nl> if len(stmts) >= m.BatchThreshold { <nl> compress = true <nl> @@ -98,11 +95,11 @@ func (m *RequestMarshaler) Marshal(r Requester) ([]byte, bool, error) { <nl> if compress { <nl> // Let's try compression. <nl> var buf bytes.Buffer <nl> - m.gz.Reset(&buf) <nl> - if _, err := m.gz.Write(b); err != nil { <nl> + gzw.Reset(&buf) <nl> + if _, err := gzw.Write(b); err != nil { <nl> return nil, false, err <nl> } <nl> - if err := m.gz.Close(); err != nil { <nl> + if err := gzw.Close(); err != nil { <nl> return nil, false, err <nl> } <nl> ", "msg": "Create GZIP writer for every compress request\nThe GZIP writer was being shared -- incorrectly -- by multiple HTTP\nrequests (both queries and executes). But it's not thread safe. Bad,\nbad, bad."}
{"diff_id": 914, "repo": "rqlite/rqlite", "sha": "5d9e55f312295951333037f9b42b75053778d03a", "time": "13.04.2021 09:05:03", "diff": "mmm a / command/marshal.go <nl> ppp b / command/marshal.go <nl>@@ -97,10 +97,10 @@ func (m *RequestMarshaler) Marshal(r Requester) ([]byte, bool, error) { <nl> var buf bytes.Buffer <nl> gzw.Reset(&buf) <nl> if _, err := gzw.Write(b); err != nil { <nl> - return nil, false, err <nl> + return nil, false, fmt.Errorf(\"gzip Write: %s\", err) <nl> } <nl> if err := gzw.Close(); err != nil { <nl> - return nil, false, err <nl> + return nil, false, fmt.Errorf(\"gzip Close: %s\", err) <nl> } <nl> // Is compression better? <nl> ", "msg": "Better error messages during gzip"}
{"diff_id": 975, "repo": "rqlite/rqlite", "sha": "1c6a7b49aa6e90f70e64d713772c8a0ff692df40", "time": "17.08.2021 10:14:31", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -128,6 +128,8 @@ var stats *expvar.Map <nl> const ( <nl> numExecutions = \"executions\" <nl> numQueries = \"queries\" <nl> + numRemoteExecutions = \"remote_executions\" <nl> + numRemoteQueries = \"remote_queries\" <nl> numBackups = \"backups\" <nl> numLoad = \"loads\" <nl> numJoins = \"joins\" <nl> @@ -153,12 +155,19 @@ const ( <nl> // VersionHTTPHeader is the HTTP header key for the version. <nl> VersionHTTPHeader = \"X-RQLITE-VERSION\" <nl> + <nl> + // ServedByHTTPHeader is the HTTP header used to report which <nl> + // node (by node Raft address) actually served the request if <nl> + // it wasn't served by this node. <nl> + ServedByHTTPHeader = \"X-RQLITE-SERVED-BY\" <nl> ) <nl> func init() { <nl> stats = expvar.NewMap(\"http\") <nl> stats.Add(numExecutions, 0) <nl> stats.Add(numQueries, 0) <nl> + stats.Add(numRemoteExecutions, 0) <nl> + stats.Add(numRemoteQueries, 0) <nl> stats.Add(numBackups, 0) <nl> stats.Add(numLoad, 0) <nl> stats.Add(numJoins, 0) <nl> @@ -725,13 +734,19 @@ func (s *Service) handleExecute(w http.ResponseWriter, r *http.Request) { <nl> isTx, err := isTx(r) <nl> if err != nil { <nl> - http.Error(w, err.Error(), http.StatusInternalServerError) <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> + return <nl> + } <nl> + <nl> + redirect, err := isRedirect(r) <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> } <nl> timings, err := timings(r) <nl> if err != nil { <nl> - http.Error(w, err.Error(), http.StatusInternalServerError) <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> } <nl> @@ -756,20 +771,30 @@ func (s *Service) handleExecute(w http.ResponseWriter, r *http.Request) { <nl> Timings: timings, <nl> } <nl> - results, err := s.store.Execute(er) <nl> - if err != nil { <nl> - if err == store.ErrNotLeader { <nl> + results, resultsErr := s.store.Execute(er) <nl> + if resultsErr != nil && resultsErr == store.ErrNotLeader { <nl> + if redirect { <nl> leaderAPIAddr := s.LeaderAPIAddr() <nl> if leaderAPIAddr == \"\" { <nl> http.Error(w, err.Error(), http.StatusServiceUnavailable) <nl> return <nl> } <nl> - <nl> - redirect := s.FormRedirect(r, leaderAPIAddr) <nl> - http.Redirect(w, r, redirect, http.StatusMovedPermanently) <nl> + loc := s.FormRedirect(r, leaderAPIAddr) <nl> + http.Redirect(w, r, loc, http.StatusMovedPermanently) <nl> return <nl> } <nl> - resp.Error = err.Error() <nl> + <nl> + addr, err := s.store.LeaderAddr() <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> + } <nl> + results, resultsErr = s.cluster.Execute(addr, er) <nl> + stats.Add(numRemoteExecutions, 1) <nl> + w.Header().Add(ServedByHTTPHeader, addr) <nl> + } <nl> + <nl> + if resultsErr != nil { <nl> + resp.Error = resultsErr.Error() <nl> } else { <nl> resp.Results.ExecuteResult = results <nl> } <nl> @@ -799,6 +824,12 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { <nl> return <nl> } <nl> + redirect, err := isRedirect(r) <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> + return <nl> + } <nl> + <nl> timings, err := timings(r) <nl> if err != nil { <nl> http.Error(w, err.Error(), http.StatusBadRequest) <nl> @@ -834,20 +865,30 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { <nl> Freshness: frsh.Nanoseconds(), <nl> } <nl> - results, err := s.store.Query(qr) <nl> - if err != nil { <nl> - if err == store.ErrNotLeader { <nl> + results, resultsErr := s.store.Query(qr) <nl> + if err != nil && err == store.ErrNotLeader { <nl> + if redirect { <nl> leaderAPIAddr := s.LeaderAPIAddr() <nl> if leaderAPIAddr == \"\" { <nl> http.Error(w, err.Error(), http.StatusServiceUnavailable) <nl> return <nl> } <nl> - <nl> - redirect := s.FormRedirect(r, leaderAPIAddr) <nl> - http.Redirect(w, r, redirect, http.StatusMovedPermanently) <nl> + loc := s.FormRedirect(r, leaderAPIAddr) <nl> + http.Redirect(w, r, loc, http.StatusMovedPermanently) <nl> return <nl> } <nl> - resp.Error = err.Error() <nl> + <nl> + addr, err := s.store.LeaderAddr() <nl> + if err != nil { <nl> + http.Error(w, err.Error(), http.StatusInternalServerError) <nl> + } <nl> + results, resultsErr = s.cluster.Query(addr, qr) <nl> + stats.Add(numRemoteQueries, 1) <nl> + w.Header().Add(ServedByHTTPHeader, addr) <nl> + } <nl> + <nl> + if resultsErr != nil { <nl> + resp.Error = resultsErr.Error() <nl> } else { <nl> resp.Results.QueryRows = results <nl> } <nl> @@ -1106,6 +1147,13 @@ func isPretty(req *http.Request) (bool, error) { <nl> return queryParam(req, \"pretty\") <nl> } <nl> +// isRedirect returns whether the HTTP request is requesting a explicit <nl> +// redirect to the leader, if necessary. <nl> +func isRedirect(req *http.Request) (bool, error) { <nl> + return true, nil // Keep current behavior for now. <nl> + return queryParam(req, \"redirect\") <nl> +} <nl> + <nl> // isTx returns whether the HTTP request is requesting a transaction. <nl> func isTx(req *http.Request) (bool, error) { <nl> return queryParam(req, \"transaction\") <nl> ", "msg": "Add support, but disable, request forwarding\nExecute and Query only for now."}
{"diff_id": 978, "repo": "rqlite/rqlite", "sha": "b01b10e4ce77515c90b3523c0aa34d3c8244b760", "time": "19.08.2021 07:51:55", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -500,7 +500,7 @@ func (s *Service) handleLoad(w http.ResponseWriter, r *http.Request) { <nl> resp := NewResponse() <nl> - timings, err := timings(r) <nl> + timings, err := isTimings(r) <nl> if err != nil { <nl> http.Error(w, err.Error(), http.StatusInternalServerError) <nl> return <nl> @@ -735,25 +735,7 @@ func (s *Service) handleExecute(w http.ResponseWriter, r *http.Request) { <nl> resp := NewResponse() <nl> - isTx, err := isTx(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - redirect, err := isRedirect(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - timings, err := timings(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - timeout, err := timeoutParam(r, defaulTimeout) <nl> + timeout, isTx, timings, redirect, err := reqParams(r, defaulTimeout) <nl> if err != nil { <nl> http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> @@ -827,25 +809,7 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { <nl> resp := NewResponse() <nl> - isTx, err := isTx(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - redirect, err := isRedirect(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - timings, err := timings(r) <nl> - if err != nil { <nl> - http.Error(w, err.Error(), http.StatusBadRequest) <nl> - return <nl> - } <nl> - <nl> - timeout, err := timeoutParam(r, defaulTimeout) <nl> + timeout, isTx, timings, redirect, err := reqParams(r, defaulTimeout) <nl> if err != nil { <nl> http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> @@ -1056,7 +1020,7 @@ func (s *Service) writeResponse(w http.ResponseWriter, r *http.Request, j *Respo <nl> var b []byte <nl> var err error <nl> pretty, _ := isPretty(r) <nl> - timings, _ := timings(r) <nl> + timings, _ := isTimings(r) <nl> if timings { <nl> j.SetTime() <nl> @@ -1188,6 +1152,28 @@ func isTx(req *http.Request) (bool, error) { <nl> return queryParam(req, \"transaction\") <nl> } <nl> +// reqParams is a convenience function to get a bunch of query params <nl> +// in one function call. <nl> +func reqParams(req *http.Request, def time.Duration) (timeout time.Duration, tx, timings, redirect bool, err error) { <nl> + timeout, err = timeoutParam(req, def) <nl> + if err != nil { <nl> + return 0, false, false, false, err <nl> + } <nl> + tx, err = isTx(req) <nl> + if err != nil { <nl> + return 0, false, false, false, err <nl> + } <nl> + timings, err = isTimings(req) <nl> + if err != nil { <nl> + return 0, false, false, false, err <nl> + } <nl> + redirect, err = isRedirect(req) <nl> + if err != nil { <nl> + return 0, false, false, false, err <nl> + } <nl> + return timeout, tx, timings, redirect, nil <nl> +} <nl> + <nl> // noLeader returns whether processing should skip the leader check. <nl> func noLeader(req *http.Request) (bool, error) { <nl> return queryParam(req, \"noleader\") <nl> @@ -1198,8 +1184,8 @@ func nonVoters(req *http.Request) (bool, error) { <nl> return queryParam(req, \"nonvoters\") <nl> } <nl> -// timings returns whether timings are requested. <nl> -func timings(req *http.Request) (bool, error) { <nl> +// isTimings returns whether timings are requested. <nl> +func isTimings(req *http.Request) (bool, error) { <nl> return queryParam(req, \"timings\") <nl> } <nl> ", "msg": "Add convenience function for query params"}
{"diff_id": 995, "repo": "rqlite/rqlite", "sha": "00f4c6bc351992e4194e17c5f942fd789d74cb81", "time": "01.09.2021 10:52:12", "diff": "mmm a / store/store_test.go <nl> ppp b / store/store_test.go <nl>@@ -32,7 +32,7 @@ func Test_OpenStoreSingleNode(t *testing.T) { <nl> if err != nil { <nl> t.Fatalf(\"failed to get leader address: %s\", err.Error()) <nl> } <nl> - id, err := s.LeaderID() <nl> + id, err := waitForLeaderID(s, 10*time.Second) <nl> if err != nil { <nl> t.Fatalf(\"failed to retrieve leader ID: %s\", err.Error()) <nl> } <nl> @@ -620,7 +620,7 @@ func Test_MultiNodeJoinRemove(t *testing.T) { <nl> if exp := s0.Addr(); got != exp { <nl> t.Fatalf(\"wrong leader address returned, got: %s, exp %s\", got, exp) <nl> } <nl> - id, err := s1.LeaderID() <nl> + id, err := waitForLeaderID(s1, 10*time.Second) <nl> if err != nil { <nl> t.Fatalf(\"failed to retrieve leader ID: %s\", err.Error()) <nl> } <nl> @@ -696,7 +696,7 @@ func Test_MultiNodeJoinNonVoterRemove(t *testing.T) { <nl> if exp := s0.Addr(); got != exp { <nl> t.Fatalf(\"wrong leader address returned, got: %s, exp %s\", got, exp) <nl> } <nl> - id, err := s1.LeaderID() <nl> + id, err := waitForLeaderID(s1, 10*time.Second) <nl> if err != nil { <nl> t.Fatalf(\"failed to retrieve leader ID: %s\", err.Error()) <nl> } <nl> @@ -1418,6 +1418,31 @@ func queryRequestFromStrings(s []string, timings, tx bool) *command.QueryRequest <nl> } <nl> } <nl> +// waitForLeaderID waits until the Store's LeaderID is set, or the timeout <nl> +// expires. Because setting Leader ID requires Raft to set the cluster <nl> +// configuration, it's not entirely deterministic when it will be set. <nl> +func waitForLeaderID(s *Store, timeout time.Duration) (string, error) { <nl> + tck := time.NewTicker(100 * time.Millisecond) <nl> + defer tck.Stop() <nl> + tmr := time.NewTimer(timeout) <nl> + defer tmr.Stop() <nl> + <nl> + for { <nl> + select { <nl> + case <-tck.C: <nl> + id, err := s.LeaderID() <nl> + if err != nil { <nl> + return \"\", err <nl> + } <nl> + if id != \"\" { <nl> + return id, nil <nl> + } <nl> + case <-tmr.C: <nl> + return \"\", fmt.Errorf(\"timeout expired\") <nl> + } <nl> + } <nl> +} <nl> + <nl> func asJSON(v interface{}) string { <nl> b, err := encoding.JSONMarshal(v) <nl> if err != nil { <nl> ", "msg": "Wait, or timeout, for LeaderID to be set\nAddress\nfailures."}
{"diff_id": 1006, "repo": "rqlite/rqlite", "sha": "5dd4aeed1e9dd355202cdd6c3f38a2b297982ecb", "time": "09.09.2021 12:50:20", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -71,6 +71,8 @@ const ( <nl> numRemovedBeforeJoins = \"num_removed_before_joins\" <nl> snapshotCreateDuration = \"snapshot_create_duration\" <nl> snapshotPersistDuration = \"snapshot_persist_duration\" <nl> + snapshotDBSerializedSize = \"snapshot_db_serialized_size\" <nl> + snapshotDBOnDiskSize = \"snapshot_db_ondisk_size\" <nl> ) <nl> // BackupFormat represents the format of database backup. <nl> @@ -99,6 +101,8 @@ func init() { <nl> stats.Add(numRemovedBeforeJoins, 0) <nl> stats.Add(snapshotCreateDuration, 0) <nl> stats.Add(snapshotPersistDuration, 0) <nl> + stats.Add(snapshotDBSerializedSize, 0) <nl> + stats.Add(snapshotDBOnDiskSize, 0) <nl> } <nl> // ClusterState defines the possible Raft states the current node can be in <nl> @@ -1044,6 +1048,7 @@ func (s *Store) Snapshot() (raft.FSMSnapshot, error) { <nl> dur := time.Since(fsm.startT) <nl> stats.Add(numSnaphots, 1) <nl> stats.Get(snapshotCreateDuration).(*expvar.Int).Set(dur.Milliseconds()) <nl> + stats.Get(snapshotDBSerializedSize).(*expvar.Int).Set(int64(len(fsm.database))) <nl> s.logger.Printf(\"node snapshot created in %s\", dur) <nl> return fsm, nil <nl> } <nl> @@ -1213,6 +1218,7 @@ func (f *fsmSnapshot) Persist(sink raft.SnapshotSink) error { <nl> if _, err := sink.Write(cdb); err != nil { <nl> return err <nl> } <nl> + stats.Get(snapshotDBOnDiskSize).(*expvar.Int).Set(int64(len(cdb))) <nl> } else { <nl> f.logger.Println(\"no database data available for snapshot\") <nl> err = writeUint64(b, uint64(0)) <nl> @@ -1222,6 +1228,7 @@ func (f *fsmSnapshot) Persist(sink raft.SnapshotSink) error { <nl> if _, err := sink.Write(b.Bytes()); err != nil { <nl> return err <nl> } <nl> + stats.Get(snapshotDBOnDiskSize).(*expvar.Int).Set(0) <nl> } <nl> // Close the sink. <nl> ", "msg": "More Snapshot metrics"}
{"diff_id": 1027, "repo": "rqlite/rqlite", "sha": "0a31c90b060fc70418bb8918356c8510d91ab79d", "time": "11.11.2021 23:12:56", "diff": "mmm a / cmd/rqlited/main.go <nl> ppp b / cmd/rqlited/main.go <nl>@@ -249,7 +249,7 @@ func main() { <nl> // Determine join addresses <nl> var joins []string <nl> - joins, err = determineJoinAddresses() <nl> + joins, err = determineJoinAddresses(isNew) <nl> if err != nil { <nl> log.Fatalf(\"unable to determine join addresses: %s\", err.Error()) <nl> } <nl> @@ -350,7 +350,7 @@ func main() { <nl> log.Println(\"rqlite server stopped\") <nl> } <nl> -func determineJoinAddresses() ([]string, error) { <nl> +func determineJoinAddresses(isNew bool) ([]string, error) { <nl> apiAdv := httpAddr <nl> if httpAdv != \"\" { <nl> apiAdv = httpAdv <nl> @@ -363,6 +363,9 @@ func determineJoinAddresses() ([]string, error) { <nl> } <nl> if discoID != \"\" { <nl> + if !isNew { <nl> + log.Printf(\"node has preexisting state, ignoring Discovery ID %s\", discoID) <nl> + } else { <nl> log.Printf(\"registering with Discovery Service at %s with ID %s\", discoURL, discoID) <nl> c := disco.New(discoURL) <nl> r, err := c.Register(discoID, apiAdv) <nl> @@ -377,6 +380,7 @@ func determineJoinAddresses() ([]string, error) { <nl> } <nl> } <nl> } <nl> + } <nl> return addrs, nil <nl> } <nl> ", "msg": "Ignore disco ID if there is preexisting state\nFixes"}
{"diff_id": 1029, "repo": "rqlite/rqlite", "sha": "eab9183acda932d89207560bdbd0e601d72087ec", "time": "16.11.2021 09:05:27", "diff": "mmm a / db/db.go <nl> ppp b / db/db.go <nl>@@ -75,7 +75,8 @@ type PoolStats struct { <nl> MaxLifetimeClosed int64 `json:\"max_lifetime_closed\"` <nl> } <nl> -// Open opens a file-based database, creating it if it does not exist. <nl> +// Open opens a file-based database, creating it if it does not exist. After this <nl> +// function returns, an actual SQLite file will always exist. <nl> func Open(dbPath string, fkEnabled bool) (*DB, error) { <nl> rwDSN := fmt.Sprintf(\"file:%s?_fk=%s\", dbPath, strconv.FormatBool(fkEnabled)) <nl> rwDB, err := sql.Open(\"sqlite3\", rwDSN) <nl> ", "msg": "Better comment in DB code"}
{"diff_id": 1032, "repo": "rqlite/rqlite", "sha": "239cdc04b53f52bafdcd3dc35a033ad79d62e3f5", "time": "26.11.2021 09:50:46", "diff": "mmm a / db/db_test.go <nl> ppp b / db/db_test.go <nl>@@ -1456,8 +1456,6 @@ func Test_SerializeInMemory(t *testing.T) { <nl> } <nl> func Test_Dump(t *testing.T) { <nl> - t.Parallel() <nl> - <nl> db, path := mustCreateDatabase() <nl> defer db.Close() <nl> defer os.Remove(path) <nl> @@ -1478,8 +1476,6 @@ func Test_Dump(t *testing.T) { <nl> } <nl> func Test_DumpMemory(t *testing.T) { <nl> - t.Parallel() <nl> - <nl> db, path := mustCreateDatabase() <nl> defer db.Close() <nl> defer os.Remove(path) <nl> @@ -1507,7 +1503,6 @@ func Test_DumpMemory(t *testing.T) { <nl> // Test_1GiBInMemory tests that in-memory databases larger than 1GiB, <nl> // but smaller than 2GiB, can be created without issue. <nl> func Test_1GiBInMemory(t *testing.T) { <nl> - t.Parallel() <nl> db := mustCreateInMemoryDatabase() <nl> defer db.Close() <nl> @@ -1553,7 +1548,6 @@ func Test_1GiBInMemory(t *testing.T) { <nl> // <nl> // See https://github.com/mattn/go-sqlite3/issues/959#issuecomment-890283264 <nl> func Test_ParallelOperationsInMemory(t *testing.T) { <nl> - t.Parallel() <nl> db := mustCreateInMemoryDatabase() <nl> defer db.Close() <nl> ", "msg": "Don't run heavy memory tests in parallel\nLet's see if this helps these tests run faster, especially with -race,\nand prevent CircleCI timeouts."}
{"diff_id": 1035, "repo": "rqlite/rqlite", "sha": "f440eedbe0027bf7641c1bc2ad1d2138c4b1dbb7", "time": "31.12.2021 14:59:44", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -923,8 +923,7 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { <nl> // Get the query statement(s), and do tx if necessary. <nl> queries, err := requestQueries(r) <nl> if err != nil { <nl> - w.WriteHeader(http.StatusBadRequest) <nl> - w.Write([]byte(err.Error())) <nl> + http.Error(w, err.Error(), http.StatusBadRequest) <nl> return <nl> } <nl> ", "msg": "Use better function"}
{"diff_id": 1036, "repo": "rqlite/rqlite", "sha": "cfe938412f38e837ed54e21e94c04be8df967a3d", "time": "31.12.2021 15:02:13", "diff": "mmm a / http/request_parser.go <nl> ppp b / http/request_parser.go <nl>@@ -11,6 +11,9 @@ var ( <nl> // ErrNoStatements is returned when a request is empty <nl> ErrNoStatements = errors.New(\"no statements\") <nl> + // ErrInvalidJSON is returned when a body is not valid JSON <nl> + ErrInvalidJSON = errors.New(\"invalid JSON body\") <nl> + <nl> // ErrInvalidRequest is returned when a request cannot be parsed. <nl> ErrInvalidRequest = errors.New(\"invalid request\") <nl> @@ -45,7 +48,7 @@ func ParseRequest(b []byte) ([]*command.Statement, error) { <nl> // Next try parameterized form. <nl> if err := json.Unmarshal(b, &parameterized); err != nil { <nl> - return nil, ErrInvalidRequest <nl> + return nil, ErrInvalidJSON <nl> } <nl> stmts := make([]*command.Statement, len(parameterized)) <nl> ", "msg": "Better error codes"}
{"diff_id": 1040, "repo": "rqlite/rqlite", "sha": "85b834cd6452a71f6290eac31c6545b52edcf309", "time": "07.01.2022 23:43:50", "diff": "mmm a / cmd/rqlited/main.go <nl> ppp b / cmd/rqlited/main.go <nl>@@ -69,12 +69,12 @@ var fkConstraints bool <nl> var raftLogLevel string <nl> var raftNonVoter bool <nl> var raftSnapThreshold uint64 <nl> -var raftSnapInterval string <nl> -var raftLeaderLeaseTimeout string <nl> -var raftHeartbeatTimeout string <nl> -var raftElectionTimeout string <nl> -var raftApplyTimeout string <nl> -var raftOpenTimeout string <nl> +var raftSnapInterval time.Duration <nl> +var raftLeaderLeaseTimeout time.Duration <nl> +var raftHeartbeatTimeout time.Duration <nl> +var raftElectionTimeout time.Duration <nl> +var raftApplyTimeout time.Duration <nl> +var raftOpenTimeout time.Duration <nl> var raftWaitForLeader bool <nl> var raftShutdownOnRemove bool <nl> var compressionSize int <nl> @@ -119,14 +119,14 @@ func init() { <nl> flag.BoolVar(&fkConstraints, \"fk\", false, \"Enable SQLite foreign key constraints\") <nl> flag.BoolVar(&showVersion, \"version\", false, \"Show version information and exit\") <nl> flag.BoolVar(&raftNonVoter, \"raft-non-voter\", false, \"Configure as non-voting node\") <nl> - flag.StringVar(&raftHeartbeatTimeout, \"raft-timeout\", \"1s\", \"Raft heartbeat timeout\") <nl> - flag.StringVar(&raftElectionTimeout, \"raft-election-timeout\", \"1s\", \"Raft election timeout\") <nl> - flag.StringVar(&raftApplyTimeout, \"raft-apply-timeout\", \"10s\", \"Raft apply timeout\") <nl> - flag.StringVar(&raftOpenTimeout, \"raft-open-timeout\", \"120s\", \"Time for initial Raft logs to be applied. Use 0s duration to skip wait\") <nl> + flag.DurationVar(&raftHeartbeatTimeout, \"raft-timeout\", time.Second, \"Raft heartbeat timeout\") <nl> + flag.DurationVar(&raftElectionTimeout, \"raft-election-timeout\", time.Second, \"Raft election timeout\") <nl> + flag.DurationVar(&raftApplyTimeout, \"raft-apply-timeout\", 10*time.Second, \"Raft apply timeout\") <nl> + flag.DurationVar(&raftOpenTimeout, \"raft-open-timeout\", 120*time.Second, \"Time for initial Raft logs to be applied. Use 0s duration to skip wait\") <nl> flag.BoolVar(&raftWaitForLeader, \"raft-leader-wait\", true, \"Node waits for a leader before answering requests\") <nl> flag.Uint64Var(&raftSnapThreshold, \"raft-snap\", 8192, \"Number of outstanding log entries that trigger snapshot\") <nl> - flag.StringVar(&raftSnapInterval, \"raft-snap-int\", \"30s\", \"Snapshot threshold check interval\") <nl> - flag.StringVar(&raftLeaderLeaseTimeout, \"raft-leader-lease-timeout\", \"0s\", \"Raft leader lease timeout. Use 0s for Raft default\") <nl> + flag.DurationVar(&raftSnapInterval, \"raft-snap-int\", 30*time.Second, \"Snapshot threshold check interval\") <nl> + flag.DurationVar(&raftLeaderLeaseTimeout, \"raft-leader-lease-timeout\", 0, \"Raft leader lease timeout. Use 0s for Raft default\") <nl> flag.BoolVar(&raftShutdownOnRemove, \"raft-remove-shutdown\", false, \"Shutdown Raft if node removed\") <nl> flag.StringVar(&raftLogLevel, \"raft-log-level\", \"INFO\", \"Minimum log level for Raft module\") <nl> flag.IntVar(&compressionSize, \"compression-size\", 150, \"Request query size for compression attempt\") <nl> @@ -387,18 +387,14 @@ func determineJoinAddresses(isNew bool) ([]string, error) { <nl> } <nl> func waitForConsensus(str *store.Store) error { <nl> - openTimeout, err := time.ParseDuration(raftOpenTimeout) <nl> - if err != nil { <nl> - return fmt.Errorf(\"failed to parse Raft open timeout %s: %s\", raftOpenTimeout, err.Error()) <nl> - } <nl> - if _, err := str.WaitForLeader(openTimeout); err != nil { <nl> + if _, err := str.WaitForLeader(raftOpenTimeout); err != nil { <nl> if raftWaitForLeader { <nl> return fmt.Errorf(\"leader did not appear within timeout: %s\", err.Error()) <nl> } <nl> log.Println(\"ignoring error while waiting for leader\") <nl> } <nl> - if openTimeout != 0 { <nl> - if err := str.WaitForInitialLogs(openTimeout); err != nil { <nl> + if raftOpenTimeout != 0 { <nl> + if err := str.WaitForInitialLogs(raftOpenTimeout); err != nil { <nl> return fmt.Errorf(\"log was not fully applied within timeout: %s\", err.Error()) <nl> } <nl> } else { <nl> @@ -430,26 +426,11 @@ func createStore(ln *tcp.Layer, dataPath string) (*store.Store, bool, error) { <nl> str.RaftLogLevel = raftLogLevel <nl> str.ShutdownOnRemove = raftShutdownOnRemove <nl> str.SnapshotThreshold = raftSnapThreshold <nl> - str.SnapshotInterval, err = time.ParseDuration(raftSnapInterval) <nl> - if err != nil { <nl> - return nil, false, fmt.Errorf(\"invalid Raft Snapsnot interval %s: %s\", raftSnapInterval, err.Error()) <nl> - } <nl> - str.LeaderLeaseTimeout, err = time.ParseDuration(raftLeaderLeaseTimeout) <nl> - if err != nil { <nl> - return nil, false, fmt.Errorf(\"invalid Raft Leader lease timeout %s: %s\", raftLeaderLeaseTimeout, err.Error()) <nl> - } <nl> - str.HeartbeatTimeout, err = time.ParseDuration(raftHeartbeatTimeout) <nl> - if err != nil { <nl> - return nil, false, fmt.Errorf(\"invalid Raft heartbeat timeout %s: %s\", raftHeartbeatTimeout, err.Error()) <nl> - } <nl> - str.ElectionTimeout, err = time.ParseDuration(raftElectionTimeout) <nl> - if err != nil { <nl> - return nil, false, fmt.Errorf(\"invalid Raft election timeout %s: %s\", raftElectionTimeout, err.Error()) <nl> - } <nl> - str.ApplyTimeout, err = time.ParseDuration(raftApplyTimeout) <nl> - if err != nil { <nl> - return nil, false, fmt.Errorf(\"invalid Raft apply timeout %s: %s\", raftApplyTimeout, err.Error()) <nl> - } <nl> + str.SnapshotInterval = raftSnapInterval <nl> + str.LeaderLeaseTimeout = raftLeaderLeaseTimeout <nl> + str.HeartbeatTimeout = raftHeartbeatTimeout <nl> + str.ElectionTimeout = raftElectionTimeout <nl> + str.ApplyTimeout = raftApplyTimeout <nl> return str, store.IsNewNode(dataPath), nil <nl> } <nl> ", "msg": "Use actual duration types for flags"}
{"diff_id": 1046, "repo": "rqlite/rqlite", "sha": "3ad2b73155b7d522ad9da38a507e527820ae453a", "time": "13.01.2022 12:52:34", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -115,7 +115,7 @@ func (d *DBResults) MarshalJSON() ([]byte, error) { <nl> } else if d.QueryRows != nil { <nl> return encoding.JSONMarshal(d.QueryRows) <nl> } <nl> - return nil, fmt.Errorf(\"no DB results set\") <nl> + return json.Marshal(make([]interface{}, 0)) // Any empty list. <nl> } <nl> // Response represents a response from the HTTP service. <nl> ", "msg": "Return a clearer response if no DB results\nThis shouldn't happen, but it might, this will allow the underlying\nerror to be returned to the caller. The body of the response will now\nlook something like:\n{\n\"results\": [],\n\"error\": \"some error\",\n\"time\": 0.021976516\n}"}
{"diff_id": 1062, "repo": "rqlite/rqlite", "sha": "0aabfbff6a85014dc13a2d96719091d59f01dafe", "time": "09.05.2022 22:43:16", "diff": "mmm a / command/marshal.go <nl> ppp b / command/marshal.go <nl>@@ -89,23 +89,15 @@ func (m *RequestMarshaler) Marshal(r Requester) ([]byte, bool, error) { <nl> if compress { <nl> // Let's try compression. <nl> - var buf bytes.Buffer <nl> - gzw, err := gzip.NewWriterLevel(&buf, gzip.BestCompression) <nl> + gzData, err := gzCompress(b) <nl> if err != nil { <nl> - return nil, false, fmt.Errorf(\"gzip new writer: %s\", err) <nl> - } <nl> - <nl> - if _, err := gzw.Write(b); err != nil { <nl> - return nil, false, fmt.Errorf(\"gzip Write: %s\", err) <nl> - } <nl> - if err := gzw.Close(); err != nil { <nl> - return nil, false, fmt.Errorf(\"gzip Close: %s\", err) <nl> + return nil, false, err <nl> } <nl> // Is compression better? <nl> - if ubz > len(buf.Bytes()) || m.ForceCompression { <nl> + if ubz > len(gzData) || m.ForceCompression { <nl> // Yes! Let's keep it. <nl> - b = buf.Bytes() <nl> + b = gzData <nl> stats.Add(numCompressedRequests, 1) <nl> stats.Add(numCompressedBytes, int64(len(b))) <nl> } else { <nl> @@ -153,13 +145,25 @@ func UnmarshalNoop(b []byte, c *Noop) error { <nl> // MarshalLoadRequest marshals a LoadRequest command <nl> func MarshalLoadRequest(lr *LoadRequest) ([]byte, error) { <nl> - // XXX Compress the SQLIte data! <nl> + b, err := gzCompress(lr.Data) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + lr.Data = b <nl> return proto.Marshal(lr) <nl> } <nl> // UnmarshalLoadRequest unmarshals a LoadRequest command <nl> func UnmarshalLoadRequest(b []byte, lr *LoadRequest) error { <nl> - return proto.Unmarshal(b, lr) <nl> + if err := proto.Unmarshal(b, lr); err != nil { <nl> + return err <nl> + } <nl> + u, err := gzUncompress(lr.Data) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + lr.Data = u <nl> + return nil <nl> } <nl> // UnmarshalSubCommand unmarshalls a sub command m. It assumes that <nl> @@ -167,24 +171,49 @@ func UnmarshalLoadRequest(b []byte, lr *LoadRequest) error { <nl> func UnmarshalSubCommand(c *Command, m proto.Message) error { <nl> b := c.SubCommand <nl> if c.Compressed { <nl> - gz, err := gzip.NewReader(bytes.NewReader(b)) <nl> + var err error <nl> + b, err = gzUncompress(b) <nl> if err != nil { <nl> - return fmt.Errorf(\"unmarshal sub gzip NewReader: %s\", err) <nl> + return fmt.Errorf(\"unmarshal sub uncompress: %s\", err) <nl> + } <nl> } <nl> - ub, err := ioutil.ReadAll(gz) <nl> + if err := proto.Unmarshal(b, m); err != nil { <nl> + return fmt.Errorf(\"proto unmarshal: %s\", err) <nl> + } <nl> + return nil <nl> +} <nl> + <nl> +// gzCompress compresses the given byte slice. <nl> +func gzCompress(b []byte) ([]byte, error) { <nl> + var buf bytes.Buffer <nl> + gzw, err := gzip.NewWriterLevel(&buf, gzip.BestCompression) <nl> if err != nil { <nl> - return fmt.Errorf(\"unmarshal sub gzip ReadAll: %s\", err) <nl> + return nil, fmt.Errorf(\"gzip new writer: %s\", err) <nl> } <nl> - if err := gz.Close(); err != nil { <nl> - return fmt.Errorf(\"unmarshal sub gzip Close: %s\", err) <nl> + if _, err := gzw.Write(b); err != nil { <nl> + return nil, fmt.Errorf(\"gzip Write: %s\", err) <nl> + } <nl> + if err := gzw.Close(); err != nil { <nl> + return nil, fmt.Errorf(\"gzip Close: %s\", err) <nl> } <nl> - b = ub <nl> + return buf.Bytes(), nil <nl> } <nl> - if err := proto.Unmarshal(b, m); err != nil { <nl> - return fmt.Errorf(\"proto unmarshal: %s\", err) <nl> +func gzUncompress(b []byte) ([]byte, error) { <nl> + gz, err := gzip.NewReader(bytes.NewReader(b)) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"unmarshal gzip NewReader: %s\", err) <nl> } <nl> - return nil <nl> + <nl> + ub, err := ioutil.ReadAll(gz) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"unmarshal gzip ReadAll: %s\", err) <nl> + } <nl> + <nl> + if err := gz.Close(); err != nil { <nl> + return nil, fmt.Errorf(\"unmarshal gzip Close: %s\", err) <nl> + } <nl> + return ub, nil <nl> } <nl> ", "msg": "Use compression for LoadRequest marshalling"}
{"diff_id": 1070, "repo": "rqlite/rqlite", "sha": "370cc6c36c44b161208533157c2207e5068fb262", "time": "17.05.2022 08:26:51", "diff": "mmm a / system_test/request_forwarding_test.go <nl> ppp b / system_test/request_forwarding_test.go <nl>@@ -232,14 +232,21 @@ func Test_MultiNodeClusterQueuedRequestForwardOK(t *testing.T) { <nl> t.Fatalf(\"failed to insert record: %s\", err.Error()) <nl> } <nl> - time.Sleep(1 * time.Second) <nl> - <nl> - rows, err = leader.Query(`SELECT COUNT(*) FROM foo`) <nl> + ticker := time.NewTicker(10 * time.Millisecond) <nl> + timer := time.NewTimer(5 * time.Second) <nl> + for { <nl> + select { <nl> + case <-ticker.C: <nl> + r, err := leader.Query(`SELECT COUNT(*) FROM foo`) <nl> if err != nil { <nl> t.Fatalf(\"failed to query for count: %s\", err.Error()) <nl> } <nl> - if exp, got := `{\"results\":[{\"columns\":[\"COUNT(*)\"],\"types\":[\"\"],\"values\":[[1]]}]}`, rows; exp != got { <nl> - t.Fatalf(\"got incorrect response from follower exp: %s, got: %s\", exp, got) <nl> + if r == `{\"results\":[{\"columns\":[\"COUNT(*)\"],\"types\":[\"\"],\"values\":[[1]]}]}` { <nl> + return <nl> + } <nl> + case <-timer.C: <nl> + t.Fatalf(\"timed out waiting for queued writes\") <nl> + } <nl> } <nl> } <nl> ", "msg": "Better test polling"}
{"diff_id": 1071, "repo": "rqlite/rqlite", "sha": "523f11ac0ffa13cea8b1ee6b9a725dc28fd32814", "time": "17.05.2022 09:07:19", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -331,11 +331,14 @@ func (s *Service) Start() error { <nl> // Close closes the service. <nl> func (s *Service) Close() { <nl> s.stmtQueue.Close() <nl> + <nl> select { <nl> case <-s.queueDone: <nl> default: <nl> close(s.closeCh) <nl> } <nl> + <-s.queueDone <nl> + <nl> s.ln.Close() <nl> return <nl> } <nl> ", "msg": "Fix more races"}
{"diff_id": 1081, "repo": "rqlite/rqlite", "sha": "78c038e24d4597e507f75b6e38659ca43ef7724f", "time": "23.05.2022 13:05:35", "diff": "mmm a / queue/queue_test.go <nl> ppp b / queue/queue_test.go <nl>@@ -253,6 +253,9 @@ func Test_NewQueueWriteNilSingleChan(t *testing.T) { <nl> if req.Statements != nil { <nl> t.Fatalf(\"statements slice is not nil\") <nl> } <nl> + if len(req.flushChans) != 1 && req.flushChans[0] != fc { <nl> + t.Fatalf(\"flush chans is not correct\") <nl> + } <nl> req.Close() <nl> case <-time.NewTimer(5 * time.Second).C: <nl> t.Fatalf(\"timed out waiting for statement\") <nl> ", "msg": "More queue unit testing"}
{"diff_id": 1094, "repo": "rqlite/rqlite", "sha": "2226083273fd071ecd11b47aeb25a1743e937d3f", "time": "25.07.2022 23:09:35", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -1103,6 +1103,10 @@ func (s *Service) execute(w http.ResponseWriter, r *http.Request) { <nl> } <nl> results, resultsErr = s.cluster.Execute(er, addr, username, password, timeout) <nl> + if resultsErr.Error() == \"Unauthorized\" { <nl> + http.Error(w, ErrLeaderNotFound.Error(), http.StatusUnauthorized) <nl> + return <nl> + } <nl> stats.Add(numRemoteExecutions, 1) <nl> w.Header().Add(ServedByHTTPHeader, addr) <nl> } <nl> ", "msg": "Implement handling of error unauthorized by client forwarding request to remote tcp service. Bubble this up as a 401 to the caller"}
{"diff_id": 1098, "repo": "rqlite/rqlite", "sha": "01c78ea8d0928056c091ee552bdb82c73a8ed406", "time": "06.08.2022 13:21:50", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -1109,7 +1109,7 @@ func (s *Service) execute(w http.ResponseWriter, r *http.Request) { <nl> results, resultsErr = s.cluster.Execute(er, addr, username, password, timeout) <nl> if resultsErr != nil && resultsErr.Error() == \"unauthorized\" { <nl> - w.WriteHeader(http.StatusUnauthorized) <nl> + http.Error(w, \"remote execute not authorized\", http.StatusUnauthorized) <nl> return <nl> } <nl> stats.Add(numRemoteExecutions, 1) <nl> @@ -1206,7 +1206,7 @@ func (s *Service) handleQuery(w http.ResponseWriter, r *http.Request) { <nl> } <nl> results, resultsErr = s.cluster.Query(qr, addr, username, password, timeout) <nl> if resultsErr != nil && resultsErr.Error() == \"unauthorized\" { <nl> - w.WriteHeader(http.StatusUnauthorized) <nl> + http.Error(w, \"remote query not authorized\", http.StatusUnauthorized) <nl> return <nl> } <nl> stats.Add(numRemoteQueries, 1) <nl> ", "msg": "Better remote auth fail HTTP messages"}
{"diff_id": 1099, "repo": "rqlite/rqlite", "sha": "f7958da1a932794f0d1be3b422cb8a7454d5151e", "time": "06.08.2022 16:15:44", "diff": "mmm a / system_test/request_forwarding_test.go <nl> ppp b / system_test/request_forwarding_test.go <nl>@@ -30,7 +30,7 @@ func Test_StoreClientSideBySide(t *testing.T) { <nl> if exp, got := \"[{}]\", asJSON(res); exp != got { <nl> t.Fatalf(\"unexpected results, expt %s, got %s\", exp, got) <nl> } <nl> - res, err = client.Execute(executeRequestFromString(\"CREATE TABLE bar (id INTEGER NOT NULL PRIMARY KEY, name TEXT)\"), leaderAddr, \"\", \"\", shortWait) <nl> + res, err = client.Execute(executeRequestFromString(\"CREATE TABLE bar (id INTEGER NOT NULL PRIMARY KEY, name TEXT)\"), leaderAddr, nil, shortWait) <nl> if err != nil { <nl> t.Fatalf(\"failed to execute via remote: %s\", err.Error()) <nl> } <nl> @@ -45,7 +45,7 @@ func Test_StoreClientSideBySide(t *testing.T) { <nl> if exp, got := `[{\"last_insert_id\":1,\"rows_affected\":1}]`, asJSON(res); exp != got { <nl> t.Fatalf(\"unexpected results, expt %s, got %s\", exp, got) <nl> } <nl> - res, err = client.Execute(executeRequestFromString(`INSERT INTO bar(name) VALUES(\"fiona\")`), leaderAddr, \"\", \"\", shortWait) <nl> + res, err = client.Execute(executeRequestFromString(`INSERT INTO bar(name) VALUES(\"fiona\")`), leaderAddr, nil, shortWait) <nl> if err != nil { <nl> t.Fatalf(\"failed to execute via remote: %s\", err.Error()) <nl> } <nl> @@ -68,14 +68,14 @@ func Test_StoreClientSideBySide(t *testing.T) { <nl> t.Fatalf(\"unexpected results, expt %s, got %s\", exp, got) <nl> } <nl> - rows, err = client.Query(queryRequestFromString(`SELECT * FROM foo`), leaderAddr, \"\", \"\", shortWait) <nl> + rows, err = client.Query(queryRequestFromString(`SELECT * FROM foo`), leaderAddr, nil, shortWait) <nl> if err != nil { <nl> t.Fatalf(\"failed to query via remote: %s\", err.Error()) <nl> } <nl> if exp, got := `[{\"columns\":[\"id\",\"name\"],\"types\":[\"integer\",\"text\"],\"values\":[[1,\"fiona\"]]}]`, asJSON(rows); exp != got { <nl> t.Fatalf(\"unexpected results, expt %s, got %s\", exp, got) <nl> } <nl> - rows, err = client.Query(queryRequestFromString(`SELECT * FROM bar`), leaderAddr, \"\", \"\", shortWait) <nl> + rows, err = client.Query(queryRequestFromString(`SELECT * FROM bar`), leaderAddr, nil, shortWait) <nl> if err != nil { <nl> t.Fatalf(\"failed to query via remote: %s\", err.Error()) <nl> } <nl> @@ -90,7 +90,7 @@ func Test_StoreClientSideBySide(t *testing.T) { <nl> if exp, got := `[{\"error\":\"no such table: qux\"}]`, asJSON(rows); exp != got { <nl> t.Fatalf(\"unexpected results, expt %s, got %s\", exp, got) <nl> } <nl> - rows, err = client.Query(queryRequestFromString(`SELECT * FROM qux`), leaderAddr, \"\", \"\", shortWait) <nl> + rows, err = client.Query(queryRequestFromString(`SELECT * FROM qux`), leaderAddr, nil, shortWait) <nl> if err != nil { <nl> t.Fatalf(\"failed to query via remote: %s\", err.Error()) <nl> } <nl> ", "msg": "More use of cluster.Credentials"}
{"diff_id": 1107, "repo": "rqlite/rqlite", "sha": "9fc6ea16e986c54f60ca423eb0f39b4a45d3c1b0", "time": "19.10.2022 20:44:58", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -825,6 +825,7 @@ func (s *Store) Backup(br *command.BackupRequest, dst io.Writer) error { <nl> if err := f.Close(); err != nil { <nl> return err <nl> } <nl> + defer os.Remove(f.Name()) <nl> if err := s.db.Backup(f.Name()); err != nil { <nl> return err <nl> @@ -834,7 +835,6 @@ func (s *Store) Backup(br *command.BackupRequest, dst io.Writer) error { <nl> if err != nil { <nl> return err <nl> } <nl> - defer os.Remove(f.Name()) <nl> defer of.Close() <nl> _, err = io.Copy(dst, of) <nl> ", "msg": "Even better defer sequence"}
{"diff_id": 1114, "repo": "rqlite/rqlite", "sha": "893fbb0249a4ce6a72a82604aa23b1568472635b", "time": "24.10.2022 13:33:58", "diff": "mmm a / store/store.go <nl> ppp b / store/store.go <nl>@@ -860,6 +860,7 @@ func (s *Store) Load(lr *command.LoadRequest) error { <nl> b, err := command.MarshalLoadRequest(lr) <nl> if err != nil { <nl> + s.logger.Printf(\"load failed during load-request marshalling %s\", err.Error()) <nl> return err <nl> } <nl> @@ -878,6 +879,7 @@ func (s *Store) Load(lr *command.LoadRequest) error { <nl> if af.Error() == raft.ErrNotLeader { <nl> return ErrNotLeader <nl> } <nl> + s.logger.Printf(\"load failed during Apply: %s\", af.Error()) <nl> return af.Error() <nl> } <nl> ", "msg": "More store-level load operation logging"}
{"diff_id": 1120, "repo": "rqlite/rqlite", "sha": "9c68b13868e4ebe339d989fedac219ffa3bb002a", "time": "28.10.2022 11:34:28", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -959,8 +959,11 @@ func (s *Service) handleNodes(w http.ResponseWriter, r *http.Request) { <nl> // Get nodes in the cluster, and possibly filter out non-voters. <nl> nodes, err := s.store.Nodes() <nl> if err != nil { <nl> - http.Error(w, fmt.Sprintf(\"store nodes: %s\", err.Error()), <nl> - http.StatusInternalServerError) <nl> + statusCode := http.StatusInternalServerError <nl> + if err == store.ErrNotOpen { <nl> + statusCode = http.StatusServiceUnavailable <nl> + } <nl> + http.Error(w, fmt.Sprintf(\"store nodes: %s\", err.Error()), statusCode) <nl> return <nl> } <nl> ", "msg": "Better error from /nodes when Store is not open"}
{"diff_id": 1126, "repo": "rqlite/rqlite", "sha": "36f97bdb556dfd1011fd8d4038bf997856dc1514", "time": "31.10.2022 15:10:00", "diff": "mmm a / cluster/join.go <nl> ppp b / cluster/join.go <nl>@@ -104,6 +104,8 @@ func (j *Joiner) Do(joinAddrs []string, id, addr string, voter bool) (string, er <nl> if err == nil { <nl> // Success! <nl> return joinee, nil <nl> + } else { <nl> + j.logger.Printf(\"failed to join via node at %s: %s\", a, err) <nl> } <nl> } <nl> if i+1 < j.numAttempts { <nl> @@ -166,14 +168,14 @@ func (j *Joiner) join(joinAddr, id, addr string, voter bool) (string, error) { <nl> // protocol a registered node is actually using. <nl> if strings.HasPrefix(fullAddr, \"https://\") { <nl> // It's already HTTPS, give up. <nl> - return \"\", fmt.Errorf(\"failed to join, node returned: %s: (%s)\", resp.Status, string(b)) <nl> + return \"\", fmt.Errorf(\"%s: (%s)\", resp.Status, string(b)) <nl> } <nl> j.logger.Print(\"join via HTTP failed, trying via HTTPS\") <nl> fullAddr = rurl.EnsureHTTPS(fullAddr) <nl> continue <nl> default: <nl> - return \"\", fmt.Errorf(\"failed to join, node returned: %s: (%s)\", resp.Status, string(b)) <nl> + return \"\", fmt.Errorf(\"%s: (%s)\", resp.Status, string(b)) <nl> } <nl> } <nl> } <nl> ", "msg": "Better logging for failure-to-join"}
{"diff_id": 1138, "repo": "rqlite/rqlite", "sha": "70ea78fd6ce21fee86f9d63d59d0438f80b129b3", "time": "28.11.2022 08:59:49", "diff": "mmm a / http/service.go <nl> ppp b / http/service.go <nl>@@ -178,6 +178,8 @@ const ( <nl> numExecutions = \"executions\" <nl> numQueuedExecutions = \"queued_executions\" <nl> numQueuedExecutionsOK = \"queued_executions_ok\" <nl> + numQueuedExecutionsStmtsRx = \"queued_executions_num_stmts_rx\" <nl> + numQueuedExecutionsStmtsTx = \"queued_executions_num_stmts_tx\" <nl> numQueuedExecutionsNoLeader = \"queued_executions_no_leader\" <nl> numQueuedExecutionsFailed = \"queued_executions_failed\" <nl> numQueuedExecutionsWait = \"queued_executions_wait\" <nl> @@ -220,6 +222,8 @@ func ResetStats() { <nl> stats.Add(numExecutions, 0) <nl> stats.Add(numQueuedExecutions, 0) <nl> stats.Add(numQueuedExecutionsOK, 0) <nl> + stats.Add(numQueuedExecutionsStmtsRx, 0) <nl> + stats.Add(numQueuedExecutionsStmtsTx, 0) <nl> stats.Add(numQueuedExecutionsNoLeader, 0) <nl> stats.Add(numQueuedExecutionsFailed, 0) <nl> stats.Add(numQueuedExecutionsWait, 0) <nl> @@ -1569,6 +1573,7 @@ func (s *Service) runQueue() { <nl> Transaction: s.DefaultQueueTx, <nl> }, <nl> } <nl> + stats.Add(numQueuedExecutionsStmtsRx, int64(len(req.Statements))) <nl> // Nil statements are valid, as clients may want to just send <nl> // a \"checkpoint\" through the queue. <nl> @@ -1609,6 +1614,7 @@ func (s *Service) runQueue() { <nl> s.seqNum = req.SequenceNumber <nl> s.seqNumMu.Unlock() <nl> req.Close() <nl> + stats.Add(numQueuedExecutionsStmtsTx, int64(len(req.Statements))) <nl> stats.Add(numQueuedExecutionsOK, 1) <nl> } <nl> } <nl> ", "msg": "More Queued Writes instrumentation"}
{"diff_id": 1157, "repo": "signalfx/signalfx-agent", "sha": "beecc82b5b64dff89f3d4dd8a3001de87dd560aa", "time": "30.03.2017 12:48:32", "diff": "mmm a / plugins/monitors/collectd/collectd.go <nl> ppp b / plugins/monitors/collectd/collectd.go <nl>@@ -48,6 +48,7 @@ type Collectd struct { <nl> reloadChan chan int <nl> stopChan chan int <nl> configMutex sync.Mutex <nl> + stateMutex sync.Mutex <nl> configDirty bool <nl> } <nl> @@ -169,10 +170,10 @@ func (collectd *Collectd) Write(services services.Instances) error { <nl> // reload reloads collectd configuration <nl> func (collectd *Collectd) reload() { <nl> - if collectd.state == Running { <nl> + if collectd.State() == Running { <nl> collectd.setState(Reloading) <nl> collectd.reloadChan <- 1 <nl> - for collectd.Status() == Reloading { <nl> + for collectd.State() == Reloading { <nl> time.Sleep(time.Duration(1) * time.Second) <nl> } <nl> } <nl> @@ -289,8 +290,9 @@ func (collectd *Collectd) createPluginsFromServices(sis services.Instances) ([]* <nl> // Start collectd monitoring <nl> func (collectd *Collectd) Start() (err error) { <nl> - println(\"starting collectd\") <nl> - if collectd.state == Running { <nl> + log.Println(\"starting collectd\") <nl> + <nl> + if collectd.State() == Running { <nl> return errors.New(\"already running\") <nl> } <nl> @@ -313,7 +315,7 @@ func (collectd *Collectd) Start() (err error) { <nl> // Stop collectd monitoring <nl> func (collectd *Collectd) Stop() { <nl> - if collectd.state != Stopped { <nl> + if collectd.State() != Stopped { <nl> collectd.stopChan <- 0 <nl> } <nl> } <nl> @@ -337,14 +339,19 @@ func (collectd *Collectd) GetWatchDirs(config *viper.Viper) []string { <nl> return config.GetStringSlice(\"templatesdirs\") <nl> } <nl> -// Status for collectd monitoring <nl> -func (collectd *Collectd) Status() string { <nl> - return collectd.state <nl> +// State for collectd monitoring <nl> +func (collectd *Collectd) State() string { <nl> + collectd.stateMutex.Lock() <nl> + state := collectd.state <nl> + collectd.stateMutex.Unlock() <nl> + return state <nl> } <nl> -// Status for collectd monitoring <nl> +// setState sets state for collectd monitoring <nl> func (collectd *Collectd) setState(state string) { <nl> + collectd.stateMutex.Lock() <nl> collectd.state = state <nl> + collectd.stateMutex.Unlock() <nl> } <nl> func (collectd *Collectd) run() { <nl> ", "msg": "Fix data races in collectd plugin\nMultiple gothreads were reading/writing to `state` which was causing data\nraces."}
{"diff_id": 1163, "repo": "signalfx/signalfx-agent", "sha": "2decb96768ee4af7f69bdc99b69d2a9d82f003ea", "time": "10.05.2017 17:42:52", "diff": "mmm a / config/config.go <nl> ppp b / config/config.go <nl>@@ -61,6 +61,8 @@ type userConfig struct { <nl> Mesosphere *struct { <nl> Cluster string <nl> Role string <nl> + SystemHealth bool `yaml:\"systemHealth,omitempty\"` <nl> + Verbose bool `yaml:\"verbose,omitempty\"` <nl> } <nl> } <nl> @@ -146,6 +148,8 @@ func loadUserConfig(pair *store.KVPair) error { <nl> // Set the cluster name for the mesos default plugin config <nl> staticPlugins[\"mesos\"] = map[string]interface{}{ <nl> \"cluster\": mesos.Cluster, <nl> + \"systemhealth\": mesos.SystemHealth, <nl> + \"verbose\": mesos.Verbose, <nl> } <nl> } <nl> ", "msg": "add verbose and system health options to mesos userconfig"}
{"diff_id": 1166, "repo": "signalfx/signalfx-agent", "sha": "52573a432ebcc2d25cfab73139837881a261848f", "time": "15.06.2017 12:28:33", "diff": "mmm a / plugins/monitors/collectd/collectd.go <nl> ppp b / plugins/monitors/collectd/collectd.go <nl>@@ -13,13 +13,12 @@ import ( <nl> \"errors\" <nl> \"fmt\" <nl> \"log\" <nl> + \"os\" <nl> \"sync\" <nl> \"time\" <nl> \"path/filepath\" <nl> - \"io/ioutil\" <nl> - <nl> cfg \"github.com/signalfx/neo-agent/config\" <nl> \"github.com/signalfx/neo-agent/plugins\" <nl> \"github.com/signalfx/neo-agent/plugins/monitors/collectd/config\" <nl> @@ -207,10 +206,24 @@ func (collectd *Collectd) writePlugins(plugins []*config.Plugin) error { <nl> return err <nl> } <nl> - if err := ioutil.WriteFile(collectd.confFile, []byte(config), 0644); err != nil { <nl> + f, err := os.Create(collectd.confFile) <nl> + if err != nil { <nl> + return fmt.Errorf(\"failed to truncate collectd config: %s\", err) <nl> + } <nl> + defer f.Close() <nl> + <nl> + _, err = f.Write([]byte(config)) <nl> + if err != nil { <nl> return fmt.Errorf(\"failed to write collectd config: %s\", err) <nl> } <nl> + // We need to sync here since collectd might be restarted very quickly <nl> + // after writing this. <nl> + err = f.Sync() <nl> + if err != nil { <nl> + return fmt.Errorf(\"failed to sync collectd config file to disk: %s\", err) <nl> + } <nl> + <nl> return nil <nl> } <nl> ", "msg": "Ensuring collectd config file is synced to disk\nThis was causing weird parse errors when the agent would start up collectd due\nto the config file only being partially written."}
{"diff_id": 1173, "repo": "signalfx/signalfx-agent", "sha": "35e3d6561bf4e1f1845cf3bd67ba34bd8ef5d8e1", "time": "25.09.2017 22:04:55", "diff": "mmm a / monitors/collectd/collectd.go <nl> ppp b / monitors/collectd/collectd.go <nl>@@ -142,6 +142,7 @@ func (cm *Manager) Shutdown() { <nl> if cm.State() != Stopped { <nl> cm.stopChan <- 0 <nl> cm.restartDebouncedStop <- struct{}{} <nl> + cm.restartDebouncedStop = nil <nl> } <nl> } <nl> @@ -196,19 +197,23 @@ func (cm *Manager) rerenderConf() bool { <nl> func (cm *Manager) runCollectd() { <nl> stoppedCh := make(chan struct{}, 1) <nl> + stop := func() { <nl> + cm.killChildProc() <nl> + <-stoppedCh <nl> + } <nl> + <nl> go cm.runAsChildProc(stoppedCh) <nl> for { <nl> select { <nl> case <-cm.stopChan: <nl> cm.setState(ShuttingDown) <nl> - cm.killChildProc() <nl> + stop() <nl> cm.setState(Stopped) <nl> return <nl> case <-cm.reloadChan: <nl> cm.setState(Restarting) <nl> - cm.killChildProc() <nl> - <-stoppedCh <nl> + stop() <nl> go cm.runAsChildProc(stoppedCh) <nl> cm.setState(Running) <nl> } <nl> ", "msg": "Making collectd restart properly when agent is reset with SIGHUP"}
{"diff_id": 1174, "repo": "signalfx/signalfx-agent", "sha": "5776885a06db01660407455344a88e60c94c41f0", "time": "27.09.2017 14:12:56", "diff": "mmm a / monitors/collectd/collectd.go <nl> ppp b / monitors/collectd/collectd.go <nl>@@ -196,10 +196,13 @@ func (cm *Manager) rerenderConf() bool { <nl> func (cm *Manager) runCollectd() { <nl> stoppedCh := make(chan struct{}, 1) <nl> + restartDelay := 2 * time.Second <nl> stop := func() { <nl> + cm.setState(ShuttingDown) <nl> cm.killChildProc() <nl> <-stoppedCh <nl> + cm.setState(Stopped) <nl> } <nl> go cm.runAsChildProc(stoppedCh) <nl> @@ -207,15 +210,22 @@ func (cm *Manager) runCollectd() { <nl> for { <nl> select { <nl> case <-cm.stopChan: <nl> - cm.setState(ShuttingDown) <nl> + log.Info(\"Stopping collectd\") <nl> stop() <nl> - cm.setState(Stopped) <nl> return <nl> case <-cm.reloadChan: <nl> - cm.setState(Restarting) <nl> stop() <nl> + log.Info(\"Collectd stopped, restarting\") <nl> + go cm.runAsChildProc(stoppedCh) <nl> + case <-stoppedCh: <nl> + if cm.state == Running { <nl> + log.Error(\"Collectd died when it was supposed to be running, restarting...\") <nl> + } else { <nl> + log.Warn(\"Collectd stopped in an unexpected way\") <nl> + continue <nl> + } <nl> + time.Sleep(restartDelay) <nl> go cm.runAsChildProc(stoppedCh) <nl> - cm.setState(Running) <nl> } <nl> } <nl> } <nl> @@ -231,8 +241,6 @@ func (cm *Manager) killChildProc() { <nl> } <nl> func (cm *Manager) runAsChildProc(stoppedCh chan<- struct{}) { <nl> - restartDelay := 2 * time.Second <nl> - for { <nl> log.Info(\"Starting Collectd child process\") <nl> cm.cmdMutex.Lock() <nl> @@ -253,22 +261,7 @@ func (cm *Manager) runAsChildProc(stoppedCh chan<- struct{}) { <nl> cm.cmdMutex.Unlock() <nl> cm.cmd.Wait() <nl> - log.Infof(\"Collectd state is %s\", cm.state) <nl> - // This should always be set whenever we call the cancel func <nl> - // corresponding to the `ctx` so that we can know whether the proc died <nl> - // on purpose or not. <nl> - if cm.state != Running { <nl> - log.Info(\"Not restarting Collectd because it is not supposed to be running\") <nl> stoppedCh <- struct{}{} <nl> - return <nl> - } <nl> - <nl> - log.WithFields(log.Fields{ <nl> - \"error\": err, <nl> - }).Error(\"Collectd child process died, restarting...\") <nl> - <nl> - time.Sleep(restartDelay) <nl> - } <nl> } <nl> // Delete existing config in case there were plugins configured before that won't <nl> ", "msg": "Fixing race condition in collectd child proc handling\nIt mainly revolves around making the stoppedCh channel not hang when reading"}
{"diff_id": 1176, "repo": "signalfx/signalfx-agent", "sha": "116c6cf4f87798a6c66ebe02d1814117d88676a5", "time": "16.11.2017 21:48:38", "diff": "mmm a / core/common/kubernetes/client.go <nl> ppp b / core/common/kubernetes/client.go <nl>@@ -35,6 +35,9 @@ type APIConfig struct { <nl> // Validate validates the K8s API config <nl> func (c *APIConfig) Validate() error { <nl> + if c.AuthType != AuthTypeNone && c.AuthType != AuthTypeTLS && c.AuthType != AuthTypeServiceAccount { <nl> + return errors.New(\"Invalid authType for kubernetes: \" + string(c.AuthType)) <nl> + } <nl> if c.AuthType == AuthTypeTLS && (c.ClientCertPath == \"\" || c.ClientKeyPath == \"\") { <nl> return errors.New(\"For TLS auth, you must set both the kubernetesAPI.clientCertPath \" + <nl> \"and kubernetesAPI.clientKeyPath config values\") <nl> ", "msg": "Adding better validation of k8s authType config"}
{"diff_id": 1177, "repo": "signalfx/signalfx-agent", "sha": "28a51e66daf4a86e8aa06efe2d4927d17d4036de", "time": "29.11.2017 10:59:43", "diff": "mmm a / observers/kubernetes/api.go <nl> ppp b / observers/kubernetes/api.go <nl>@@ -26,7 +26,6 @@ var now = time.Now <nl> const ( <nl> observerType = \"k8s-api\" <nl> nodeEnvVar = \"MY_NODE_NAME\" <nl> - namespaceEnvVar = \"MY_NAMESPACE\" <nl> runningPhase = \"Running\" <nl> ) <nl> @@ -52,10 +51,6 @@ func (c *Config) Validate() error { <nl> return err <nl> } <nl> - if os.Getenv(namespaceEnvVar) == \"\" { <nl> - return fmt.Errorf(\"K8s namespace was not provided in the %s envvar\", namespaceEnvVar) <nl> - } <nl> - <nl> if os.Getenv(nodeEnvVar) == \"\" { <nl> return fmt.Errorf(\"K8s node name was not provided in the %s envvar\", nodeEnvVar) <nl> } <nl> @@ -67,7 +62,6 @@ func (c *Config) Validate() error { <nl> type Observer struct { <nl> config *Config <nl> clientset *k8s.Clientset <nl> - thisNamespace string <nl> thisNode string <nl> serviceCallbacks *observers.ServiceCallbacks <nl> stopper chan struct{} <nl> @@ -75,7 +69,6 @@ type Observer struct { <nl> // Configure configures and starts watching for endpoints <nl> func (o *Observer) Configure(config *Config) bool { <nl> - o.thisNamespace = os.Getenv(namespaceEnvVar) <nl> o.thisNode = os.Getenv(nodeEnvVar) <nl> var err error <nl> @@ -84,11 +77,7 @@ func (o *Observer) Configure(config *Config) bool { <nl> return false <nl> } <nl> - // Stop previous informers <nl> - if o.stopper != nil { <nl> - o.stopper <- struct{}{} <nl> - } <nl> - <nl> + o.stopIfRunning() <nl> o.watchPods() <nl> return true <nl> @@ -98,7 +87,7 @@ func (o *Observer) watchPods() { <nl> o.stopper = make(chan struct{}) <nl> client := o.clientset.Core().RESTClient() <nl> - watchList := cache.NewListWatchFromClient(client, \"pods\", o.thisNamespace, fields.Everything()) <nl> + watchList := cache.NewListWatchFromClient(client, \"pods\", \"\", fields.Everything()) <nl> _, controller := cache.NewInformer( <nl> watchList, <nl> @@ -119,7 +108,19 @@ func (o *Observer) watchPods() { <nl> go controller.Run(o.stopper) <nl> } <nl> +func (o *Observer) stopIfRunning() { <nl> + // Stop previous informers <nl> + if o.stopper != nil { <nl> + o.stopper <- struct{}{} <nl> + o.stopper = nil <nl> + } <nl> +} <nl> + <nl> +// Handles notifications of changes to pods from the API server <nl> func (o *Observer) changeHandler(oldPod *v1.Pod, newPod *v1.Pod) { <nl> + if o.stopper == nil { <nl> + return <nl> + } <nl> // If it is an update, there will be a remove and immediately subsequent <nl> // add. <nl> if oldPod != nil && oldPod.Spec.NodeName == o.thisNode { <nl> @@ -203,3 +204,8 @@ func endpointsInPod(pod *v1.Pod) []services.Endpoint { <nl> } <nl> return endpoints <nl> } <nl> + <nl> +// Shutdown the service differ routine <nl> +func (o *Observer) Shutdown() { <nl> + o.stopIfRunning() <nl> +} <nl> ", "msg": "Making k8s api observer look across all namespaces\nAlso adding shutdown so it doesn't keep running"}
{"diff_id": 1179, "repo": "signalfx/signalfx-agent", "sha": "897893e0f0db958c491006a3250ebe3d8c147c33", "time": "21.12.2017 13:58:51", "diff": "mmm a / core/config/config.go <nl> ppp b / core/config/config.go <nl>@@ -53,15 +53,22 @@ type Config struct { <nl> } <nl> func (c *Config) setDefaultHostname() { <nl> - fqdn := fqdn.Get() <nl> - if fqdn == \"unknown\" { <nl> - log.Info(\"Error getting fully qualified hostname\") <nl> - } else { <nl> - log.Infof(\"Using hostname %s\", fqdn) <nl> - c.Hostname = fqdn <nl> + host := fqdn.Get() <nl> + if host == \"unknown\" || host == \"localhost\" { <nl> + log.Info(\"Error getting fully qualified hostname, using plain hostname\") <nl> + <nl> + var err error <nl> + host, err = os.Hostname() <nl> + if err != nil { <nl> + log.Error(\"Error getting system simple hostname, cannot set hostname\") <nl> + return <nl> } <nl> } <nl> + log.Infof(\"Using hostname %s\", host) <nl> + c.Hostname = host <nl> +} <nl> + <nl> func (c *Config) initialize(metaStore *stores.MetaStore) (*Config, error) { <nl> c.overrideFromEnv() <nl> ", "msg": "Making sure hostname is never resolved to localhost"}
{"diff_id": 1183, "repo": "signalfx/signalfx-agent", "sha": "b2c657cc521da33f2be0d65bcba17e4f52ddb2c1", "time": "02.02.2018 17:22:12", "diff": "mmm a / monitors/diagnostics.go <nl> ppp b / monitors/diagnostics.go <nl>@@ -37,6 +37,13 @@ func (mm *MonitorManager) DiagnosticText() string { <nl> mm.lock.Lock() <nl> defer mm.lock.Unlock() <nl> + configurationText := \"\\n\" <nl> + for i := range mm.monitorConfigs { <nl> + configurationText += fmt.Sprintf( <nl> + \"%s\\n\", <nl> + utils.IndentLines(config.ToString(mm.monitorConfigs[i]), 2)) <nl> + } <nl> + <nl> activeMonText := \"\" <nl> for i := range mm.activeMonitors { <nl> am := mm.activeMonitors[i] <nl> @@ -70,12 +77,15 @@ func (mm *MonitorManager) DiagnosticText() string { <nl> } <nl> return fmt.Sprintf( <nl> + \"Monitor Configurations (Not necessarily active):\\n\"+ <nl> + \"%s\"+ <nl> \"Active Monitors:\\n\"+ <nl> \"%s\"+ <nl> \"Discovered Endpoints:\\n\"+ <nl> \"%s\\n\"+ <nl> \"Bad Monitor Configurations:\\n\"+ <nl> \"%s\\n\", <nl> + configurationText, <nl> activeMonText, <nl> discoveredEndpointsText, <nl> badConfigText(mm.badConfigs)) <nl> ", "msg": "Adding diagnostic output for configured monitors"}
{"diff_id": 1186, "repo": "signalfx/signalfx-agent", "sha": "fdcbd2e006da125ef65f87f18fbf146a5231dd91", "time": "04.02.2018 22:12:54", "diff": "mmm a / core/config/sources/cacher.go <nl> ppp b / core/config/sources/cacher.go <nl>@@ -32,7 +32,6 @@ func (c *configSourceCacher) Get(path string, optional bool) (map[string][]byte, <nl> if v, ok := c.cache[path]; ok { <nl> return v, nil <nl> } <nl> - log.Infof(\"Getting %s for cacher\", path) <nl> v, version, err := c.source.Get(path) <nl> if err != nil { <nl> if _, ok := err.(types.ErrNotFound); !ok || !optional { <nl> @@ -42,7 +41,6 @@ func (c *configSourceCacher) Get(path string, optional bool) (map[string][]byte, <nl> c.cache[path] = v <nl> - log.Infof(\"Set %s = %s for cacher\", path, v) <nl> if c.shouldWatch { <nl> // From now on, subsequent Gets will read from the cache. It is the <nl> // responsibility of the watch method to keep the cache up to date. <nl> @@ -55,12 +53,15 @@ func (c *configSourceCacher) Get(path string, optional bool) (map[string][]byte, <nl> func (c *configSourceCacher) watch(path string, version uint64) { <nl> for { <nl> var err error <nl> - log.Debugf(\"Cache is watching %s://%s:%d\", c.source.Name(), path, version) <nl> err = c.source.WaitForChange(path, version, c.stop) <nl> if utils.IsSignalChanClosed(c.stop) { <nl> return <nl> } <nl> if err != nil { <nl> + // If the file isn't found, just continue <nl> + if _, ok := err.(types.ErrNotFound); ok { <nl> + continue <nl> + } <nl> log.WithFields(log.Fields{ <nl> \"path\": path, <nl> \"source\": c.source.Name(), <nl> ", "msg": "Cleaning up logging for file sources"}
{"diff_id": 1188, "repo": "signalfx/signalfx-agent", "sha": "10c95b6ed808ee2f4fc8dfe8b65889b1e1310d64", "time": "07.02.2018 11:02:05", "diff": "mmm a / monitors/cadvisor/converter/sfxConverter.go <nl> ppp b / monitors/cadvisor/converter/sfxConverter.go <nl>@@ -150,6 +150,30 @@ func getContainerMetrics(excludedMetrics map[string]bool) []containerMetric { <nl> return metricValues <nl> }, <nl> }, <nl> + { <nl> + name: \"container_cpu_cfs_periods\", <nl> + help: \"Total number of elapsed CFS enforcement intervals.\", <nl> + valueType: datapoint.Counter, <nl> + getValues: func(s *info.ContainerStats) metricValues { <nl> + return metricValues{{value: datapoint.NewIntValue(int64(s.Cpu.CFS.Periods))}} <nl> + }, <nl> + }, <nl> + { <nl> + name: \"container_cpu_cfs_throttled_periods\", <nl> + help: \"Total number of times tasks in the cgroup have been throttled\", <nl> + valueType: datapoint.Counter, <nl> + getValues: func(s *info.ContainerStats) metricValues { <nl> + return metricValues{{value: datapoint.NewIntValue(int64(s.Cpu.CFS.ThrottledPeriods))}} <nl> + }, <nl> + }, <nl> + { <nl> + name: \"container_cpu_cfs_throttled_time\", <nl> + help: \"Total time duration, in nanoseconds, for which tasks in the cgroup have been throttled.\", <nl> + valueType: datapoint.Counter, <nl> + getValues: func(s *info.ContainerStats) metricValues { <nl> + return metricValues{{value: datapoint.NewIntValue(int64(s.Cpu.CFS.ThrottledTime))}} <nl> + }, <nl> + }, <nl> { <nl> name: \"container_memory_failcnt\", <nl> help: \"Number of memory usage hits limits\", <nl> @@ -490,6 +514,28 @@ func getContainerSpecCPUMetrics(excludedMetrics map[string]bool) []containerSpec <nl> return metricValues{{value: datapoint.NewIntValue(int64(container.Spec.Cpu.Limit))}} <nl> }, <nl> }, <nl> + { <nl> + containerMetric: containerMetric{ <nl> + name: \"container_spec_cpu_quota\", <nl> + help: \"\", <nl> + valueType: datapoint.Gauge, <nl> + extraLabels: []string{}, <nl> + }, <nl> + getValues: func(container *info.ContainerInfo) metricValues { <nl> + return metricValues{{value: datapoint.NewIntValue(int64(container.Spec.Cpu.Quota))}} <nl> + }, <nl> + }, <nl> + { <nl> + containerMetric: containerMetric{ <nl> + name: \"container_spec_cpu_period\", <nl> + help: \"\", <nl> + valueType: datapoint.Gauge, <nl> + extraLabels: []string{}, <nl> + }, <nl> + getValues: func(container *info.ContainerInfo) metricValues { <nl> + return metricValues{{value: datapoint.NewIntValue(int64(container.Spec.Cpu.Period))}} <nl> + }, <nl> + }, <nl> } <nl> var index = 0 <nl> ", "msg": "Adding cAdvisor CPU CFS quota metrics\nThis should make it possible to better monitor container CPU usage vs. the\nallocated limits.  The exisitng container_spec_cpu_shares metric only shows\nrequested amounts and not enforced limits."}
{"diff_id": 1200, "repo": "signalfx/signalfx-agent", "sha": "d354de8ac14ce017fe14484541e59043c919b7aa", "time": "06.03.2018 15:38:32", "diff": "mmm a / internal/core/config/writer.go <nl> ppp b / internal/core/config/writer.go <nl>@@ -26,9 +26,9 @@ type WriterConfig struct { <nl> // The agent does not send datapoints immediately upon a monitor generating <nl> // them, but buffers them and sends them in batches. The lower this <nl> // number, the less delay for datapoints to appear in SignalFx. <nl> - DatapointSendIntervalSeconds int `yaml:\"datapointSendIntervalSeconds\" default:\"5\"` <nl> + DatapointSendIntervalSeconds int `yaml:\"datapointSendIntervalSeconds\" default:\"1\"` <nl> // The analogue of `datapointSendIntervalSeconds` for events <nl> - EventSendIntervalSeconds int `yaml:\"eventSendIntervalSeconds\" default:\"5\"` <nl> + EventSendIntervalSeconds int `yaml:\"eventSendIntervalSeconds\" default:\"1\"` <nl> // If the log level is set to `debug` and this is true, all datapoints <nl> // generated by the agent will be logged. <nl> LogDatapoints bool `yaml:\"logDatapoints\"` <nl> ", "msg": "Reducing default batching time for dps and events"}
{"diff_id": 1207, "repo": "signalfx/signalfx-agent", "sha": "1ef64d17bddc0309ee5be72184be000c4959812f", "time": "03.04.2018 12:57:38", "diff": "mmm a / internal/monitors/cadvisor/converter/converter.go <nl> ppp b / internal/monitors/cadvisor/converter/converter.go <nl>@@ -743,6 +743,7 @@ func copyDims(dims map[string]string) map[string]string { <nl> // DIMENSION(container_image): The container image name <nl> func (c *CadvisorCollector) collectContainersInfo() { <nl> + now := time.Now() <nl> containers, err := c.infoProvider.SubcontainersInfo(\"/\") <nl> if err != nil { <nl> //c.errors.Set(1) <nl> @@ -766,7 +767,6 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> dims[\"kubernetes_pod_name\"] = container.Spec.Labels[\"io.kubernetes.pod.name\"] <nl> dims[\"kubernetes_namespace\"] = container.Spec.Labels[\"io.kubernetes.pod.namespace\"] <nl> - tt := time.Now() <nl> // Container spec <nl> for _, cm := range c.containerSpecMetrics { <nl> for _, metricValue := range cm.getValues(&container) { <nl> @@ -777,7 +777,7 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> newDims[label] = metricValue.labels[i] <nl> } <nl> - c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, tt)) <nl> + c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, now)) <nl> } <nl> } <nl> @@ -791,7 +791,7 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> newDims[label] = metricValue.labels[i] <nl> } <nl> - c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, tt)) <nl> + c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, now)) <nl> } <nl> } <nl> } <nl> @@ -806,7 +806,7 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> newDims[label] = metricValue.labels[i] <nl> } <nl> - c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, tt)) <nl> + c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, now)) <nl> } <nl> } <nl> } <nl> @@ -826,7 +826,7 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> newDims[label] = metricValue.labels[i] <nl> } <nl> - c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, tt)) <nl> + c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, now)) <nl> } <nl> } <nl> } <nl> @@ -836,6 +836,7 @@ func (c *CadvisorCollector) collectContainersInfo() { <nl> func (c *CadvisorCollector) collectVersionInfo() {} <nl> func (c *CadvisorCollector) collectMachineInfo() { <nl> + now := time.Now() <nl> machineInfo, err := c.infoProvider.GetMachineInfo() <nl> if err != nil { <nl> //c.errors.Set(1) <nl> @@ -847,7 +848,6 @@ func (c *CadvisorCollector) collectMachineInfo() { <nl> } <nl> dims := make(map[string]string) <nl> - tt := time.Now() <nl> for _, cm := range c.machineInfoMetrics { <nl> for _, metricValue := range cm.getValues(machineInfo) { <nl> @@ -858,7 +858,7 @@ func (c *CadvisorCollector) collectMachineInfo() { <nl> newDims[label] = metricValue.labels[i] <nl> } <nl> - c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, tt)) <nl> + c.sendDatapoint(datapoint.New(cm.name, newDims, metricValue.value, cm.valueType, now)) <nl> } <nl> } <nl> } <nl> ", "msg": "Making cadvisor metric timestamp more regular\nIt was previously being set after the kubelet/cadvsior request which\ncauses an incorrect interval if kubelet/cadvisor requests have\nsignificantly different durations"}
{"diff_id": 1210, "repo": "signalfx/signalfx-agent", "sha": "2f47c85dbb0d64b965f1a65af9d750142777a6a4", "time": "04.04.2018 17:11:10", "diff": "mmm a / internal/monitors/kubernetes/cluster/metrics/cache.go <nl> ppp b / internal/monitors/kubernetes/cluster/metrics/cache.go <nl>@@ -138,7 +138,12 @@ func (dc *DatapointCache) AllDatapoints() []*datapoint.Datapoint { <nl> for k := range dc.dpCache { <nl> if dc.dpCache[k] != nil { <nl> - dps = append(dps, dc.dpCache[k]...) <nl> + for i := range dc.dpCache[k] { <nl> + // Copy the datapoint since nothing in datapoints is thread <nl> + // safe. <nl> + dp := *dc.dpCache[k][i] <nl> + dps = append(dps, &dp) <nl> + } <nl> } <nl> } <nl> ", "msg": "Copying k8s dps since they aren't thread safe\nThis only becomes an issue if intervals start overlapping due to CPU limits"}
{"diff_id": 1225, "repo": "signalfx/signalfx-agent", "sha": "b45e310eb2f38a65ada16f41dbc8bde850057f63", "time": "27.07.2018 15:44:31", "diff": "mmm a / cmd/agent/main.go <nl> ppp b / cmd/agent/main.go <nl>@@ -8,6 +8,7 @@ import ( <nl> \"os/signal\" <nl> \"strings\" <nl> \"syscall\" <nl> + \"time\" <nl> log \"github.com/sirupsen/logrus\" <nl> @@ -130,7 +131,14 @@ func runAgent() { <nl> case <-interruptCh: <nl> log.Info(\"Interrupt signal received, stopping agent\") <nl> shutdown() <nl> - <-shutdownComplete <nl> + select { <nl> + case <-shutdownComplete: <nl> + break <nl> + case <-time.After(10 * time.Second): <nl> + log.Error(\"Shutdown timed out, forcing process down\") <nl> + break <nl> + } <nl> + <nl> close(exit) <nl> } <nl> }() <nl> ", "msg": "Force agent to shutdown after 10 seconds of interrupt"}
{"diff_id": 1227, "repo": "signalfx/signalfx-agent", "sha": "f76e17592b8a8136160bce44fd741ef15a313763", "time": "05.09.2018 13:17:38", "diff": "mmm a / internal/core/common/docker/containerlist.go <nl> ppp b / internal/core/common/docker/containerlist.go <nl>@@ -25,16 +25,18 @@ func ListAndWatchContainers(ctx context.Context, client *docker.Client, changeHa <nl> containers := make(map[string]*dtypes.ContainerJSON) <nl> // Make sure you hold the lock before calling this <nl> - updateContainer := func(id string) { <nl> + updateContainer := func(id string) bool { <nl> inspectCtx, cancel := context.WithTimeout(ctx, 5*time.Second) <nl> c, err := client.ContainerInspect(inspectCtx, id) <nl> + defer cancel() <nl> if err != nil { <nl> logger.WithError(err).Errorf(\"Could not inspect updated container %s\", id) <nl> } else if imageFilter == nil || !imageFilter.Matches(c.Config.Image) { <nl> logger.Debugf(\"Updated Docker container %s\", id) <nl> containers[id] = &c <nl> + return true <nl> } <nl> - cancel() <nl> + return false <nl> } <nl> watchStarted := make(chan struct{}) <nl> @@ -78,13 +80,16 @@ func ListAndWatchContainers(ctx context.Context, client *docker.Client, changeHa <nl> // be unbounded. <nl> case \"destroy\": <nl> logger.Debugf(\"Docker container was destroyed: %s\", event.ID) <nl> + if _, ok := containers[event.ID]; ok { <nl> delete(containers, event.ID) <nl> changeHandler(containers[event.ID], nil) <nl> + } <nl> default: <nl> oldContainer := containers[event.ID] <nl> - updateContainer(event.ID) <nl> + if updateContainer(event.ID) { <nl> changeHandler(oldContainer, containers[event.ID]) <nl> } <nl> + } <nl> lock.Unlock() <nl> ", "msg": "Making docker watch logic only trigger handler on real updates"}
{"diff_id": 1228, "repo": "signalfx/signalfx-agent", "sha": "39495fdbb97b125181752dd2cf2d7a1d7da1be49", "time": "11.09.2018 10:19:44", "diff": "mmm a / internal/core/config/loader.go <nl> ppp b / internal/core/config/loader.go <nl>@@ -133,6 +133,10 @@ var envVarRE = regexp.MustCompile(`\\${\\s*([\\w-]+?)\\s*}`) <nl> // still get to them when we need to rerender config <nl> var envVarCache = make(map[string]string) <nl> +var envVarWhitelist = map[string]bool{ <nl> + \"MY_NODE_NAME\": true, <nl> +} <nl> + <nl> // Replaces envvar syntax with the actual envvars <nl> func preprocessConfig(content []byte) []byte { <nl> return envVarRE.ReplaceAllFunc(content, func(bs []byte) []byte { <nl> @@ -149,8 +153,10 @@ func preprocessConfig(content []byte) []byte { <nl> \"envvar\": envvar, <nl> }).Debug(\"Sanitizing envvar from agent\") <nl> + if !envVarWhitelist[envvar] { <nl> os.Unsetenv(envvar) <nl> } <nl> + } <nl> return []byte(val) <nl> }) <nl> ", "msg": "Allow node name to stay in the env if referenced in config"}
{"diff_id": 1237, "repo": "signalfx/signalfx-agent", "sha": "1816315dabd85db431eeacb1486191f1c4493268", "time": "04.01.2019 13:16:23", "diff": "mmm a / internal/monitors/collectd/solr/solr.go <nl> ppp b / internal/monitors/collectd/solr/solr.go <nl>@@ -39,8 +39,6 @@ type Config struct { <nl> IncludeMetric string `yaml:\"includeMetric\"` <nl> // ExcludeMetric metric name from the /admin/metrics endpoint to exclude (valid when EnhancedMetrics is \"true\") <nl> ExcludeMetric string `yaml:\"excludeMetric\"` <nl> - // Dimension space-separated key-value pair for a user-defined dimension <nl> - Dimension string `yaml:\"dimension\"` <nl> } <nl> // PythonConfig returns the embedded python.Config struct from the interface <nl> @@ -69,7 +67,6 @@ func (m *Monitor) Configure(conf *Config) error { <nl> \"EnhancedMetrics\": conf.EnhancedMetrics, <nl> \"IncludeMetric\": conf.IncludeMetric, <nl> \"ExcludeMetric\": conf.ExcludeMetric, <nl> - \"Dimension\": conf.Dimension, <nl> \"Interval\": conf.IntervalSeconds, <nl> }, <nl> } <nl> ", "msg": "dimension field not necessary because embedded extraDimensions field"}
{"diff_id": 1252, "repo": "signalfx/signalfx-agent", "sha": "d6090a70ccc4577ece3a86aa1ae6ce5c94151ef1", "time": "19.07.2019 06:53:58", "diff": "mmm a / internal/core/writer/writer.go <nl> ppp b / internal/core/writer/writer.go <nl>@@ -225,7 +225,7 @@ func (sw *SignalFxWriter) preprocessDatapoint(dp *datapoint.Datapoint) { <nl> func (sw *SignalFxWriter) sendDatapoints(dps []*datapoint.Datapoint) error { <nl> // This sends synchonously <nl> - err := sw.client.AddDatapoints(context.Background(), dps) <nl> + err := sw.client.AddDatapoints(sw.ctx, dps) <nl> if err != nil { <nl> log.WithFields(log.Fields{ <nl> \"error\": err, <nl> ", "msg": "Use writer context when sending datapointsg"}
{"diff_id": 1254, "repo": "signalfx/signalfx-agent", "sha": "98d25a3de2612cbb65728eb6e1c5ee381fa6dc95", "time": "22.08.2019 04:23:34", "diff": "mmm a / internal/core/writer/spans.go <nl> ppp b / internal/core/writer/spans.go <nl>@@ -7,6 +7,7 @@ import ( <nl> \"github.com/davecgh/go-spew/spew\" <nl> \"github.com/signalfx/golib/datapoint\" <nl> \"github.com/signalfx/golib/trace\" <nl> + \"github.com/signalfx/signalfx-agent/internal/core/common/constants\" <nl> \"github.com/signalfx/signalfx-agent/internal/core/common/dpmeta\" <nl> \"github.com/signalfx/signalfx-agent/internal/core/writer/tracetracker\" <nl> \"github.com/signalfx/signalfx-agent/internal/utils\" <nl> @@ -151,6 +152,8 @@ func (sw *SignalFxWriter) preprocessSpan(span *trace.Span) { <nl> delete(span.Tags, dpmeta.NotHostSpecificMeta) <nl> } <nl> + // adding smart agent version as a tag <nl> + span.Tags[\"signalfx.smartagent.version\"] = constants.Version <nl> if sw.conf.LogTraceSpans { <nl> log.Debugf(\"Sending trace span:\\n%s\", spew.Sdump(span)) <nl> } <nl> ", "msg": "Adding smart agent version as a tag to spans"}
{"diff_id": 1258, "repo": "signalfx/signalfx-agent", "sha": "fd4da9211eb6b9b53cdfb0dcfa010b5474f6a5e8", "time": "26.09.2019 08:15:59", "diff": "mmm a / internal/monitors/kubernetes/cluster/metrics/pods.go <nl> ppp b / internal/monitors/kubernetes/cluster/metrics/pods.go <nl>@@ -65,11 +65,6 @@ func dimPropsForPod(cachedPod *k8sutil.CachedPod, sc *k8sutil.ServiceCache, <nl> props[utils.LowercaseFirstChar(or.Kind)+\"_uid\"] = string(or.UID) <nl> } <nl> - tolerationProps := getPropsFromTolerations(cachedPod.Tolerations) <nl> - for k, v := range tolerationProps { <nl> - props[k] = v <nl> - } <nl> - <nl> // if pod is selected by a service, sync service as a tag <nl> serviceTags := sc.GetMatchingServices(cachedPod) <nl> for _, tag := range serviceTags { <nl> ", "msg": "Remove K8s pod toleration properties until some better way of storing them can be found"}
{"diff_id": 1268, "repo": "signalfx/signalfx-agent", "sha": "dd518f78d5a2f3072ca99121236aa78813d1b7a2", "time": "29.03.2020 18:38:09", "diff": "mmm a / pkg/core/writer/spans.go <nl> ppp b / pkg/core/writer/spans.go <nl>@@ -18,8 +18,18 @@ func (sw *SignalFxWriter) sendSpans(ctx context.Context, spans []*trace.Span) er <nl> sw.serviceTracker.AddSpans(sw.ctx, spans) <nl> } <nl> - // This sends synchronously <nl> - return sw.client.AddSpans(context.Background(), spans) <nl> + // This sends synchonously <nl> + err := sw.client.AddSpans(context.Background(), spans) <nl> + if err != nil { <nl> + log.WithFields(log.Fields{ <nl> + \"error\": err, <nl> + }).Error(\"Error shipping spans to SignalFx\") <nl> + // If there is an error sending spans then just forget about them. <nl> + return err <nl> + } <nl> + log.Debugf(\"Sent %d spans out of the agent\", len(spans)) <nl> + <nl> + return nil <nl> } <nl> // Mutates span tags in place to add global span tags. Also <nl> ", "msg": "Add log error msg when there are errors sending spans\nThis was done symmetrically to what is done for SFx datapoints."}
{"diff_id": 1274, "repo": "signalfx/signalfx-agent", "sha": "c58a487246c2cb6789f22983a6b1901e54fd756d", "time": "29.06.2020 16:05:48", "diff": "mmm a / pkg/monitors/kubernetes/kubeletmetrics/metrics.go <nl> ppp b / pkg/monitors/kubernetes/kubeletmetrics/metrics.go <nl>@@ -13,6 +13,7 @@ func convertContainerMetrics(c *v1alpha1.ContainerStats, status *v1.ContainerSta <nl> if status != nil { <nl> dims[\"container_id\"] = k8sutil.StripContainerID(status.ContainerID) <nl> + dims[\"container_image\"] = status.Image <nl> } <nl> dims[\"container_spec_name\"] = c.Name <nl> ", "msg": "kubelet-metrics monitor: Add container_image dim when using pod endpoint"}
{"diff_id": 1275, "repo": "signalfx/signalfx-agent", "sha": "0b015a28a516bc0b56e7ce76deadb80f43dc8393", "time": "07.07.2020 09:26:09", "diff": "mmm a / pkg/monitors/processlist/processlist_windows.go <nl> ppp b / pkg/monitors/processlist/processlist_windows.go <nl>@@ -57,21 +57,25 @@ var getAllProcesses = func() (ps []Win32Process, err error) { <nl> var getUsername = func(id uint32) (username string, err error) { <nl> // open the process handle and collect any information that requires it <nl> var h windows.Handle <nl> - defer func() { _ = windows.CloseHandle(h) }() <nl> if h, err = windows.OpenProcess(processQueryLimitedInformation, false, id); err != nil { <nl> err = fmt.Errorf(\"unable to open process handle. %v\", err) <nl> return username, err <nl> } <nl> + // Deferring CloseHandle(h) before it is set require a reference on the closure, avoid that <nl> + // by only deferring it only after the handle is successfully opened. <nl> + defer func(h windows.Handle) { _ = windows.CloseHandle(h) }(h) <nl> // the windows api docs suggest that windows.TOKEN_READ is a super set of windows.TOKEN_QUERY, <nl> // but in practice windows.TOKEN_READ seems to be less permissive for the admin user <nl> var token windows.Token <nl> - defer token.Close() <nl> err = windows.OpenProcessToken(h, windows.TOKEN_QUERY, &token) <nl> if err != nil { <nl> err = fmt.Errorf(\"unable to retrieve process token. %v\", err) <nl> return username, err <nl> } <nl> + // Do not defer token.Close right after declaration, only after the token is successfully set <nl> + // since token.Close is a value receiver. <nl> + defer token.Close() <nl> // extract the user from the process token <nl> user, err := token.GetTokenUser() <nl> ", "msg": "Fix leak of tokens in processlist monitor on Windows\n* Fix leak of tokens in processlist monitor on Windows\nThe defer call of token.Close() can only be made after the value of the\ntoken is set, otherwise, it \"closes\" an invalid handle.\n* Ignore error returned by windows.CloseHandle()"}
{"diff_id": 1280, "repo": "signalfx/signalfx-agent", "sha": "2682d13249cfd0b072899b05d7e4f765b736c4eb", "time": "07.10.2020 14:27:18", "diff": "mmm a / pkg/monitors/vsphere/service/inventory.go <nl> ppp b / pkg/monitors/vsphere/service/inventory.go <nl>@@ -131,6 +131,12 @@ func (svc *InventorySvc) followHost( <nl> if err != nil { <nl> return err <nl> } <nl> + <nl> + if host.Runtime.PowerState == types.HostSystemPowerStatePoweredOff { <nl> + svc.log.Debugf(\"inventory: host powered off: name=[%s]\", host.Name) <nl> + return nil <nl> + } <nl> + <nl> svc.debug(&host) <nl> dims = append(dims, pair{\"esx_ip\", host.Name}) <nl> hostDims := map[string]string{} <nl> @@ -156,6 +162,12 @@ func (svc *InventorySvc) followVM( <nl> if err != nil { <nl> return err <nl> } <nl> + <nl> + if vm.Runtime.PowerState == types.VirtualMachinePowerStatePoweredOff { <nl> + svc.log.Debugf(\"inventory: vm powered off: name=[%s]\", vm.Name) <nl> + return nil <nl> + } <nl> + <nl> svc.debug(&vm) <nl> vmDims := map[string]string{ <nl> \"vm_name\": vm.Name, // e.g. \"MyDebian10Host\" <nl> ", "msg": "Skip powered-off VMs and hosts\nThe vmomi sdk reports CPU values of -0.01 for powered off VMs. This change\nskips adding powered off VMs and hosts to the monitor's inventory cache."}
{"diff_id": 1287, "repo": "signalfx/signalfx-agent", "sha": "ce166a6fa14cba30d0559bb87b0f6d4f9cde0784", "time": "18.12.2020 16:20:01", "diff": "mmm a / pkg/monitors/prometheusexporter/prometheus.go <nl> ppp b / pkg/monitors/prometheusexporter/prometheus.go <nl>@@ -7,6 +7,7 @@ import ( <nl> \"io\" <nl> \"io/ioutil\" <nl> \"net/http\" <nl> + \"strings\" <nl> \"time\" <nl> \"github.com/signalfx/signalfx-agent/pkg/core/common/auth\" <nl> @@ -170,8 +171,14 @@ func doFetch(fetch fetcher) ([]*dto.MetricFamily, error) { <nl> return nil, err <nl> } <nl> defer body.Close() <nl> + var decoder expfmt.Decoder <nl> + // some \"text\" responses are missing \\n from the last line <nl> + if expformat != expfmt.FmtProtoDelim { <nl> + decoder = expfmt.NewDecoder(io.MultiReader(body, strings.NewReader(\"\\n\")), expformat) <nl> + } else { <nl> + decoder = expfmt.NewDecoder(body, expformat) <nl> + } <nl> - decoder := expfmt.NewDecoder(body, expformat) <nl> var mfs []*dto.MetricFamily <nl> for { <nl> ", "msg": "Append new line to the reader stream to make sure it's prometheus happy"}
{"diff_id": 1288, "repo": "signalfx/signalfx-agent", "sha": "14908338bb39d533bd4716c972de0f9b93cbb120", "time": "22.12.2020 08:37:11", "diff": "mmm a / pkg/core/writer/tracetracker/host.go <nl> ppp b / pkg/core/writer/tracetracker/host.go <nl>@@ -9,7 +9,6 @@ import ( <nl> \"github.com/signalfx/golib/v3/trace\" <nl> \"github.com/sirupsen/logrus\" <nl> - apmtracker \"github.com/signalfx/signalfx-agent/pkg/apm/tracetracker\" <nl> \"github.com/signalfx/signalfx-agent/pkg/core/common/constants\" <nl> \"github.com/signalfx/signalfx-agent/pkg/core/services\" <nl> \"github.com/signalfx/signalfx-agent/pkg/monitors/types\" <nl> @@ -29,6 +28,14 @@ type SpanSourceTracker struct { <nl> const dimHistoryCacheSize = 1000 <nl> +// DefaultDimsToAddToSpans are the default dimensions to add as span tags for correlated environments <nl> +var DefaultDimsToAddToSpans = map[string]string{ <nl> + \"container_id\": \"container_id\", <nl> + \"kubernetes_pod_uid\": \"kubernetes_pod_uid\", <nl> + \"kubernetes_pod_name\": \"kubernetes_pod_name\", <nl> + \"kubernetes_namespace\": \"kubernetes_namespace\", <nl> +} <nl> + <nl> func NewSpanSourceTracker(hostTracker *services.EndpointHostTracker, dimChan chan<- *types.Dimension, clusterName string) *SpanSourceTracker { <nl> dimHistory, _ := lru.New(dimHistoryCacheSize) <nl> @@ -50,7 +57,7 @@ func (st *SpanSourceTracker) AddSourceTagsToSpan(span *trace.Span) { <nl> found := 0 <nl> for _, endpoint := range endpoints { <nl> dims := endpoint.Dimensions() <nl> - for _, dim := range apmtracker.DefaultDimsToSyncSource { <nl> + for _, dim := range DefaultDimsToAddToSpans { <nl> if val := dims[dim]; val != \"\" { <nl> found++ <nl> @@ -72,7 +79,7 @@ func (st *SpanSourceTracker) AddSourceTagsToSpan(span *trace.Span) { <nl> } <nl> // Short circuit it if we have added all the desired dimensions with <nl> // this endpoint. <nl> - if found == len(apmtracker.DefaultDimsToSyncSource) { <nl> + if found == len(DefaultDimsToAddToSpans) { <nl> break <nl> } <nl> } <nl> ", "msg": "added separate map for default dimensions to add onto spans"}
{"diff_id": 1291, "repo": "signalfx/signalfx-agent", "sha": "ef145281440047b55a411b2db2545aa533e462d4", "time": "21.01.2021 16:48:47", "diff": "mmm a / pkg/core/hostid/aws.go <nl> ppp b / pkg/core/hostid/aws.go <nl>@@ -44,6 +44,7 @@ func AWSUniqueID(cloudMetadataTimeout timeutil.Duration) string { <nl> if err != nil { <nl> log.WithFields(log.Fields{ <nl> \"error\": err, <nl> + \"body\": string(body), <nl> }).Debug(\"Failed to unmarshal AWS instance-identity response\") <nl> return \"\" <nl> } <nl> ", "msg": "log the body for better debugging"}
{"diff_id": 1294, "repo": "signalfx/signalfx-agent", "sha": "82fa3040d01ac209365ab7c972af004c294b7c69", "time": "01.04.2021 15:47:31", "diff": "mmm a / pkg/monitors/prometheusexporter/prometheus.go <nl> ppp b / pkg/monitors/prometheusexporter/prometheus.go <nl>@@ -142,10 +142,6 @@ func (m *Monitor) Configure(conf *Config) error { <nl> return <nl> } <nl> - now := time.Now() <nl> - for i := range dps { <nl> - dps[i].Timestamp = now <nl> - } <nl> m.Output.SendDatapoints(dps...) <nl> }, time.Duration(conf.IntervalSeconds)*time.Second) <nl> ", "msg": "prometheus-exporter monitor: Don't set timestamp\nLet it be unset so it represents the 'latest available'."}
{"diff_id": 1297, "repo": "signalfx/signalfx-agent", "sha": "9b49778f58b708702622a5c8765b2e879ffc7b30", "time": "22.07.2021 09:10:17", "diff": "mmm a / pkg/observers/kubernetes/api.go <nl> ppp b / pkg/observers/kubernetes/api.go <nl>@@ -198,6 +198,11 @@ func (o *Observer) Configure(config *Config) error { <nl> return nil <nl> } <nl> +func castPod(obj interface{}) *v1.Pod { <nl> + pod, _ := obj.(*v1.Pod) <nl> + return pod <nl> +} <nl> + <nl> func (o *Observer) watchPods() { <nl> o.stopper = make(chan struct{}) <nl> @@ -215,19 +220,24 @@ func (o *Observer) watchPods() { <nl> 0, <nl> cache.ResourceEventHandlerFuncs{ <nl> AddFunc: func(obj interface{}) { <nl> - o.changeHandler(nil, obj.(*v1.Pod)) <nl> + o.changeHandler(nil, castPod(obj)) <nl> }, <nl> UpdateFunc: func(oldObj, newObj interface{}) { <nl> - o.changeHandler(oldObj.(*v1.Pod), newObj.(*v1.Pod)) <nl> + o.changeHandler(castPod(oldObj), castPod(newObj)) <nl> }, <nl> DeleteFunc: func(obj interface{}) { <nl> - o.changeHandler(obj.(*v1.Pod), nil) <nl> + o.changeHandler(castPod(obj), nil) <nl> }, <nl> }) <nl> go controller.Run(o.stopper) <nl> } <nl> +func castNode(obj interface{}) *v1.Node { <nl> + node, _ := obj.(*v1.Node) <nl> + return node <nl> +} <nl> + <nl> func (o *Observer) watchNodes() { <nl> o.stopper = make(chan struct{}) <nl> @@ -242,13 +252,13 @@ func (o *Observer) watchNodes() { <nl> 0, <nl> cache.ResourceEventHandlerFuncs{ <nl> AddFunc: func(obj interface{}) { <nl> - o.changeHandler(nil, obj.(*v1.Node)) <nl> + o.changeHandler(nil, castNode(obj)) <nl> }, <nl> UpdateFunc: func(oldObj, newObj interface{}) { <nl> - o.changeHandler(oldObj.(*v1.Node), newObj.(*v1.Node)) <nl> + o.changeHandler(castNode(oldObj), castNode(newObj)) <nl> }, <nl> DeleteFunc: func(obj interface{}) { <nl> - o.changeHandler(obj.(*v1.Node), nil) <nl> + o.changeHandler(castNode(obj), nil) <nl> }, <nl> }) <nl> ", "msg": "k8s observer: Fix potential panic when API Server is erroring\nFixes"}
{"diff_id": 1307, "repo": "signalfx/signalfx-agent", "sha": "e4d05e133a00d94693b410f254604c69fd0005d9", "time": "17.06.2022 13:09:10", "diff": "mmm a / pkg/monitors/kubernetes/events/events.go <nl> ppp b / pkg/monitors/kubernetes/events/events.go <nl>@@ -177,7 +177,6 @@ func k8sEventToSignalFxEvent(ev *v1.Event) *event.Event { <nl> dims[\"kubernetes_name\"] = ev.InvolvedObject.Name <nl> dims[\"kubernetes_uid\"] = string(ev.InvolvedObject.UID) <nl> } <nl> - <nl> properties := utils.RemoveEmptyMapValues(map[string]string{ <nl> \"message\": ev.Message, <nl> \"source_component\": ev.Source.Component, <nl> @@ -186,8 +185,14 @@ func k8sEventToSignalFxEvent(ev *v1.Event) *event.Event { <nl> \"kubernetes_resource_version\": ev.InvolvedObject.ResourceVersion, <nl> }) <nl> + eventType := ev.Reason <nl> + if eventType == \"\" { <nl> + logger.Debug(\"ev.Reason is not set; setting event type to unknown_reason\") <nl> + eventType = \"unknown_reason\" <nl> + } <nl> + <nl> return event.NewWithProperties( <nl> - ev.Reason, <nl> + eventType, <nl> event.AGENT, <nl> utils.RemoveEmptyMapValues(dims), <nl> utils.StringMapToInterfaceMap(properties), <nl> ", "msg": "k8s events monitor handle empty event type"}
{"diff_id": 1315, "repo": "faiface/pixel", "sha": "8d2306bcbf72dd6c47af4e6d15f7b8dfc89e396a", "time": "06.01.2017 02:50:03", "diff": "mmm a / window.go <nl> ppp b / window.go <nl>@@ -401,7 +401,7 @@ func (wt *windowTriangles) flush() { <nl> wt.attrData[i] = make(map[pixelgl.Attr]interface{}) <nl> } <nl> p := v.Position <nl> - c := NRGBAModel.Convert(v.Color).(NRGBA) <nl> + c := v.Color <nl> t := v.Texture <nl> wt.attrData[i][positionVec2] = mgl32.Vec2{float32(p.X()), float32(p.Y())} <nl> wt.attrData[i][colorVec4] = mgl32.Vec4{float32(c.R), float32(c.G), float32(c.B), float32(c.A)} <nl> @@ -448,9 +448,6 @@ func (wt *windowTriangles) Update(t Triangles) { <nl> } <nl> wt.data = append(wt.data, newData...) <nl> } <nl> - if t.Len() < wt.Len() { <nl> - wt.data = wt.data[:t.Len()] <nl> - } <nl> wt.data.Update(t) <nl> } <nl> @@ -497,6 +494,9 @@ func (w *Window) SetTransform(t ...Transform) { <nl> // SetMaskColor sets a global mask color for the Window. <nl> func (w *Window) SetMaskColor(c color.Color) { <nl> + if c == nil { <nl> + c = NRGBA{1, 1, 1, 1} <nl> + } <nl> nrgba := NRGBAModel.Convert(c).(NRGBA) <nl> r := float32(nrgba.R) <nl> g := float32(nrgba.G) <nl> ", "msg": "fix passing nil color to Window"}
{"diff_id": 1316, "repo": "faiface/pixel", "sha": "fb3df0633787c33aebfa249126af1565778099b5", "time": "06.01.2017 15:00:05", "diff": "mmm a / pixelgl/thread.go <nl> ppp b / pixelgl/thread.go <nl>@@ -9,7 +9,10 @@ import ( <nl> // Due to the limitations of OpenGL and operating systems, all OpenGL related calls must be <nl> // done from the main thread. <nl> -var callQueue = make(chan func(), 32) <nl> +var ( <nl> + callQueue = make(chan func(), 8) <nl> + respChan = make(chan interface{}, 4) <nl> +) <nl> func init() { <nl> runtime.LockOSThread() <nl> @@ -65,12 +68,11 @@ func DoNoBlock(f func()) { <nl> // <nl> // All OpenGL calls must be done in the dedicated thread. <nl> func Do(f func()) { <nl> - done := make(chan struct{}) <nl> callQueue <- func() { <nl> f() <nl> - close(done) <nl> + respChan <- struct{}{} <nl> } <nl> - <-done <nl> + <-respChan <nl> } <nl> // DoErr executes a function inside the main OpenGL thread and returns an error to the called. <nl> @@ -78,11 +80,14 @@ func Do(f func()) { <nl> // <nl> // All OpenGL calls must be done in the dedicated thread. <nl> func DoErr(f func() error) error { <nl> - err := make(chan error) <nl> callQueue <- func() { <nl> - err <- f() <nl> + respChan <- f() <nl> + } <nl> + err := <-respChan <nl> + if err != nil { <nl> + return err.(error) <nl> } <nl> - return <-err <nl> + return nil <nl> } <nl> // DoVal executes a function inside the main OpenGL thread and returns a value to the caller. <nl> @@ -90,9 +95,8 @@ func DoErr(f func() error) error { <nl> // <nl> // All OpenGL calls must be done in the main thread. <nl> func DoVal(f func() interface{}) interface{} { <nl> - val := make(chan interface{}) <nl> callQueue <- func() { <nl> - val <- f() <nl> + respChan <- f() <nl> } <nl> - return <-val <nl> + return <-respChan <nl> } <nl> ", "msg": "add common response channel for main thread"}
{"diff_id": 1341, "repo": "faiface/pixel", "sha": "e656130ce3db63a20865203d1cbaf2a01d104eb5", "time": "22.01.2017 14:16:06", "diff": "mmm a / pixelgl/texture.go <nl> ppp b / pixelgl/texture.go <nl>package pixelgl <nl> import ( <nl> + \"fmt\" <nl> \"runtime\" <nl> \"github.com/faiface/mainthread\" <nl> @@ -100,22 +101,22 @@ func (t *Texture) SetPixels(x, y, w, h int, pixels []uint8) { <nl> // Pixels returns the content of a sub-region of the Texture as an RGBA byte sequence. <nl> func (t *Texture) Pixels(x, y, w, h int) []uint8 { <nl> - pixels := make([]uint8, w*h*4) <nl> - gl.GetTextureSubImage( <nl> + pixels := make([]uint8, t.width*t.height*4) <nl> + gl.GetTexImage( <nl> gl.TEXTURE_2D, <nl> 0, <nl> - int32(x), <nl> - int32(y), <nl> - 0, <nl> - int32(w), <nl> - int32(h), <nl> - 0, <nl> gl.RGBA, <nl> gl.UNSIGNED_BYTE, <nl> - int32(len(pixels)), <nl> gl.Ptr(pixels), <nl> ) <nl> - return pixels <nl> + subPixels := make([]uint8, w*h*4) <nl> + for i := 0; i < h; i++ { <nl> + row := pixels[(i+y)*t.width*4+x*4 : (i+y)*t.width*4+(x+w)*4] <nl> + subRow := subPixels[i*w*4 : (i+1)*w*4] <nl> + fmt.Println((i+y)*t.width*4+x*4, (i+y)*t.width*4+(x+w)*4) <nl> + copy(subRow, row) <nl> + } <nl> + return subPixels <nl> } <nl> // Begin binds a texture. This is necessary before using the texture. <nl> ", "msg": "change pixelgl.Texture.Pixels to work with OpenGL 3.3"}
{"diff_id": 1384, "repo": "faiface/pixel", "sha": "5a9c43bc6c155c19174b6a7297dbb86e2cb40d1e", "time": "08.03.2017 19:19:02", "diff": "mmm a / graphics.go <nl> ppp b / graphics.go <nl>@@ -3,6 +3,7 @@ package pixel <nl> // Sprite is a drawable Picture. It's always anchored by the center of it's Picture. <nl> type Sprite struct { <nl> tri *TrianglesData <nl> + bounds Rect <nl> d Drawer <nl> } <nl> @@ -20,18 +21,17 @@ func NewSprite(pic Picture) *Sprite { <nl> // SetPicture changes the Sprite's Picture. The new Picture may have a different size, everything <nl> // works. <nl> func (s *Sprite) SetPicture(pic Picture) { <nl> - oldPic := s.d.Picture <nl> s.d.Picture = pic <nl> - if oldPic != nil && oldPic.Bounds() == pic.Bounds() { <nl> + if s.bounds == pic.Bounds() { <nl> return <nl> } <nl> + s.bounds = pic.Bounds() <nl> var ( <nl> - bounds = pic.Bounds() <nl> - center = bounds.Center() <nl> - horizontal = V(bounds.W()/2, 0) <nl> - vertical = V(0, bounds.H()/2) <nl> + center = s.bounds.Center() <nl> + horizontal = V(s.bounds.W()/2, 0) <nl> + vertical = V(0, s.bounds.H()/2) <nl> ) <nl> (*s.tri)[0].Position = -horizontal - vertical <nl> ", "msg": "fix Sprite for changing Picture bounds"}
{"diff_id": 1405, "repo": "faiface/pixel", "sha": "cd11c39e6468617c2636dbac35259dba8b7cbb4e", "time": "15.03.2017 22:56:23", "diff": "mmm a / pixelgl/window.go <nl> ppp b / pixelgl/window.go <nl>@@ -23,26 +23,26 @@ type WindowConfig struct { <nl> // Bounds specify the bounds of the Window in pixels. <nl> Bounds pixel.Rect <nl> - // If set to nil, a window will be windowed. Otherwise it will be fullscreen on the <nl> - // specified monitor. <nl> + // If set to nil, a Window will be windowed. Otherwise it will be fullscreen on the <nl> + // specified Monitor. <nl> Fullscreen *Monitor <nl> - // Whether a window is resizable. <nl> + // Whether a Window is resizable. <nl> Resizable bool <nl> - // If set to true, the window will be initially invisible. <nl> + // If set to true, the Window will be initially invisible. <nl> Hidden bool <nl> - // Undecorated window ommits the borders and decorations (close button, etc.). <nl> + // Undecorated Window ommits the borders and decorations (close button, etc.). <nl> Undecorated bool <nl> - // If set to true, a window will not get focused upon showing up. <nl> + // If set to true, a Window will not get focused upon showing up. <nl> Unfocused bool <nl> - // Whether a window is maximized. <nl> + // Whether a Window is maximized. <nl> Maximized bool <nl> - // VSync (vertical synchronization) synchronizes window's framerate with the framerate of <nl> + // VSync (vertical synchronization) synchronizes Window's framerate with the framerate of <nl> // the monitor. <nl> VSync bool <nl> } <nl> ", "msg": "one more doc bug"}
{"diff_id": 1423, "repo": "faiface/pixel", "sha": "fb8424cd32be2086046ea80b265f0f40806befcb", "time": "23.03.2017 19:38:53", "diff": "mmm a / imdraw/imdraw.go <nl> ppp b / imdraw/imdraw.go <nl>@@ -433,9 +433,13 @@ func (imd *IMDraw) outlineEllipseArc(radius pixel.Vec, low, high, thickness floa <nl> func (imd *IMDraw) polyline(thickness float64, closed bool) { <nl> points := imd.getAndClearPoints() <nl> - if len(points) < 2 { <nl> + if len(points) == 0 { <nl> return <nl> } <nl> + if len(points) == 1 { <nl> + // one point special case <nl> + points = append(points, points[0]) <nl> + } <nl> // first point <nl> j, i := 0, 1 <nl> ", "msg": "add support for one point lines in imdraw"}
{"diff_id": 1434, "repo": "faiface/pixel", "sha": "5097bc9cac540e5ba9ac8e54c48da18bff1765f0", "time": "07.04.2017 12:34:16", "diff": "mmm a / pixelgl/window.go <nl> ppp b / pixelgl/window.go <nl>@@ -23,11 +23,11 @@ type WindowConfig struct { <nl> // Bounds specify the bounds of the Window in pixels. <nl> Bounds pixel.Rect <nl> - // If set to nil, a Window will be windowed. Otherwise it will be fullscreen on the <nl> + // If set to nil, the Window will be windowed. Otherwise it will be fullscreen on the <nl> // specified Monitor. <nl> Fullscreen *Monitor <nl> - // Whether a Window is resizable. <nl> + // Whether the Window is resizable. <nl> Resizable bool <nl> // If set to true, the Window will be initially invisible. <nl> @@ -36,10 +36,10 @@ type WindowConfig struct { <nl> // Undecorated Window ommits the borders and decorations (close button, etc.). <nl> Undecorated bool <nl> - // If set to true, a Window will not get focused upon showing up. <nl> + // If set to true, the Window will not get focused upon showing up. <nl> Unfocused bool <nl> - // Whether a Window is maximized. <nl> + // Whether the Window is maximized. <nl> Maximized bool <nl> // VSync (vertical synchronization) synchronizes Window's framerate with the framerate of <nl> ", "msg": "adjust WindowConfig doc, more consistent with the rest"}
{"diff_id": 1436, "repo": "faiface/pixel", "sha": "7d31dd8d6fd675eda8cb4d8d019a3115af314cc7", "time": "09.04.2017 15:32:31", "diff": "mmm a / batch.go <nl> ppp b / batch.go <nl>@@ -5,8 +5,7 @@ import ( <nl> \"image/color\" <nl> ) <nl> -// Batch is a Target that allows for efficient drawing of many objects with the same Picture (but <nl> -// different slices of the same Picture are allowed). <nl> +// Batch is a Target that allows for efficient drawing of many objects with the same Picture. <nl> // <nl> // To put an object into a Batch, just draw it onto it: <nl> // object.Draw(batch) <nl> ", "msg": "fix obsolete Batch doc"}
{"diff_id": 1446, "repo": "faiface/pixel", "sha": "41e5e8ca77b4ac001c61792679447bffc36357fa", "time": "11.04.2017 15:02:58", "diff": "mmm a / pixelgl/gltriangles.go <nl> ppp b / pixelgl/gltriangles.go <nl>@@ -2,6 +2,7 @@ package pixelgl <nl> import ( <nl> \"fmt\" <nl> + \"sync\" <nl> \"github.com/faiface/glhf\" <nl> \"github.com/faiface/mainthread\" <nl> @@ -16,6 +17,7 @@ type GLTriangles struct { <nl> vs *glhf.VertexSlice <nl> data []float32 <nl> shader *glhf.Shader <nl> + updateLock sync.Mutex <nl> } <nl> var ( <nl> @@ -75,6 +77,7 @@ func (gt *GLTriangles) SetLen(len int) { <nl> if len < gt.Len() { <nl> gt.data = gt.data[:len*gt.vs.Stride()] <nl> } <nl> + gt.submitData() <nl> } <nl> // Slice returns a sub-Triangles of this GLTriangles in range [i, j). <nl> @@ -142,15 +145,27 @@ func (gt *GLTriangles) updateData(t pixel.Triangles) { <nl> } <nl> func (gt *GLTriangles) submitData() { <nl> - data := append([]float32{}, gt.data...) // avoid race condition <nl> + // this code is supposed to copy the vertex data and CallNonBlock the update if <nl> + // the data is small enough, otherwise it'll block and not copy the data <nl> + if len(gt.data) < 256 { // arbitrary heurestic constant <nl> + data := append([]float32{}, gt.data...) <nl> mainthread.CallNonBlock(func() { <nl> gt.vs.Begin() <nl> dataLen := len(data) / gt.vs.Stride() <nl> gt.vs.SetLen(dataLen) <nl> + gt.vs.SetVertexData(data) <nl> + gt.vs.End() <nl> + }) <nl> + } else { <nl> + mainthread.Call(func() { <nl> + gt.vs.Begin() <nl> + dataLen := len(gt.data) / gt.vs.Stride() <nl> + gt.vs.SetLen(dataLen) <nl> gt.vs.SetVertexData(gt.data) <nl> gt.vs.End() <nl> }) <nl> } <nl> +} <nl> // Update copies vertex properties from the supplied Triangles into this GLTriangles. <nl> // <nl> ", "msg": "fix race condition in GLTriangles"}
{"diff_id": 1455, "repo": "faiface/pixel", "sha": "4abba37aa36dbef5a9d56c7da7c97acef89f555a", "time": "13.04.2017 15:15:17", "diff": "mmm a / drawer.go <nl> ppp b / drawer.go <nl>@@ -14,7 +14,7 @@ package pixel <nl> // Picture. <nl> // <nl> // Whenever you change the Triangles, call Dirty to notify Drawer that Triangles changed. You don't <nl> -// need to notify Drawer about a change of Picture. <nl> +// need to notify Drawer about a change of the Picture. <nl> type Drawer struct { <nl> Triangles Triangles <nl> Picture Picture <nl> ", "msg": "fix grammar in Drawer doc"}
{"diff_id": 1472, "repo": "faiface/pixel", "sha": "2c1528a92740feb074b901f5bf7e3f34605fa152", "time": "30.04.2017 20:42:25", "diff": "mmm a / pixelgl/window.go <nl> ppp b / pixelgl/window.go <nl>@@ -70,7 +70,7 @@ func NewWindow(cfg WindowConfig) (*Window, error) { <nl> false: glfw.False, <nl> } <nl> - w := &Window{bounds: cfg.Bounds} <nl> + w := &Window{bounds: cfg.Bounds, cursorVisible: true} <nl> err := mainthread.CallErr(func() error { <nl> var err error <nl> ", "msg": "fix Window.CursorVisible intial value (was false)"}
{"diff_id": 1485, "repo": "faiface/pixel", "sha": "91448dcd686aa9dad247bd0995976236172800cb", "time": "03.05.2017 23:57:09", "diff": "mmm a / text/text.go <nl> ppp b / text/text.go <nl>@@ -64,8 +64,8 @@ func New(face font.Face, runeSets ...[]rune) *Text { <nl> txt := &Text{ <nl> atlas: atlas, <nl> color: pixel.Alpha(1), <nl> - lineHeight: 1, <nl> - tabWidth: atlas.mapping[' '].Advance * 4, <nl> + lineHeight: atlas.LineHeight(), <nl> + tabWidth: atlas.Glyph(' ').Advance * 4, <nl> } <nl> txt.glyph.SetLen(6) <nl> txt.d.Picture = txt.atlas.pic <nl> @@ -155,7 +155,7 @@ func (txt *Text) WriteRune(r rune) (n int, err error) { <nl> switch r { <nl> case '\\n': <nl> - txt.Dot -= pixel.Y(txt.atlas.lineHeight * txt.lineHeight) <nl> + txt.Dot -= pixel.Y(txt.lineHeight) <nl> txt.Dot = txt.Dot.WithX(txt.Orig.X()) <nl> return <nl> case '\\r': <nl> ", "msg": "change Text.LineHeight to use actual units instead of scale (such as 1.5)"}
{"diff_id": 1492, "repo": "faiface/pixel", "sha": "ad606d2d0aa07e06e4f8c2ba69ab6a845a4cff27", "time": "07.05.2017 20:59:56", "diff": "mmm a / text/text.go <nl> ppp b / text/text.go <nl>@@ -43,6 +43,7 @@ type Text struct { <nl> lineHeight float64 <nl> tabWidth float64 <nl> + buf []byte <nl> prevR rune <nl> bounds pixel.Rect <nl> glyph pixel.TrianglesData <nl> @@ -131,50 +132,44 @@ func (txt *Text) Clear() { <nl> } <nl> func (txt *Text) Write(p []byte) (n int, err error) { <nl> - n, err = len(p), nil // always returns this <nl> - <nl> - if len(p) == 0 { <nl> - return <nl> - } <nl> - <nl> - for len(p) > 0 { <nl> - r, size := utf8.DecodeRune(p) <nl> - p = p[size:] <nl> - txt.WriteRune(r) <nl> - } <nl> - <nl> - return <nl> + txt.buf = append(txt.buf, p...) <nl> + txt.drawBuf() <nl> + return len(p), nil <nl> } <nl> func (txt *Text) WriteString(s string) (n int, err error) { <nl> - if len(s) == 0 { <nl> - return <nl> - } <nl> - <nl> - for _, r := range s { <nl> - txt.WriteRune(r) <nl> - } <nl> - <nl> + txt.buf = append(txt.buf, s...) <nl> + txt.drawBuf() <nl> return len(s), nil <nl> } <nl> func (txt *Text) WriteByte(c byte) error { <nl> - //FIXME: this is not correct, what if I want to write a 4-byte rune byte by byte? <nl> - _, err := txt.WriteRune(rune(c)) <nl> - return err <nl> + txt.buf = append(txt.buf, c) <nl> + txt.drawBuf() <nl> + return nil <nl> } <nl> func (txt *Text) WriteRune(r rune) (n int, err error) { <nl> - n, err = utf8.RuneLen(r), nil // always returns this <nl> + var b [4]byte <nl> + n = utf8.EncodeRune(b[:], r) <nl> + txt.buf = append(txt.buf, b[:n]...) <nl> + txt.drawBuf() <nl> + return n, nil <nl> +} <nl> + <nl> +func (txt *Text) drawBuf() { <nl> + for utf8.FullRune(txt.buf) { <nl> + r, size := utf8.DecodeRune(txt.buf) <nl> + txt.buf = txt.buf[size:] <nl> switch r { <nl> case '\\n': <nl> txt.Dot -= pixel.Y(txt.lineHeight) <nl> txt.Dot = txt.Dot.WithX(txt.Orig.X()) <nl> - return <nl> + continue <nl> case '\\r': <nl> txt.Dot = txt.Dot.WithX(txt.Orig.X()) <nl> - return <nl> + continue <nl> case '\\t': <nl> rem := math.Mod(txt.Dot.X()-txt.Orig.X(), txt.tabWidth) <nl> rem = math.Mod(rem, rem+txt.tabWidth) <nl> @@ -182,55 +177,40 @@ func (txt *Text) WriteRune(r rune) (n int, err error) { <nl> rem = txt.tabWidth <nl> } <nl> txt.Dot += pixel.X(rem) <nl> - return <nl> + continue <nl> } <nl> - if !txt.atlas.Contains(r) { <nl> - r = unicode.ReplacementChar <nl> - } <nl> - if !txt.atlas.Contains(unicode.ReplacementChar) { <nl> - return <nl> - } <nl> + var rect, frame, bounds pixel.Rect <nl> + rect, frame, bounds, txt.Dot = txt.Atlas().DrawRune(txt.prevR, r, txt.Dot) <nl> - glyph := txt.atlas.Glyph(r) <nl> + txt.prevR = r <nl> - if txt.prevR >= 0 { <nl> - txt.Dot += pixel.X(txt.atlas.Kern(txt.prevR, r)) <nl> + rv := [...]pixel.Vec{pixel.V(rect.Min.X(), rect.Min.Y()), <nl> + pixel.V(rect.Max.X(), rect.Min.Y()), <nl> + pixel.V(rect.Max.X(), rect.Max.Y()), <nl> + pixel.V(rect.Min.X(), rect.Max.Y()), <nl> } <nl> - glyphBounds := glyph.Frame.Moved(txt.Dot - glyph.Orig) <nl> - if glyphBounds.W()*glyphBounds.H() != 0 { <nl> - glyphBounds = pixel.R( <nl> - glyphBounds.Min.X(), <nl> - txt.Dot.Y()-txt.Atlas().Descent(), <nl> - glyphBounds.Max.X(), <nl> - txt.Dot.Y()+txt.Atlas().Ascent(), <nl> - ) <nl> - if txt.bounds.W()*txt.bounds.H() == 0 { <nl> - txt.bounds = glyphBounds <nl> - } else { <nl> - txt.bounds = txt.bounds.Union(glyphBounds) <nl> + fv := [...]pixel.Vec{pixel.V(frame.Min.X(), frame.Min.Y()), <nl> + pixel.V(frame.Max.X(), frame.Min.Y()), <nl> + pixel.V(frame.Max.X(), frame.Max.Y()), <nl> + pixel.V(frame.Min.X(), frame.Max.Y()), <nl> } <nl> - } <nl> - <nl> - a := pixel.V(glyph.Frame.Min.X(), glyph.Frame.Min.Y()) <nl> - b := pixel.V(glyph.Frame.Max.X(), glyph.Frame.Min.Y()) <nl> - c := pixel.V(glyph.Frame.Max.X(), glyph.Frame.Max.Y()) <nl> - d := pixel.V(glyph.Frame.Min.X(), glyph.Frame.Max.Y()) <nl> - for i, v := range []pixel.Vec{a, b, c, a, c, d} { <nl> - txt.glyph[i].Position = v - glyph.Orig + txt.Dot <nl> - txt.glyph[i].Picture = v <nl> + for i, j := range [...]int{0, 1, 2, 0, 2, 3} { <nl> + txt.glyph[i].Position = rv[j] <nl> + txt.glyph[i].Picture = fv[j] <nl> } <nl> txt.tris = append(txt.tris, txt.glyph...) <nl> - <nl> - txt.Dot += pixel.X(glyph.Advance) <nl> - txt.prevR = r <nl> - <nl> txt.dirty = true <nl> - return <nl> + if txt.bounds.W()*txt.bounds.H() == 0 { <nl> + txt.bounds = bounds <nl> + } else { <nl> + txt.bounds = txt.bounds.Union(bounds) <nl> + } <nl> + } <nl> } <nl> func (txt *Text) Draw(t pixel.Target) { <nl> ", "msg": "restructure Text writing for more flexibility and consistency"}
{"diff_id": 1503, "repo": "faiface/pixel", "sha": "fc8eafe3d57de1f1a8d9d3a0733227fecf93fcc1", "time": "09.05.2017 16:34:54", "diff": "mmm a / text/text.go <nl> ppp b / text/text.go <nl>@@ -43,7 +43,7 @@ func RangeTable(table *unicode.RangeTable) []rune { <nl> // txt := text.New(face, text.ASCII) <nl> // <nl> // As suggested by the constructor, a Text object is always associated with one font face and a <nl> -// fixed set of runes. For example, the Text we create above can draw text using the font face <nl> +// fixed set of runes. For example, the Text we created above can draw text using the font face <nl> // contained in the `face` variable and is capable of drawing ASCII characters. <nl> // <nl> // Here we create a Text object which can draw ASCII and Katakana characters: <nl> ", "msg": "fix typo in Text doc"}
{"diff_id": 1515, "repo": "faiface/pixel", "sha": "8221ab58bc23a561abcd0bb40f90a2ee2eccfaeb", "time": "21.05.2017 18:23:51", "diff": "mmm a / text/text.go <nl> ppp b / text/text.go <nl>@@ -148,23 +148,6 @@ func (txt *Text) Atlas() *Atlas { <nl> return txt.atlas <nl> } <nl> -// SetMatrix sets a Matrix by which the text will be transformed before drawing to another Target. <nl> -func (txt *Text) SetMatrix(m pixel.Matrix) { <nl> - if txt.mat != m { <nl> - txt.mat = m <nl> - txt.dirty = true <nl> - } <nl> -} <nl> - <nl> -// SetColorMask sets a color by which the text will be masked before drawingto another Target. <nl> -func (txt *Text) SetColorMask(c color.Color) { <nl> - rgba := pixel.ToRGBA(c) <nl> - if txt.col != rgba { <nl> - txt.col = rgba <nl> - txt.dirty = true <nl> - } <nl> -} <nl> - <nl> // Bounds returns the bounding box of the text currently written to the Text excluding whitespace. <nl> // <nl> // If the Text is empty, a zero rectangle is returned. <nl> @@ -241,8 +224,35 @@ func (txt *Text) WriteRune(r rune) (n int, err error) { <nl> } <nl> // Draw draws all text written to the Text to the provided Target. The text is transformed by the <nl> -// Text's matrix and color mask. <nl> -func (txt *Text) Draw(t pixel.Target) { <nl> +// provided Matrix. <nl> +// <nl> +// This method is equivalent to calling DrawColorMask with nil color mask. <nl> +// <nl> +// If there's a lot of text written to the Text, changing a matrix or a color mask often might hurt <nl> +// performance. Consider using your Target's SetMatrix or SetColorMask methods if available. <nl> +func (txt *Text) Draw(t pixel.Target, matrix pixel.Matrix) { <nl> + txt.DrawColorMask(t, matrix, nil) <nl> +} <nl> + <nl> +// DrawColorMask draws all text written to the Text to the provided Target. The text is transformed <nl> +// by the provided Matrix and masked by the provided color mask. <nl> +// <nl> +// If there's a lot of text written to the Text, changing a matrix or a color mask often might hurt <nl> +// performance. Consider using your Target's SetMatrix or SetColorMask methods if available. <nl> +func (txt *Text) DrawColorMask(t pixel.Target, matrix pixel.Matrix, mask color.Color) { <nl> + if matrix != txt.mat { <nl> + txt.mat = matrix <nl> + txt.dirty = true <nl> + } <nl> + if mask == nil { <nl> + mask = pixel.Alpha(1) <nl> + } <nl> + rgba := pixel.ToRGBA(mask) <nl> + if rgba != txt.col { <nl> + txt.col = rgba <nl> + txt.dirty = true <nl> + } <nl> + <nl> if txt.dirty { <nl> txt.trans.SetLen(txt.tris.Len()) <nl> txt.trans.Update(&txt.tris) <nl> ", "msg": "remove Text.SetMatrix and Text.SetColorMask, add Text.Draw(target, matrix) and Text.DrawColorMask(target, matrix, mask)"}
{"diff_id": 1524, "repo": "faiface/pixel", "sha": "678da34fc34f76f9edc4e0967189a74cf9d4318d", "time": "09.06.2017 20:19:17", "diff": "mmm a / imdraw/imdraw.go <nl> ppp b / imdraw/imdraw.go <nl>@@ -528,29 +528,28 @@ func (imd *IMDraw) polyline(thickness float64, closed bool) { <nl> // first point <nl> j, i := 0, 1 <nl> - normal := points[i].pos.Sub(points[j].pos).Rotated(math.Pi / 2).Unit().Scaled(thickness / 2) <nl> + ijNormal := points[1].pos.Sub(points[0].pos).Rotated(math.Pi / 2).Unit().Scaled(thickness / 2) <nl> if !closed { <nl> switch points[j].endshape { <nl> case NoEndShape: <nl> // nothing <nl> case SharpEndShape: <nl> - imd.pushPt(points[j].pos.Add(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Sub(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Add(normal.Rotated(math.Pi/2)), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Sub(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal.Rotated(math.Pi/2)), points[j]) <nl> imd.fillPolygon() <nl> case RoundEndShape: <nl> imd.pushPt(points[j].pos, points[j]) <nl> - imd.fillEllipseArc(pixel.V(thickness/2, thickness/2), normal.Angle(), normal.Angle()+math.Pi) <nl> + imd.fillEllipseArc(pixel.V(thickness/2, thickness/2), ijNormal.Angle(), ijNormal.Angle()+math.Pi) <nl> } <nl> } <nl> - imd.pushPt(points[j].pos.Add(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Sub(normal), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Sub(ijNormal), points[j]) <nl> // middle points <nl> // compute \"previous\" normal: <nl> - ijNormal := points[1].pos.Sub(points[0].pos).Rotated(math.Pi / 2).Unit().Scaled(thickness / 2) <nl> for i := 0; i < len(points); i++ { <nl> j, k := i+1, i+2 <nl> @@ -602,10 +601,10 @@ func (imd *IMDraw) polyline(thickness float64, closed bool) { <nl> // last point <nl> i, j = len(points)-2, len(points)-1 <nl> - normal = points[j].pos.Sub(points[i].pos).Rotated(math.Pi / 2).Unit().Scaled(thickness / 2) <nl> + ijNormal = points[j].pos.Sub(points[i].pos).Rotated(math.Pi / 2).Unit().Scaled(thickness / 2) <nl> - imd.pushPt(points[j].pos.Sub(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Add(normal), points[j]) <nl> + imd.pushPt(points[j].pos.Sub(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal), points[j]) <nl> imd.fillPolygon() <nl> if !closed { <nl> @@ -613,13 +612,13 @@ func (imd *IMDraw) polyline(thickness float64, closed bool) { <nl> case NoEndShape: <nl> // nothing <nl> case SharpEndShape: <nl> - imd.pushPt(points[j].pos.Add(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Sub(normal), points[j]) <nl> - imd.pushPt(points[j].pos.Add(normal.Rotated(-math.Pi/2)), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Sub(ijNormal), points[j]) <nl> + imd.pushPt(points[j].pos.Add(ijNormal.Rotated(-math.Pi/2)), points[j]) <nl> imd.fillPolygon() <nl> case RoundEndShape: <nl> imd.pushPt(points[j].pos, points[j]) <nl> - imd.fillEllipseArc(pixel.V(thickness/2, thickness/2), normal.Angle(), normal.Angle()-math.Pi) <nl> + imd.fillEllipseArc(pixel.V(thickness/2, thickness/2), ijNormal.Angle(), ijNormal.Angle()-math.Pi) <nl> } <nl> } <nl> ", "msg": "Slightly clean up normal calculations\nWe never actually need the \"normal\" value; it's an extra calculation\nwe didn't need, because ijNormal is the same value early on. It's\ntotally possible that we could further simplify this; there's a lot\nof time going into the normal computations."}
{"diff_id": 1532, "repo": "faiface/pixel", "sha": "dd1ddd15b36f1f631a2be9c43eae948390e2229c", "time": "03.07.2017 00:22:45", "diff": "mmm a / drawer_test.go <nl> ppp b / drawer_test.go <nl>package pixel_test <nl> import ( <nl> + \"image\" <nl> \"testing\" <nl> \"github.com/faiface/pixel\" <nl> ) <nl> func BenchmarkSpriteDrawBatch(b *testing.B) { <nl> - sprite := pixel.NewSprite(nil, pixel.R(0, 0, 64, 64)) <nl> - batch := pixel.NewBatch(&pixel.TrianglesData{}, nil) <nl> + img := image.NewRGBA(image.Rect(0, 0, 64, 64)) <nl> + pic := pixel.PictureDataFromImage(img) <nl> + sprite := pixel.NewSprite(pic, pixel.R(0, 0, 64, 64)) <nl> + batch := pixel.NewBatch(&pixel.TrianglesData{}, pic) <nl> for i := 0; i < b.N; i++ { <nl> sprite.Draw(batch, pixel.IM) <nl> } <nl> ", "msg": "improve sprite.Draw(batch) benchmark"}
{"diff_id": 1546, "repo": "faiface/pixel", "sha": "238400db272eddcc8e00e063a6246edc0b3f84a2", "time": "06.07.2017 23:36:18", "diff": "mmm a / audio/speaker/speaker.go <nl> ppp b / audio/speaker/speaker.go <nl>@@ -84,7 +84,7 @@ func Update() error { <nl> if val > +1 { <nl> val = +1 <nl> } <nl> - valInt16 := int16(val * (1 << 15)) <nl> + valInt16 := int16(val * (1<<15 - 1)) <nl> low := byte(valInt16 % (1 << 8)) <nl> high := byte(valInt16 / (1 << 8)) <nl> buf[i*4+c*2+0] = low <nl> ", "msg": "fix edge sample value (-1 and +1) overflow"}
{"diff_id": 1565, "repo": "faiface/pixel", "sha": "7624d11cfcd83d990a7c4ca98c5847595ee7a795", "time": "14.07.2017 02:10:12", "diff": "mmm a / audio/wav/decode.go <nl> ppp b / audio/wav/decode.go <nl>@@ -146,6 +146,7 @@ func (s *decoder) Stream(samples [][2]float64) (n int, ok bool) { <nl> samples[j][1] = float64(int16(p[i+2])+int16(p[i+3])*(1<<8)) / (1<<15 - 1) <nl> } <nl> } <nl> + s.pos += int32(n) <nl> return n / bytesPerFrame, true <nl> } <nl> ", "msg": "audio: wav: fix Stream (move position, forgot to do it previously)"}
{"diff_id": 1587, "repo": "faiface/pixel", "sha": "d3f6331240c49ca3701505c6d86314f5b8fc3412", "time": "20.01.2019 09:56:30", "diff": "mmm a / geometry.go <nl> ppp b / geometry.go <nl>@@ -84,6 +84,14 @@ func (u Vec) Sub(v Vec) Vec { <nl> } <nl> } <nl> +// Floor returns converts x and y to their integer equivalents. <nl> +func (u Vec) Floor(v Vec) Vec { <nl> + return Vec{ <nl> + math.Floor(u.X), <nl> + math.Floor(u.Y), <nl> + } <nl> +} <nl> + <nl> // To returns the vector from u to v. Equivalent to v.Sub(u). <nl> func (u Vec) To(v Vec) Vec { <nl> return Vec{ <nl> ", "msg": "Adding a pixel.Vec Floor method to geometry.go"}
{"diff_id": 1588, "repo": "faiface/pixel", "sha": "e7a625f5d29eb22dbaebb31dacf672e13f45fd9f", "time": "20.01.2019 10:00:35", "diff": "mmm a / geometry.go <nl> ppp b / geometry.go <nl>@@ -85,7 +85,7 @@ func (u Vec) Sub(v Vec) Vec { <nl> } <nl> // Floor returns converts x and y to their integer equivalents. <nl> -func (u Vec) Floor(v Vec) Vec { <nl> +func (u Vec) Floor() Vec { <nl> return Vec{ <nl> math.Floor(u.X), <nl> math.Floor(u.Y), <nl> ", "msg": "updated Floor method"}
{"diff_id": 1618, "repo": "faiface/pixel", "sha": "d601bc65e423e3297858f955e66fc5ac5f66e4dd", "time": "15.04.2019 08:46:29", "diff": "mmm a / sprite.go <nl> ppp b / sprite.go <nl>@@ -102,10 +102,10 @@ func (s *Sprite) calcData() { <nl> (*s.tri)[5].Position = Vec{}.Sub(horizontal).Add(vertical) <nl> for i := range *s.tri { <nl> - (*s.tri)[i].Position = s.matrix.Project((*s.tri)[i].Position) <nl> (*s.tri)[i].Color = s.mask <nl> (*s.tri)[i].Picture = center.Add((*s.tri)[i].Position) <nl> (*s.tri)[i].Intensity = 1 <nl> + (*s.tri)[i].Position = s.matrix.Project((*s.tri)[i].Position) <nl> } <nl> s.d.Dirty() <nl> ", "msg": "Setting position in correct order"}
{"diff_id": 1620, "repo": "faiface/pixel", "sha": "b18647916faf5feb9efd0a54facd2dbe8a4e9892", "time": "24.05.2019 12:00:11", "diff": "mmm a / geometry.go <nl> ppp b / geometry.go <nl>@@ -479,6 +479,9 @@ type Rect struct { <nl> Min, Max Vec <nl> } <nl> +// ZR is a zero rectangle. <nl> +var ZR = Rect{Min:ZV, Max:ZV} <nl> + <nl> // R returns a new Rect with given the Min and Max coordinates. <nl> // <nl> // Note that the returned rectangle is not automatically normalized. <nl> ", "msg": "Added ZR for zero rect\nAdded the ZR for rectangle for both utility and consistensy with pixel.ZV"}
{"diff_id": 1630, "repo": "faiface/pixel", "sha": "effd43bc5671009de80fb3e9cec76b1ec33201cd", "time": "09.02.2020 20:59:59", "diff": "mmm a / pixelgl/window.go <nl> ppp b / pixelgl/window.go <nl>@@ -173,6 +173,13 @@ func (w *Window) Destroy() { <nl> // Update swaps buffers and polls events. Call this method at the end of each frame. <nl> func (w *Window) Update() { <nl> + w.SwapBuffers() <nl> + w.UpdateInput() <nl> +} <nl> + <nl> +// SwapBuffers swaps buffers. Call this to swap buffers without polling window events. <nl> +// Note that Update invokes SwapBuffers. <nl> +func (w *Window) SwapBuffers() { <nl> mainthread.Call(func() { <nl> _, _, oldW, oldH := intBounds(w.bounds) <nl> newW, newH := w.window.GetSize() <nl> @@ -207,8 +214,6 @@ func (w *Window) Update() { <nl> w.window.SwapBuffers() <nl> w.end() <nl> }) <nl> - <nl> - w.UpdateInput() <nl> } <nl> // SetClosed sets the closed flag of the Window. <nl> ", "msg": "Expose pixelgl.Window.SwapBuffers\nAllow buffers to be swapped without polling input, offering decoupling symmetrical with that provided by UpdateInput."}
{"diff_id": 1670, "repo": "rancher/rke", "sha": "5bcf0845c71e9f4409268bf7bf07a931450b54b9", "time": "05.12.2017 21:44:17", "diff": "mmm a / cluster/cluster.go <nl> ppp b / cluster/cluster.go <nl>@@ -130,17 +130,17 @@ func (c *Cluster) setClusterDefaults() { <nl> } <nl> func (c *Cluster) setClusterServicesDefaults() { <nl> - serviceConfigDefaultsMap := map[string]string{ <nl> - c.Services.KubeAPI.ServiceClusterIPRange: DefaultServiceClusterIPRange, <nl> - c.Services.KubeController.ServiceClusterIPRange: DefaultServiceClusterIPRange, <nl> - c.Services.KubeController.ClusterCIDR: DefaultClusterCIDR, <nl> - c.Services.Kubelet.ClusterDNSServer: DefaultClusterDNSService, <nl> - c.Services.Kubelet.ClusterDomain: DefaultClusterDomain, <nl> - c.Services.Kubelet.InfraContainerImage: DefaultInfraContainerImage, <nl> - c.Authentication.Strategy: DefaultAuthStrategy, <nl> + serviceConfigDefaultsMap := map[*string]string{ <nl> + &c.Services.KubeAPI.ServiceClusterIPRange: DefaultServiceClusterIPRange, <nl> + &c.Services.KubeController.ServiceClusterIPRange: DefaultServiceClusterIPRange, <nl> + &c.Services.KubeController.ClusterCIDR: DefaultClusterCIDR, <nl> + &c.Services.Kubelet.ClusterDNSServer: DefaultClusterDNSService, <nl> + &c.Services.Kubelet.ClusterDomain: DefaultClusterDomain, <nl> + &c.Services.Kubelet.InfraContainerImage: DefaultInfraContainerImage, <nl> + &c.Authentication.Strategy: DefaultAuthStrategy, <nl> } <nl> for k, v := range serviceConfigDefaultsMap { <nl> - setDefaultIfEmpty(&k, v) <nl> + setDefaultIfEmpty(k, v) <nl> } <nl> } <nl> ", "msg": "Fix bug in setting service defaults"}
{"diff_id": 1673, "repo": "rancher/rke", "sha": "b20c0cf4197bd47492f9783f45ce3ceb78e6001b", "time": "13.12.2017 20:14:10", "diff": "mmm a / cluster/network.go <nl> ppp b / cluster/network.go <nl>@@ -11,7 +11,6 @@ import ( <nl> const ( <nl> NetworkPluginResourceName = \"rke-network-plugin\" <nl> - CloudProvider = \"cloud_provider\" <nl> FlannelNetworkPlugin = \"flannel\" <nl> FlannelImage = \"flannel_image\" <nl> @@ -23,6 +22,7 @@ const ( <nl> CalicoCNIImage = \"calico_cni_image\" <nl> CalicoControllersImages = \"calico_controllers_image\" <nl> CalicoctlImage = \"calicoctl_image\" <nl> + CalicoCloudProvider = \"calico_cloud_provider\" <nl> CanalNetworkPlugin = \"canal\" <nl> CanalNodeImage = \"canal_node_image\" <nl> @@ -74,7 +74,7 @@ func (c *Cluster) doCalicoDeploy() error { <nl> network.NodeImage: c.Network.Options[CalicoNodeImage], <nl> network.ControllersImage: c.Network.Options[CalicoControllersImages], <nl> network.CalicoctlImage: c.Network.Options[CalicoctlImage], <nl> - network.CloudProvider: c.Network.Options[CloudProvider], <nl> + network.CloudProvider: c.Network.Options[CalicoCloudProvider], <nl> } <nl> pluginYaml := network.GetCalicoManifest(calicoConfig) <nl> return c.doAddonDeploy(pluginYaml, NetworkPluginResourceName) <nl> @@ -120,7 +120,7 @@ func (c *Cluster) setClusterNetworkDefaults() { <nl> CalicoCNIImage: DefaultCalicoCNIImage, <nl> CalicoNodeImage: DefaultCalicoNodeImage, <nl> CalicoControllersImages: DefaultCalicoControllersImage, <nl> - CloudProvider: DefaultNetworkCloudProvider, <nl> + CalicoCloudProvider: DefaultNetworkCloudProvider, <nl> CalicoctlImage: DefaultCalicoctlImage, <nl> } <nl> ", "msg": "Change cloud_provider to calico_cloud_provider"}
{"diff_id": 1701, "repo": "rancher/rke", "sha": "dcf497add2fbfe7939dabce37b42718207f93664", "time": "26.02.2018 23:43:24", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -397,6 +397,7 @@ func (c *Cluster) BuildEtcdProcess(host *hosts.Host, etcdHosts []*hosts.Host) v3 <nl> Args: args, <nl> Binds: Binds, <nl> NetworkMode: \"host\", <nl> + RestartPolicy: \"always\", <nl> Image: c.Services.Etcd.Image, <nl> HealthCheck: healthCheck, <nl> } <nl> ", "msg": "Fix etcd process regression"}
{"diff_id": 1704, "repo": "rancher/rke", "sha": "7fc06596d1286c8da16c9f3c8f9c9f5f9499e522", "time": "01.03.2018 16:45:26", "diff": "mmm a / templates/kubedns.go <nl> ppp b / templates/kubedns.go <nl>@@ -15,6 +15,7 @@ spec: <nl> labels: <nl> k8s-app: kube-dns-autoscaler <nl> spec: <nl> + serviceAccountName: kube-dns-autoscaler <nl> containers: <nl> - name: autoscaler <nl> image: {{.KubeDNSAutoScalerImage}} <nl> @@ -32,7 +33,46 @@ spec: <nl> - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1}} <nl> - --logtostderr=true <nl> - --v=2 <nl> - <nl> +--- <nl> +apiVersion: v1 <nl> +kind: ServiceAccount <nl> +metadata: <nl> + name: kube-dns-autoscaler <nl> + namespace: kube-system <nl> + labels: <nl> + kubernetes.io/cluster-service: \"true\" <nl> + addonmanager.kubernetes.io/mode: Reconcile <nl> +--- <nl> +kind: ClusterRole <nl> +apiVersion: rbac.authorization.k8s.io/v1 <nl> +metadata: <nl> + name: system:kube-dns-autoscaler <nl> +rules: <nl> + - apiGroups: [\"\"] <nl> + resources: [\"nodes\"] <nl> + verbs: [\"list\"] <nl> + - apiGroups: [\"\"] <nl> + resources: [\"replicationcontrollers/scale\"] <nl> + verbs: [\"get\", \"update\"] <nl> + - apiGroups: [\"extensions\"] <nl> + resources: [\"deployments/scale\", \"replicasets/scale\"] <nl> + verbs: [\"get\", \"update\"] <nl> + - apiGroups: [\"\"] <nl> + resources: [\"configmaps\"] <nl> + verbs: [\"get\", \"create\"] <nl> +--- <nl> +kind: ClusterRoleBinding <nl> +apiVersion: rbac.authorization.k8s.io/v1 <nl> +metadata: <nl> + name: system:kube-dns-autoscaler <nl> +subjects: <nl> + - kind: ServiceAccount <nl> + name: kube-dns-autoscaler <nl> + namespace: kube-system <nl> +roleRef: <nl> + kind: ClusterRole <nl> + name: system:kube-dns-autoscaler <nl> + apiGroup: rbac.authorization.k8s.io <nl> --- <nl> apiVersion: v1 <nl> kind: ServiceAccount <nl> ", "msg": "Add Kube DNS Autoscaler RBAC\nthis adds a service account and appropriate cluster role binding\nfor the kubedns autoscaler to work."}
{"diff_id": 1729, "repo": "rancher/rke", "sha": "4136d8291638a5bde9fd473738b5de8cb2aaa677", "time": "04.04.2018 12:02:11", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -69,7 +69,7 @@ func BuildRKEConfigNodePlan(ctx context.Context, myCluster *Cluster, host *hosts <nl> k8s.ExternalAddressAnnotation: host.Address, <nl> k8s.InternalAddressAnnotation: host.InternalAddress, <nl> }, <nl> - Labels: host.Labels, <nl> + Labels: host.ToAddLabels, <nl> } <nl> } <nl> ", "msg": "Add role labels to node plan"}
{"diff_id": 1730, "repo": "rancher/rke", "sha": "2ca8e6421a5bf36e82ba9b3ab3751b8f4989cdab", "time": "05.04.2018 13:09:22", "diff": "mmm a / cluster/defaults.go <nl> ppp b / cluster/defaults.go <nl>@@ -25,7 +25,7 @@ const ( <nl> DefaultAuthStrategy = \"x509\" <nl> DefaultAuthorizationMode = \"rbac\" <nl> - DefaultNetworkPlugin = \"flannel\" <nl> + DefaultNetworkPlugin = \"canal\" <nl> DefaultNetworkCloudProvider = \"none\" <nl> DefaultIngressController = \"nginx\" <nl> ", "msg": "Change to canal as default network plugin"}
{"diff_id": 1780, "repo": "rancher/rke", "sha": "d155cc8e7626773ed4ba19c7ea532011424ef70c", "time": "06.07.2018 19:48:21", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -124,6 +124,11 @@ func (c *Cluster) BuildKubeAPIProcess(prefixPath string) v3.Process { <nl> \"kubelet-client-certificate\": pki.GetCertPath(pki.KubeAPICertName), <nl> \"kubelet-client-key\": pki.GetKeyPath(pki.KubeAPICertName), <nl> \"service-account-key-file\": pki.GetKeyPath(pki.KubeAPICertName), <nl> + \"etcd-cafile\": etcdCAClientCert, <nl> + \"etcd-certfile\": etcdClientCert, <nl> + \"etcd-keyfile\": etcdClientKey, <nl> + \"etcd-servers\": etcdConnectionString, <nl> + \"etcd-prefix\": etcdPathPrefix, <nl> } <nl> if len(c.CloudProvider.Name) > 0 && c.CloudProvider.Name != aws.AWSCloudProviderName { <nl> CommandArgs[\"cloud-config\"] = CloudConfigPath <nl> @@ -145,14 +150,6 @@ func (c *Cluster) BuildKubeAPIProcess(prefixPath string) v3.Process { <nl> CommandArgs[\"apiserver-count\"] = strconv.Itoa(len(c.ControlPlaneHosts)) <nl> } <nl> - args := []string{ <nl> - \"--etcd-cafile=\" + etcdCAClientCert, <nl> - \"--etcd-certfile=\" + etcdClientCert, <nl> - \"--etcd-keyfile=\" + etcdClientKey, <nl> - \"--etcd-servers=\" + etcdConnectionString, <nl> - \"--etcd-prefix=\" + etcdPathPrefix, <nl> - } <nl> - <nl> if c.Authorization.Mode == services.RBACAuthorizationMode { <nl> CommandArgs[\"authorization-mode\"] = \"Node,RBAC\" <nl> } <nl> @@ -190,7 +187,6 @@ func (c *Cluster) BuildKubeAPIProcess(prefixPath string) v3.Process { <nl> return v3.Process{ <nl> Name: services.KubeAPIContainerName, <nl> Command: Command, <nl> - Args: args, <nl> VolumesFrom: VolumesFrom, <nl> Binds: Binds, <nl> Env: getUniqStringList(c.Services.KubeAPI.ExtraEnv), <nl> ", "msg": "Allow etcd parameters to be overridden"}
{"diff_id": 1781, "repo": "rancher/rke", "sha": "eab141136747c0202bfa19bbc8571ddd26ca221e", "time": "11.07.2018 14:32:58", "diff": "mmm a / k8s/job.go <nl> ppp b / k8s/job.go <nl>@@ -84,7 +84,7 @@ func ensureJobDeleted(k8sClient *kubernetes.Clientset, j interface{}) error { <nl> } <nl> return err <nl> } <nl> - return fmt.Errorf(\"[k8s] Job [%s] deletion timedout. Consider increasing addon_job_timout value\", job.Name) <nl> + return fmt.Errorf(\"[k8s] Job [%s] deletion timed out. Consider increasing addon_job_timeout value\", job.Name) <nl> } <nl> func deleteK8sJob(k8sClient *kubernetes.Clientset, name, namespace string) error { <nl> ", "msg": "Fix typo in timeout setting"}
{"diff_id": 1782, "repo": "rancher/rke", "sha": "1ff65d1fa3824c21a979dcde458f765eeac65ce9", "time": "11.07.2018 17:22:38", "diff": "mmm a / hosts/dialer.go <nl> ppp b / hosts/dialer.go <nl>@@ -4,6 +4,7 @@ import ( <nl> \"fmt\" <nl> \"net\" <nl> \"net/http\" <nl> + \"strings\" <nl> \"time\" <nl> \"github.com/rancher/rke/k8s\" <nl> @@ -106,6 +107,13 @@ func (d *dialer) Dial(network, addr string) (net.Conn, error) { <nl> conn, err = d.getSSHTunnelConnection() <nl> } <nl> if err != nil { <nl> + if strings.Contains(err.Error(), \"no key found\") { <nl> + return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Please check if the configured key or specified key file is a valid SSH Private Key. Error: %v\", d.sshAddress, err) <nl> + } else if strings.Contains(err.Error(), \"no supported methods remain\") { <nl> + return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Please check if you are able to SSH to the node using the specified SSH Private Key and if you have configured the correct SSH username. Error: %v\", d.sshAddress, err) <nl> + } else if strings.Contains(err.Error(), \"cannot decode encrypted private keys\") { <nl> + return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Using encrypted private keys is only supported using ssh-agent. Please configure the option `ssh_agent_auth: true` in the configuration file or use --ssh-agent-auth as a parameter when running RKE. This will use the `SSH_AUTH_SOCK` environment variable. Error: %v\", d.sshAddress, err) <nl> + } <nl> return nil, fmt.Errorf(\"Failed to dial ssh using address [%s]: %v\", d.sshAddress, err) <nl> } <nl> @@ -117,7 +125,7 @@ func (d *dialer) Dial(network, addr string) (net.Conn, error) { <nl> remote, err := conn.Dial(network, addr) <nl> if err != nil { <nl> - return nil, fmt.Errorf(\"Failed to dial to %s: %v\", addr, err) <nl> + return nil, fmt.Errorf(\"Unable to access the Docker socket (%s). Please check if the configured user can execute `docker ps` on the node, and if the SSH server version is at least version 6.7 or higher. If you are using RedHat/CentOS, you can't use the user `root`. Please refer to the documentation for more instructions. Error: %v\", addr, err) <nl> } <nl> return remote, err <nl> } <nl> ", "msg": "Better guidance on SSH errors"}
{"diff_id": 1785, "repo": "rancher/rke", "sha": "f9ecba0ab240ddd88a98dd4e3e77771da9112baa", "time": "20.06.2018 18:54:35", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -314,7 +314,7 @@ func (c *Cluster) BuildKubeletProcess(host *hosts.Host, prefixPath string) v3.Pr <nl> \"kubeconfig\": pki.GetConfigPath(pki.KubeNodeCertName), <nl> \"client-ca-file\": pki.GetCertPath(pki.CACertName), <nl> \"anonymous-auth\": \"false\", <nl> - \"volume-plugin-dir\": \"/var/lib/kubelet/volumeplugins\", <nl> + \"volume-plugin-dir\": path.Join(prefixPath, \"/var/lib/kubelet/volumeplugins\"), <nl> \"fail-swap-on\": strconv.FormatBool(c.Services.Kubelet.FailSwapOn), <nl> \"root-dir\": path.Join(prefixPath, \"/var/lib/kubelet\"), <nl> } <nl> ", "msg": "adjust kubelet volume-plugin-dir to match root-dir"}
{"diff_id": 1796, "repo": "rancher/rke", "sha": "bc75f0bdcf81c0919a7038c1c6fea88a918914ae", "time": "27.07.2018 20:36:33", "diff": "mmm a / hosts/dialer.go <nl> ppp b / hosts/dialer.go <nl>@@ -113,6 +113,8 @@ func (d *dialer) Dial(network, addr string) (net.Conn, error) { <nl> return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Please check if you are able to SSH to the node using the specified SSH Private Key and if you have configured the correct SSH username. Error: %v\", d.sshAddress, err) <nl> } else if strings.Contains(err.Error(), \"cannot decode encrypted private keys\") { <nl> return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Using encrypted private keys is only supported using ssh-agent. Please configure the option `ssh_agent_auth: true` in the configuration file or use --ssh-agent-auth as a parameter when running RKE. This will use the `SSH_AUTH_SOCK` environment variable. Error: %v\", d.sshAddress, err) <nl> + } else if strings.Contains(err.Error(), \"operation timed out\") { <nl> + return nil, fmt.Errorf(\"Unable to access node with address [%s] using SSH. Please check if the node is up and is accepting SSH connections or check network policies and firewall rules. Error: %v\", d.sshAddress, err) <nl> } <nl> return nil, fmt.Errorf(\"Failed to dial ssh using address [%s]: %v\", d.sshAddress, err) <nl> } <nl> ", "msg": "Add better error when node is unreachable"}
{"diff_id": 1800, "repo": "rancher/rke", "sha": "6b7ab838992b1202f26285a534f4b797cbc3fb70", "time": "01.08.2018 12:20:23", "diff": "mmm a / templates/vsphere.go <nl> ppp b / templates/vsphere.go <nl>@@ -41,9 +41,7 @@ vm-name = \"{{ .VsphereConfig.Global.VMName }}\" <nl> {{- if ne $v.Datacenters \"\" }} <nl> datacenters = \"{{ $v.Datacenters }}\" <nl> {{- end }} <nl> - {{- if ne $v.Datacenters \"\" }} <nl> - soap-roundtrip-count = \"{{ $v.Datacenters }}\" <nl> - {{- end }} <nl> + soap-roundtrip-count = \"{{ $v.RoundTripperCount }}\" <nl> {{- end }} <nl> [Workspace] <nl> ", "msg": "fixes wrong value for soap-roundtrip-count in vsphere config"}
{"diff_id": 1810, "repo": "rancher/rke", "sha": "8f47078988da92b57df6e8abe9c27a2ca6b74f43", "time": "28.08.2018 21:44:06", "diff": "mmm a / hosts/dialer.go <nl> ppp b / hosts/dialer.go <nl>@@ -40,7 +40,7 @@ func newDialer(h *Host, kind string) (*dialer, error) { <nl> netConn: \"tcp\", <nl> useSSHAgentAuth: h.SSHAgentAuth, <nl> } <nl> - if bastionDialer.sshKeyString == \"\" { <nl> + if bastionDialer.sshKeyString == \"\" && !bastionDialer.useSSHAgentAuth { <nl> var err error <nl> bastionDialer.sshKeyString, err = privateKeyPath(h.BastionHost.SSHKeyPath) <nl> if err != nil { <nl> @@ -59,7 +59,7 @@ func newDialer(h *Host, kind string) (*dialer, error) { <nl> bastionDialer: bastionDialer, <nl> } <nl> - if dialer.sshKeyString == \"\" { <nl> + if dialer.sshKeyString == \"\" && !dialer.useSSHAgentAuth { <nl> var err error <nl> dialer.sshKeyString, err = privateKeyPath(h.SSHKeyPath) <nl> if err != nil { <nl> ", "msg": "Skip check for private key if using ssh agent"}
{"diff_id": 1815, "repo": "rancher/rke", "sha": "f314d1dc96cd31c1020c49c7fb4a9568d30bcd44", "time": "04.09.2018 21:26:59", "diff": "mmm a / cluster/certificates.go <nl> ppp b / cluster/certificates.go <nl>@@ -140,14 +140,23 @@ func getClusterCerts(ctx context.Context, kubeClient *kubernetes.Clientset, etcd <nl> return nil, err <nl> } <nl> // If I can't find an etcd cert, I will not fail and will create it later. <nl> - if secret == nil && strings.HasPrefix(certName, \"kube-etcd\") { <nl> + if (secret == nil || secret.Data == nil) && strings.HasPrefix(certName, \"kube-etcd\") { <nl> certMap[certName] = pki.CertificatePKI{} <nl> continue <nl> } <nl> - secretCert, _ := cert.ParseCertsPEM(secret.Data[\"Certificate\"]) <nl> - secretKey, _ := cert.ParsePrivateKeyPEM(secret.Data[\"Key\"]) <nl> + secretCert, err := cert.ParseCertsPEM(secret.Data[\"Certificate\"]) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to parse certificate of %s: %v\", certName, err) <nl> + } <nl> + secretKey, err := cert.ParsePrivateKeyPEM(secret.Data[\"Key\"]) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to parse private key of %s: %v\", certName, err) <nl> + } <nl> secretConfig := string(secret.Data[\"Config\"]) <nl> + if len(secretCert) == 0 || secretKey == nil { <nl> + return nil, fmt.Errorf(\"certificate or key of %s is not found\", certName) <nl> + } <nl> certMap[certName] = pki.CertificatePKI{ <nl> Certificate: secretCert[0], <nl> Key: secretKey.(*rsa.PrivateKey), <nl> ", "msg": "Avoid panic if cert or key of a secret is not found"}
{"diff_id": 1822, "repo": "rancher/rke", "sha": "266f6b8123145278c600ebbb379be3ad65e2556a", "time": "15.10.2018 17:42:09", "diff": "mmm a / templates/nginx-ingress.go <nl> ppp b / templates/nginx-ingress.go <nl>@@ -203,6 +203,9 @@ spec: <nl> - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services <nl> - --udp-services-configmap=$(POD_NAMESPACE)/udp-services <nl> - --annotations-prefix=nginx.ingress.kubernetes.io <nl> + {{ range $k, $v := .ExtraArgs }} <nl> + - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}} <nl> + {{ end }} <nl> securityContext: <nl> capabilities: <nl> drop: <nl> @@ -210,9 +213,6 @@ spec: <nl> add: <nl> - NET_BIND_SERVICE <nl> runAsUser: 33 <nl> - {{ range $k, $v := .ExtraArgs }} <nl> - - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}} <nl> - {{ end }} <nl> env: <nl> - name: POD_NAME <nl> valueFrom: <nl> ", "msg": "Moving extra_args iteration to allow extra_args to work successfully"}
{"diff_id": 1824, "repo": "rancher/rke", "sha": "c2071495acd77cfa236f7c591e064624a890ad70", "time": "16.10.2018 21:51:57", "diff": "mmm a / cluster/hosts.go <nl> ppp b / cluster/hosts.go <nl>@@ -31,15 +31,23 @@ func (c *Cluster) TunnelHosts(ctx context.Context, local bool) error { <nl> } <nl> c.InactiveHosts = make([]*hosts.Host, 0) <nl> uniqueHosts := hosts.GetUniqueHostList(c.EtcdHosts, c.ControlPlaneHosts, c.WorkerHosts) <nl> - for i := range uniqueHosts { <nl> - if err := uniqueHosts[i].TunnelUp(ctx, c.DockerDialerFactory, c.PrefixPath, c.Version); err != nil { <nl> + var errgrp errgroup.Group <nl> + for _, uniqueHost := range uniqueHosts { <nl> + runHost := uniqueHost <nl> + errgrp.Go(func() error { <nl> + if err := runHost.TunnelUp(ctx, c.DockerDialerFactory, c.PrefixPath, c.Version); err != nil { <nl> // Unsupported Docker version is NOT a connectivity problem that we can recover! So we bail out on it <nl> if strings.Contains(err.Error(), \"Unsupported Docker version found\") { <nl> return err <nl> } <nl> - log.Warnf(ctx, \"Failed to set up SSH tunneling for host [%s]: %v\", uniqueHosts[i].Address, err) <nl> - c.InactiveHosts = append(c.InactiveHosts, uniqueHosts[i]) <nl> + log.Warnf(ctx, \"Failed to set up SSH tunneling for host [%s]: %v\", runHost.Address, err) <nl> + c.InactiveHosts = append(c.InactiveHosts, runHost) <nl> } <nl> + return nil <nl> + }) <nl> + } <nl> + if err := errgrp.Wait(); err != nil { <nl> + return err <nl> } <nl> for _, host := range c.InactiveHosts { <nl> log.Warnf(ctx, \"Removing host [%s] from node lists\", host.Address) <nl> ", "msg": "use errgroup for tunneling hosts"}
{"diff_id": 1836, "repo": "rancher/rke", "sha": "a905a6df808cd8b4f3d4ff9bf332dc0c37f6782f", "time": "15.01.2019 23:15:20", "diff": "mmm a / pki/services.go <nl> ppp b / pki/services.go <nl>@@ -30,7 +30,11 @@ func GenerateKubeAPICertificate(ctx context.Context, certs map[string]Certificat <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Kubernetes API server certificates\") <nl> - kubeAPICrt, kubeAPIKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, KubeAPICertName, kubeAPIAltNames, certs[KubeAPICertName].Key, nil) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeAPICertName].Key <nl> + } <nl> + kubeAPICrt, kubeAPIKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, KubeAPICertName, kubeAPIAltNames, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -73,11 +77,15 @@ func GenerateKubeControllerCertificate(ctx context.Context, certs map[string]Cer <nl> // generate Kube controller-manager certificate and key <nl> caCrt := certs[CACertName].Certificate <nl> caKey := certs[CACertName].Key <nl> - if certs[KubeControllerCertName].Certificate != nil { <nl> + if certs[KubeControllerCertName].Certificate != nil && !rotate { <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Kube Controller certificates\") <nl> - kubeControllerCrt, kubeControllerKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeControllerCertName), nil, certs[KubeControllerCertName].Key, nil) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeControllerCertName].Key <nl> + } <nl> + kubeControllerCrt, kubeControllerKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeControllerCertName), nil, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -105,11 +113,15 @@ func GenerateKubeSchedulerCertificate(ctx context.Context, certs map[string]Cert <nl> // generate Kube scheduler certificate and key <nl> caCrt := certs[CACertName].Certificate <nl> caKey := certs[CACertName].Key <nl> - if certs[KubeSchedulerCertName].Certificate != nil { <nl> + if certs[KubeSchedulerCertName].Certificate != nil && !rotate { <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Kube Scheduler certificates\") <nl> - kubeSchedulerCrt, kubeSchedulerKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeSchedulerCertName), nil, certs[KubeSchedulerCertName].Key, nil) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeSchedulerCertName].Key <nl> + } <nl> + kubeSchedulerCrt, kubeSchedulerKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeSchedulerCertName), nil, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -137,11 +149,15 @@ func GenerateKubeProxyCertificate(ctx context.Context, certs map[string]Certific <nl> // generate Kube Proxy certificate and key <nl> caCrt := certs[CACertName].Certificate <nl> caKey := certs[CACertName].Key <nl> - if certs[KubeProxyCertName].Certificate != nil { <nl> + if certs[KubeProxyCertName].Certificate != nil && !rotate { <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Kube Proxy certificates\") <nl> - kubeProxyCrt, kubeProxyKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeProxyCertName), nil, certs[KubeProxyCertName].Key, nil) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeProxyCertName].Key <nl> + } <nl> + kubeProxyCrt, kubeProxyKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, getDefaultCN(KubeProxyCertName), nil, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -169,11 +185,15 @@ func GenerateKubeNodeCertificate(ctx context.Context, certs map[string]Certifica <nl> // generate kubelet certificate <nl> caCrt := certs[CACertName].Certificate <nl> caKey := certs[CACertName].Key <nl> - if certs[KubeNodeCertName].Certificate != nil { <nl> + if certs[KubeNodeCertName].Certificate != nil && !rotate { <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Node certificate\") <nl> - nodeCrt, nodeKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, KubeNodeCommonName, nil, certs[KubeNodeCertName].Key, []string{KubeNodeOrganizationName}) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeProxyCertName].Key <nl> + } <nl> + nodeCrt, nodeKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, KubeNodeCommonName, nil, serviceKey, []string{KubeNodeOrganizationName}) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -207,7 +227,11 @@ func GenerateKubeAdminCertificate(ctx context.Context, certs map[string]Certific <nl> configPath = ClusterConfig <nl> } <nl> localKubeConfigPath := GetLocalKubeConfig(configPath, configDir) <nl> - kubeAdminCrt, kubeAdminKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, KubeAdminCertName, nil, certs[KubeAdminCertName].Key, []string{KubeAdminOrganizationName}) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[KubeAdminCertName].Key <nl> + } <nl> + kubeAdminCrt, kubeAdminKey, err := GenerateSignedCertAndKey(caCrt, caKey, false, KubeAdminCertName, nil, serviceKey, []string{KubeAdminOrganizationName}) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -250,11 +274,15 @@ func GenerateAPIProxyClientCertificate(ctx context.Context, certs map[string]Cer <nl> //generate API server proxy client key and certs <nl> caCrt := certs[RequestHeaderCACertName].Certificate <nl> caKey := certs[RequestHeaderCACertName].Key <nl> - if certs[APIProxyClientCertName].Certificate != nil { <nl> + if certs[APIProxyClientCertName].Certificate != nil && !rotate { <nl> return nil <nl> } <nl> log.Infof(ctx, \"[certificates] Generating Kubernetes API server proxy client certificates\") <nl> - apiserverProxyClientCrt, apiserverProxyClientKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, APIProxyClientCertName, nil, certs[APIProxyClientCertName].Key, nil) <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[APIProxyClientCertName].Key <nl> + } <nl> + apiserverProxyClientCrt, apiserverProxyClientKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, APIProxyClientCertName, nil, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -312,8 +340,12 @@ func GenerateEtcdCertificates(ctx context.Context, certs map[string]CertificateP <nl> if _, ok := certs[etcdName]; ok && !rotate { <nl> continue <nl> } <nl> + var serviceKey *rsa.PrivateKey <nl> + if !rotate { <nl> + serviceKey = certs[etcdName].Key <nl> + } <nl> log.Infof(ctx, \"[certificates] Generating etcd-%s certificate and key\", host.InternalAddress) <nl> - etcdCrt, etcdKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, EtcdCertName, etcdAltNames, certs[etcdName].Key, nil) <nl> + etcdCrt, etcdKey, err := GenerateSignedCertAndKey(caCrt, caKey, true, EtcdCertName, etcdAltNames, serviceKey, nil) <nl> if err != nil { <nl> return err <nl> } <nl> @@ -370,7 +402,8 @@ func GenerateServiceTokenKey(ctx context.Context, certs map[string]CertificatePK <nl> func GenerateRKECACerts(ctx context.Context, certs map[string]CertificatePKI, configPath, configDir string) error { <nl> // generate kubernetes CA certificate and key <nl> log.Infof(ctx, \"[certificates] Generating CA kubernetes certificates\") <nl> - caCrt, caKey, err := GenerateCACertAndKey(CACertName, certs[CACertName].Key) <nl> + <nl> + caCrt, caKey, err := GenerateCACertAndKey(CACertName, nil) <nl> if err != nil { <nl> return err <nl> } <nl> ", "msg": "Fix rotation for service components"}
{"diff_id": 1842, "repo": "rancher/rke", "sha": "ea4b16b1161e01f7810b928ddb6ec30b3d5d3ff0", "time": "28.01.2019 19:47:30", "diff": "mmm a / cluster/network.go <nl> ppp b / cluster/network.go <nl>@@ -534,9 +534,10 @@ func checkPlanePortsFromHost(ctx context.Context, host *hosts.Host, portList []s <nl> if tcp { <nl> cmd = append(cmd, \"for host in $HOSTS; do for port in $PORTS ; do echo \\\"Checking host ${host} on port ${port}\\\" >&1 & nc -w5 -z $host $port > /dev/null || echo \\\"${host}:${port}\\\" >&2 & done; wait; done\") <nl> } else { <nl> - // UDP port scans using the -uz combination of flags will always report success irrespective of the target machine's state, <nl> - // so instead we use -uzv to log verbose output if the checking port is not open or unreachable, and it will print nothing if it succeeds <nl> - cmd = append(cmd, \"for host in $HOSTS; do for port in $PORTS ; do echo \\\"Checking host ${host} on port ${port}\\\" >&1 & nc -w5 -uzv $host $port > /dev/null & done; wait; done\") <nl> + // TODO: add proper UDP port checks, and because UDP is not reliable so it has no acknowledgment, retransmission, or timeout. <nl> + // Also the k8s layer 3 network like flannel will filtering the host port like 8472 once is installed, so commands like `nc -w5 -uzv $host $port` will always return the same message regardless of the udp port is opened or not. <nl> + // More details on: https://github.com/rancher/rke/issues/1102 <nl> + return nil <nl> } <nl> for _, host := range planeHosts { <nl> ", "msg": "revert to skip network plugin port checks of udp port"}
{"diff_id": 1855, "repo": "rancher/rke", "sha": "64fed4bd9bef338a650db917d4b644a77cf8fc06", "time": "09.03.2019 03:51:41", "diff": "mmm a / cmd/etcd.go <nl> ppp b / cmd/etcd.go <nl>@@ -108,63 +108,64 @@ func RestoreEtcdSnapshot( <nl> ctx context.Context, <nl> rkeConfig *v3.RancherKubernetesEngineConfig, <nl> dialersOptions hosts.DialersOptions, <nl> - flags cluster.ExternalFlags, snapshotName string) error { <nl> - <nl> + flags cluster.ExternalFlags, snapshotName string) (string, string, string, string, map[string]pki.CertificatePKI, error) { <nl> + var APIURL, caCrt, clientCert, clientKey string <nl> log.Infof(ctx, \"Restoring etcd snapshot %s\", snapshotName) <nl> kubeCluster, err := cluster.InitClusterObject(ctx, rkeConfig, flags) <nl> if err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> stateFilePath := cluster.GetStateFilePath(flags.ClusterFilePath, flags.ConfigDir) <nl> rkeFullState, _ := cluster.ReadStateFile(ctx, stateFilePath) <nl> if err := doUpgradeLegacyCluster(ctx, kubeCluster, rkeFullState); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> rkeFullState.CurrentState = cluster.State{} <nl> if err := rkeFullState.WriteStateFile(ctx, stateFilePath); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> if err := kubeCluster.SetupDialers(ctx, dialersOptions); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> if err := kubeCluster.TunnelHosts(ctx, flags); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> // if we fail after cleanup, we can't find the certs to do the download, we need to redeploy them <nl> if err := kubeCluster.DeployRestoreCerts(ctx, rkeFullState.DesiredState.CertificatesBundle); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> // first download and check <nl> if err := kubeCluster.PrepareBackup(ctx, snapshotName); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> log.Infof(ctx, \"Cleaning old kubernetes cluster\") <nl> if err := kubeCluster.CleanupNodes(ctx); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> if err := kubeCluster.RestoreEtcdSnapshot(ctx, snapshotName); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> if err := ClusterInit(ctx, rkeConfig, dialersOptions, flags); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> - if _, _, _, _, _, err := ClusterUp(ctx, dialersOptions, flags); err != nil { <nl> + APIURL, caCrt, clientCert, clientKey, certs, err := ClusterUp(ctx, dialersOptions, flags) <nl> + if err != nil { <nl> if !strings.Contains(err.Error(), \"Provisioning incomplete\") { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> log.Warnf(ctx, err.Error()) <nl> } <nl> if err := cluster.RestartClusterPods(ctx, kubeCluster); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> if err := kubeCluster.RemoveOldNodes(ctx); err != nil { <nl> - return err <nl> + return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> log.Infof(ctx, \"Finished restoring snapshot [%s] on all etcd hosts\", snapshotName) <nl> - return nil <nl> + return APIURL, caCrt, clientCert, clientKey, certs, err <nl> } <nl> func SnapshotSaveEtcdHostsFromCli(ctx *cli.Context) error { <nl> @@ -216,6 +217,6 @@ func RestoreEtcdSnapshotFromCli(ctx *cli.Context) error { <nl> // setting up the flags <nl> flags := cluster.GetExternalFlags(false, false, false, \"\", filePath) <nl> - return RestoreEtcdSnapshot(context.Background(), rkeConfig, hosts.DialersOptions{}, flags, etcdSnapshotName) <nl> - <nl> + _, _, _, _, _, err = RestoreEtcdSnapshot(context.Background(), rkeConfig, hosts.DialersOptions{}, flags, etcdSnapshotName) <nl> + return err <nl> } <nl> ", "msg": "Change RestoreEtcdSnapshot to return updated cluster information"}
{"diff_id": 1865, "repo": "rancher/rke", "sha": "32e10710411a7962a084859d43e030d4e0dc7a44", "time": "19.03.2019 11:06:01", "diff": "mmm a / cluster/certificates.go <nl> ppp b / cluster/certificates.go <nl>@@ -212,6 +212,12 @@ func GetClusterCertsFromNodes(ctx context.Context, kubeCluster *Cluster) (map[st <nl> for _, host := range backupHosts { <nl> certificates, err = pki.FetchCertificatesFromHost(ctx, kubeCluster.EtcdHosts, host, kubeCluster.SystemImages.Alpine, kubeCluster.LocalKubeConfigPath, kubeCluster.PrivateRegistriesMap) <nl> if certificates != nil { <nl> + // Handle service account token key issue <nl> + kubeAPICert := certificates[pki.KubeAPICertName] <nl> + if certificates[pki.ServiceAccountTokenKeyName].Key == nil { <nl> + log.Infof(ctx, \"[certificates] Creating service account token key\") <nl> + certificates[pki.ServiceAccountTokenKeyName] = pki.ToCertObject(pki.ServiceAccountTokenKeyName, pki.ServiceAccountTokenKeyName, \"\", kubeAPICert.Certificate, kubeAPICert.Key, nil) <nl> + } <nl> return certificates, nil <nl> } <nl> } <nl> ", "msg": "Handle missing service account token key when fetching certs from nodes"}
{"diff_id": 1874, "repo": "rancher/rke", "sha": "9679aca20c3fc52e5bd978b6a16955642066e0b3", "time": "10.04.2019 19:59:58", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -544,6 +544,7 @@ func (c *Cluster) BuildKubeProxyProcess(host *hosts.Host, prefixPath string) v3. <nl> } <nl> Binds := []string{ <nl> fmt.Sprintf(\"%s:/etc/kubernetes:z\", path.Join(prefixPath, \"/etc/kubernetes\")), <nl> + \"/run:/run\", <nl> } <nl> for arg, value := range c.Services.Kubeproxy.ExtraArgs { <nl> ", "msg": "Fix: kube-proxy not mounting /run/xtables.lock leading to racy iptables access\nkube-proxy and other processes invoking iptables (e.g. flannel, weave) must share the host fs `/run/xtables.lock` to prevent concurrent access to iptables resulting in errors like \"iptables: Resource temporarily unavailable\"."}
{"diff_id": 1882, "repo": "rancher/rke", "sha": "cc3c03746fe5a5a5446be2514477f83aa110aa9e", "time": "29.05.2019 13:57:50", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -702,15 +702,11 @@ func (c *Cluster) BuildSidecarProcess() v3.Process { <nl> func (c *Cluster) BuildEtcdProcess(host *hosts.Host, etcdHosts []*hosts.Host, prefixPath string) v3.Process { <nl> nodeName := pki.GetEtcdCrtName(host.InternalAddress) <nl> initCluster := \"\" <nl> - architecture := \"amd64\" <nl> + architecture := host.DockerInfo.Architecture <nl> if len(etcdHosts) == 0 { <nl> initCluster = services.GetEtcdInitialCluster(c.EtcdHosts) <nl> - if len(c.EtcdHosts) > 0 { <nl> - architecture = c.EtcdHosts[0].DockerInfo.Architecture <nl> - } <nl> } else { <nl> initCluster = services.GetEtcdInitialCluster(etcdHosts) <nl> - architecture = etcdHosts[0].DockerInfo.Architecture <nl> } <nl> clusterState := \"new\" <nl> ", "msg": "Use the node's architecture to build etcd process\nThis allows for mixed-architecture etcd clusters."}
{"diff_id": 1883, "repo": "rancher/rke", "sha": "870c073c100d0462b7861ce0ebfded506826c96f", "time": "31.05.2019 18:39:56", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -18,7 +18,7 @@ import ( <nl> \"github.com/rancher/rke/pki\" <nl> \"github.com/rancher/rke/services\" <nl> \"github.com/rancher/rke/util\" <nl> - \"github.com/rancher/types/apis/management.cattle.io/v3\" <nl> + v3 \"github.com/rancher/types/apis/management.cattle.io/v3\" <nl> \"github.com/sirupsen/logrus\" <nl> ) <nl> @@ -100,7 +100,7 @@ func BuildRKEConfigNodePlan(ctx context.Context, myCluster *Cluster, host *hosts <nl> func (c *Cluster) BuildKubeAPIProcess(host *hosts.Host, prefixPath string) v3.Process { <nl> // check if external etcd is used <nl> - etcdConnectionString := services.GetEtcdConnString(c.EtcdHosts, host.Address) <nl> + etcdConnectionString := services.GetEtcdConnString(c.EtcdHosts, host.InternalAddress) <nl> etcdPathPrefix := EtcdPathPrefix <nl> etcdClientCert := pki.GetCertPath(pki.KubeNodeCertName) <nl> etcdClientKey := pki.GetKeyPath(pki.KubeNodeCertName) <nl> ", "msg": "Use Internal Addresses to sort the etcd connection string"}
{"diff_id": 1890, "repo": "rancher/rke", "sha": "ad415059cb5ee09e9623c62062e6a3cdb860ed64", "time": "12.06.2019 17:19:22", "diff": "mmm a / cmd/common.go <nl> ppp b / cmd/common.go <nl>@@ -89,7 +89,7 @@ func ClusterInit(ctx context.Context, rkeConfig *v3.RancherKubernetesEngineConfi <nl> if strings.Contains(err.Error(), \"aborting upgrade\") { <nl> return err <nl> } <nl> - log.Warnf(ctx, \"[state] can't fetch legacy cluster state from Kubernetes\") <nl> + log.Warnf(ctx, \"[state] can't fetch legacy cluster state from Kubernetes: %v\", err) <nl> } <nl> // check if certificate rotate or normal init <nl> if kubeCluster.RancherKubernetesEngineConfig.RotateCertificates != nil { <nl> ", "msg": "Log error message on failure to get legacy cluster state from k8s"}
{"diff_id": 1892, "repo": "rancher/rke", "sha": "9697b3c452f1bd675c4512ff14264d2cd9e4e271", "time": "01.07.2019 16:09:26", "diff": "mmm a / pki/util.go <nl> ppp b / pki/util.go <nl>@@ -5,6 +5,7 @@ import ( <nl> \"crypto/rsa\" <nl> \"crypto/x509\" <nl> \"crypto/x509/pkix\" <nl> + \"encoding/asn1\" <nl> \"encoding/pem\" <nl> \"errors\" <nl> \"fmt\" <nl> @@ -25,6 +26,12 @@ import ( <nl> \"k8s.io/client-go/util/cert\" <nl> ) <nl> +var ( <nl> + oidExtensionExtendedKeyUsage = asn1.ObjectIdentifier{2, 5, 29, 37} <nl> + oidExtKeyUsageServerAuth = asn1.ObjectIdentifier{1, 3, 6, 1, 5, 5, 7, 3, 1} <nl> + oidExtKeyUsageClientAuth = asn1.ObjectIdentifier{1, 3, 6, 1, 5, 5, 7, 3, 2} <nl> +) <nl> + <nl> func GenerateSignedCertAndKey( <nl> caCrt *x509.Certificate, <nl> caKey *rsa.PrivateKey, <nl> @@ -79,20 +86,29 @@ func GenerateCertSigningRequestAndKey( <nl> return nil, nil, fmt.Errorf(\"Failed to generate private key for %s certificate: %v\", commonName, err) <nl> } <nl> } <nl> - usages := []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth} <nl> + usages := []asn1.ObjectIdentifier{oidExtKeyUsageClientAuth} <nl> if serverCrt { <nl> - usages = append(usages, x509.ExtKeyUsageServerAuth) <nl> + usages = append(usages, oidExtKeyUsageServerAuth) <nl> + } <nl> + marshalledUsages, err := asn1.Marshal(usages) <nl> + if err != nil { <nl> + return nil, nil, fmt.Errorf(\"error marshalling key usages while generating csr: %v\", err) <nl> } <nl> + <nl> + extensions := []pkix.Extension{{ <nl> + Id: oidExtensionExtendedKeyUsage, <nl> + Critical: false, <nl> + Value: marshalledUsages, <nl> + }} <nl> if altNames == nil { <nl> altNames = &cert.AltNames{} <nl> } <nl> caConfig := cert.Config{ <nl> CommonName: commonName, <nl> Organization: orgs, <nl> - Usages: usages, <nl> AltNames: *altNames, <nl> } <nl> - clientCSR, err := newCertSigningRequest(caConfig, rootKey) <nl> + clientCSR, err := newCertSigningRequest(caConfig, rootKey, extensions) <nl> if err != nil { <nl> return nil, nil, fmt.Errorf(\"Failed to generate %s certificate: %v\", commonName, err) <nl> @@ -435,12 +451,12 @@ func newSignedCert(cfg cert.Config, key *rsa.PrivateKey, caCert *x509.Certificat <nl> return x509.ParseCertificate(certDERBytes) <nl> } <nl> -func newCertSigningRequest(cfg cert.Config, key *rsa.PrivateKey) ([]byte, error) { <nl> +func newCertSigningRequest(cfg cert.Config, key *rsa.PrivateKey, extensions []pkix.Extension) ([]byte, error) { <nl> if len(cfg.CommonName) == 0 { <nl> return nil, errors.New(\"must specify a CommonName\") <nl> } <nl> - if len(cfg.Usages) == 0 { <nl> - return nil, errors.New(\"must specify at least one ExtKeyUsage\") <nl> + if len(extensions) == 0 { <nl> + return nil, errors.New(\"must specify at least one Extension\") <nl> } <nl> certTmpl := x509.CertificateRequest{ <nl> @@ -450,6 +466,7 @@ func newCertSigningRequest(cfg cert.Config, key *rsa.PrivateKey) ([]byte, error) <nl> }, <nl> DNSNames: cfg.AltNames.DNSNames, <nl> IPAddresses: cfg.AltNames.IPs, <nl> + ExtraExtensions: extensions, <nl> } <nl> return x509.CreateCertificateRequest(cryptorand.Reader, &certTmpl, key) <nl> } <nl> ", "msg": "Adding extensions for extended key usage"}
{"diff_id": 1910, "repo": "rancher/rke", "sha": "ff663b5a5996e2dba52453c20da1ec2ee5f8b9ec", "time": "02.09.2019 18:58:58", "diff": "mmm a / hosts/tunnel.go <nl> ppp b / hosts/tunnel.go <nl>@@ -36,7 +36,6 @@ func (h *Host) TunnelUp(ctx context.Context, dialerFactory DialerFactory, cluste <nl> // set Docker client <nl> logrus.Debugf(\"Connecting to Docker API for host [%s]\", h.Address) <nl> h.DClient, err = client.NewClientWithOpts( <nl> - client.WithHost(\"unix:///var/run/docker.sock\"), <nl> client.WithVersion(DockerAPIVersion), <nl> client.WithHTTPClient(httpClient)) <nl> if err != nil { <nl> ", "msg": "Dont set withHost on Docker client\nProblem: Because of the unix socket reference, Windows breaks on rke up\nRoot cause: Vendor of docker/docker\nResolution: I was checking what was changed in vendor but it seems this setting is not used, as we configure Docker socket per host."}
{"diff_id": 1911, "repo": "rancher/rke", "sha": "2291a1b734aeb381f65ad42216d7d23f4059c740", "time": "25.07.2019 13:13:24", "diff": "mmm a / hosts/tunnel.go <nl> ppp b / hosts/tunnel.go <nl>@@ -68,6 +68,9 @@ func checkDockerVersion(ctx context.Context, h *Host, clusterVersion string) err <nl> return fmt.Errorf(\"Can't retrieve Docker Info: %v\", err) <nl> } <nl> logrus.Debugf(\"Docker Info found: %#v\", info) <nl> + if h.IgnoreDockerVersion { <nl> + return nil <nl> + } <nl> h.DockerInfo = info <nl> K8sSemVer, err := util.StrToSemVer(clusterVersion) <nl> if err != nil { <nl> @@ -79,7 +82,7 @@ func checkDockerVersion(ctx context.Context, h *Host, clusterVersion string) err <nl> return fmt.Errorf(\"Error while determining supported Docker version [%s]: %v\", info.ServerVersion, err) <nl> } <nl> - if !isvalid && !h.IgnoreDockerVersion { <nl> + if !isvalid { <nl> return fmt.Errorf(\"Unsupported Docker version found [%s], supported versions are %v\", info.ServerVersion, metadata.K8sVersionToDockerVersions[K8sVersion]) <nl> } else if !isvalid { <nl> log.Warnf(ctx, \"Unsupported Docker version found [%s], supported versions are %v\", info.ServerVersion, metadata.K8sVersionToDockerVersions[K8sVersion]) <nl> ", "msg": "Do not parse docker version when validation is disabled\nWhen IgnoreDockerVersion is set, do not try to parse its version number\nStill retrieve docker info to check check fi docker is reachable\ncloses"}
{"diff_id": 1922, "repo": "rancher/rke", "sha": "7c4c1324f9ed99f32c9e8c59fef01b0a5b181695", "time": "20.09.2019 14:40:53", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -585,6 +585,14 @@ func (c *Cluster) BuildKubeProxyProcess(host *hosts.Host, prefixPath string, svc <nl> CommandArgs[k] = v <nl> } <nl> } <nl> + // If cloudprovider is set (not empty), set the bind address because the node will not be able to retrieve it's IP address in case cloud provider changes the node object name (i.e. AWS and Openstack) <nl> + if c.CloudProvider.Name != \"\" { <nl> + if host.InternalAddress != \"\" && host.Address != host.InternalAddress { <nl> + CommandArgs[\"bind-address\"] = host.InternalAddress <nl> + } else { <nl> + CommandArgs[\"bind-address\"] = host.Address <nl> + } <nl> + } <nl> // Best security practice is to listen on localhost, but DinD uses private container network instead of Host. <nl> if c.DinD { <nl> ", "msg": "Provide IP for kube-proxy if cloudprovider is set\nIf cloudprovider is set (not empty), set the bind address because the node will not be able to retrieve it's IP address because the nodename could be set by the cloud provider (e.g. AWS and Openstack)"}
{"diff_id": 1931, "repo": "rancher/rke", "sha": "72fd42b8d2a75573c07c3f40c50965d1450899a7", "time": "11.11.2019 14:29:24", "diff": "mmm a / cmd/cert.go <nl> ppp b / cmd/cert.go <nl>@@ -3,6 +3,7 @@ package cmd <nl> import ( <nl> \"context\" <nl> \"fmt\" <nl> + \"time\" <nl> \"github.com/sirupsen/logrus\" <nl> @@ -159,8 +160,9 @@ func rebuildClusterWithRotatedCertificates(ctx context.Context, <nl> if err := kubeCluster.SetUpHosts(ctx, flags); err != nil { <nl> return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> + <nl> // Save new State <nl> - if err := kubeCluster.UpdateClusterCurrentState(ctx, clusterState); err != nil { <nl> + if err := saveClusterState(ctx, kubeCluster, clusterState); err != nil { <nl> return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> @@ -202,6 +204,26 @@ func rebuildClusterWithRotatedCertificates(ctx context.Context, <nl> return APIURL, caCrt, clientCert, clientKey, kubeCluster.Certificates, nil <nl> } <nl> +func saveClusterState(ctx context.Context, kubeCluster *cluster.Cluster, clusterState *cluster.FullState) error { <nl> + var err error <nl> + if err = kubeCluster.UpdateClusterCurrentState(ctx, clusterState); err != nil { <nl> + return err <nl> + } <nl> + // Attempt to store cluster full state to Kubernetes <nl> + for i := 1; i <= 3; i++ { <nl> + err = cluster.SaveFullStateToKubernetes(ctx, kubeCluster, clusterState) <nl> + if err != nil { <nl> + time.Sleep(time.Second * time.Duration(2)) <nl> + continue <nl> + } <nl> + break <nl> + } <nl> + if err != nil { <nl> + logrus.Warnf(\"Failed to save full cluster state to Kubernetes\") <nl> + } <nl> + return nil <nl> +} <nl> + <nl> func rotateRKECertificates(ctx context.Context, kubeCluster *cluster.Cluster, flags cluster.ExternalFlags, rkeFullState *cluster.FullState) (*cluster.FullState, error) { <nl> log.Infof(ctx, \"Rotating Kubernetes cluster certificates\") <nl> currentCluster, err := kubeCluster.GetClusterState(ctx, rkeFullState) <nl> ", "msg": "Save cluster state to k8s on cert rotation\nIn addition to storing it on the disk"}
{"diff_id": 1935, "repo": "rancher/rke", "sha": "843e14135fe1b9c5288eaa2ca28f7a5ad665ff99", "time": "15.11.2019 04:58:59", "diff": "mmm a / cluster/cluster.go <nl> ppp b / cluster/cluster.go <nl>@@ -156,6 +156,7 @@ func (c *Cluster) DeployWorkerPlane(ctx context.Context, svcOptionData map[strin <nl> func parseAuditLogConfig(clusterFile string, rkeConfig *v3.RancherKubernetesEngineConfig) error { <nl> if rkeConfig.Services.KubeAPI.AuditLog != nil && <nl> rkeConfig.Services.KubeAPI.AuditLog.Enabled && <nl> + rkeConfig.Services.KubeAPI.AuditLog.Configuration != nil && <nl> rkeConfig.Services.KubeAPI.AuditLog.Configuration.Policy == nil { <nl> return nil <nl> } <nl> ", "msg": "add null check for audit log config"}
{"diff_id": 1938, "repo": "rancher/rke", "sha": "fb90d1429cde1a0d96e9500f9050c60231a383a6", "time": "05.12.2019 15:31:41", "diff": "mmm a / services/etcd.go <nl> ppp b / services/etcd.go <nl>@@ -28,7 +28,6 @@ const ( <nl> EtcdDataDir = \"/var/lib/rancher/etcd/\" <nl> EtcdInitWaitTime = 10 <nl> EtcdSnapshotWaitTime = 5 <nl> - EtcdSnapshotCompressedExtension = \"zip\" <nl> EtcdPermFixContainerName = \"etcd-fix-perm\" <nl> ) <nl> @@ -379,7 +378,6 @@ func RunEtcdSnapshotSave(ctx context.Context, etcdHost *hosts.Host, prsMap map[s <nl> } <nl> func DownloadEtcdSnapshotFromS3(ctx context.Context, etcdHost *hosts.Host, prsMap map[string]v3.PrivateRegistry, etcdSnapshotImage string, name string, es v3.ETCDService) error { <nl> - <nl> log.Infof(ctx, \"[etcd] Get snapshot [%s] on host [%s]\", name, etcdHost.Address) <nl> s3Backend := es.BackupConfig.S3BackupConfig <nl> if len(s3Backend.Endpoint) == 0 || len(s3Backend.BucketName) == 0 { <nl> @@ -741,7 +739,11 @@ func setEtcdPermissions(ctx context.Context, etcdHost *hosts.Host, prsMap map[st <nl> hostCfg := &container.HostConfig{ <nl> Binds: []string{dataBind}, <nl> } <nl> - return docker.DoRunOnetimeContainer(ctx, etcdHost.DClient, imageCfg, hostCfg, EtcdPermFixContainerName, etcdHost.Address, ETCDRole, prsMap) <nl> + if err := docker.DoRunOnetimeContainer(ctx, etcdHost.DClient, imageCfg, hostCfg, EtcdPermFixContainerName, <nl> + etcdHost.Address, ETCDRole, prsMap); err != nil { <nl> + return err <nl> + } <nl> + return docker.DoRemoveContainer(ctx, etcdHost.DClient, EtcdPermFixContainerName, etcdHost.Address) <nl> } <nl> func getSanitizedSnapshotCmd(imageCfg *container.Config, bc *v3.BackupConfig) string { <nl> ", "msg": "Permissions fix for etcd restore with custom uid/gid"}
{"diff_id": 1954, "repo": "rancher/rke", "sha": "b2a71ce067b906b11574f4305ff39f001f113136", "time": "12.03.2020 12:22:52", "diff": "mmm a / cmd/up.go <nl> ppp b / cmd/up.go <nl>@@ -79,7 +79,7 @@ func UpCommand() cli.Command { <nl> func ClusterUp(ctx context.Context, dialersOptions hosts.DialersOptions, flags cluster.ExternalFlags, data map[string]interface{}) (string, string, string, string, map[string]pki.CertificatePKI, error) { <nl> var APIURL, caCrt, clientCert, clientKey string <nl> - var reconcileCluster bool <nl> + var reconcileCluster, restore bool <nl> clusterState, err := cluster.ReadStateFile(ctx, cluster.GetStateFilePath(flags.ClusterFilePath, flags.ConfigDir)) <nl> if err != nil { <nl> @@ -145,7 +145,15 @@ func ClusterUp(ctx context.Context, dialersOptions hosts.DialersOptions, flags c <nl> return APIURL, caCrt, clientCert, clientKey, nil, err <nl> } <nl> - if currentCluster != nil { <nl> + /* reconcileCluster flag decides whether zero downtime upgrade logic is used or not. <nl> + Zero-downtime upgrades should happen only when upgrading existing clusters. Not for new clusters or during etcd snapshot restore. <nl> + currentCluster != nil indicates this is an existing cluster. Restore flag on DesiredState.RancherKubernetesEngineConfig indicates if it's a snapshot restore or not. <nl> + reconcileCluster flag should be set to true only if currentCluster is not nil and restore is set to false <nl> + */ <nl> + if clusterState.DesiredState.RancherKubernetesEngineConfig != nil { <nl> + restore = clusterState.DesiredState.RancherKubernetesEngineConfig.Restore.Restore <nl> + } <nl> + if currentCluster != nil && !restore { <nl> // reconcile this cluster, to check if upgrade is needed, or new nodes are getting added/removed <nl> /*This is to separate newly added nodes, so we don't try to check their status/cordon them before upgrade. <nl> This will also cover nodes that were considered inactive first time cluster was provisioned, but are now active during upgrade*/ <nl> ", "msg": "Use non-zero downtime upgrade logic during restore\nIf restore flag is set to true on RKEConfig, the reconcileCluster flag in\nClusterUp should not be set to true, so that DeployControlPlane and DeployWorkerPlane\ncall the non-zero downtime upgrade functions RunControlPlane and RunWorkerPlane respectively"}
{"diff_id": 1963, "repo": "rancher/rke", "sha": "fb17836063a92438a71f7a6899ea20f3bc6e8584", "time": "16.06.2020 15:51:54", "diff": "mmm a / services/node_util.go <nl> ppp b / services/node_util.go <nl>@@ -49,10 +49,14 @@ func cordonAndDrainNode(kubeClient *kubernetes.Clientset, host *hosts.Host, drai <nl> } <nl> func getDrainHelper(kubeClient *kubernetes.Clientset, upgradeStrategy v3.NodeUpgradeStrategy) drain.Helper { <nl> + var ignoreDaemonSets bool <nl> + if upgradeStrategy.DrainInput == nil || *upgradeStrategy.DrainInput.IgnoreDaemonSets { <nl> + ignoreDaemonSets = true <nl> + } <nl> drainHelper := drain.Helper{ <nl> Client: kubeClient, <nl> Force: upgradeStrategy.DrainInput.Force, <nl> - IgnoreAllDaemonSets: *upgradeStrategy.DrainInput.IgnoreDaemonSets, <nl> + IgnoreAllDaemonSets: ignoreDaemonSets, <nl> DeleteLocalData: upgradeStrategy.DrainInput.DeleteLocalData, <nl> GracePeriodSeconds: upgradeStrategy.DrainInput.GracePeriod, <nl> Timeout: time.Second * time.Duration(upgradeStrategy.DrainInput.Timeout), <nl> ", "msg": "Check drainInput for nil to avoid panic"}
{"diff_id": 1971, "repo": "rancher/rke", "sha": "2c270fa5ab2556e9eec3f2079cb6f2075d6dbfa9", "time": "27.07.2020 14:06:50", "diff": "mmm a / k8s/node.go <nl> ppp b / k8s/node.go <nl>@@ -24,8 +24,8 @@ const ( <nl> ) <nl> func DeleteNode(k8sClient *kubernetes.Clientset, nodeName, cloudProvider string) error { <nl> - <nl> - if cloudProvider == AWSCloudProvider { <nl> + // If cloud provider is configured, the node name can be set by the cloud provider, which can be different from the original node name <nl> + if cloudProvider != \"\" { <nl> node, err := GetNode(k8sClient, nodeName) <nl> if err != nil { <nl> return err <nl> ", "msg": "Use node label when cloudprovider is configured\nWhen cloud provider is configured, the node name can be set by the cloud provider. If we don't account for this, the node cannot be found when we do cluster operation (for example, node delete)"}
{"diff_id": 1976, "repo": "rancher/rke", "sha": "a252645797e5e3f6ee56cd9ad0a97e2166433a30", "time": "09.11.2020 13:11:43", "diff": "mmm a / cluster/network.go <nl> ppp b / cluster/network.go <nl>@@ -6,6 +6,7 @@ import ( <nl> \"net\" <nl> \"strconv\" <nl> \"strings\" <nl> + \"time\" <nl> \"github.com/docker/docker/api/types/container\" <nl> \"github.com/docker/go-connections/nat\" <nl> @@ -529,7 +530,7 @@ func (c *Cluster) runServicePortChecks(ctx context.Context) error { <nl> func checkPlaneTCPPortsFromHost(ctx context.Context, host *hosts.Host, portList []string, planeHosts []*hosts.Host, image string, prsMap map[string]v3.PrivateRegistry) error { <nl> var hosts []string <nl> - <nl> + var portCheckLogs string <nl> for _, host := range planeHosts { <nl> hosts = append(hosts, host.InternalAddress) <nl> } <nl> @@ -551,6 +552,8 @@ func checkPlaneTCPPortsFromHost(ctx context.Context, host *hosts.Host, portList <nl> Type: \"json-file\", <nl> }, <nl> } <nl> + for retries := 0; retries < 3; retries++ { <nl> + logrus.Infof(\"[network] Checking if host [%s] can connect to host(s) [%s] on port(s) [%s], try #%d\", host.Address, strings.Join(hosts, \" \"), strings.Join(portList, \" \"), retries+1) <nl> if err := docker.DoRemoveContainer(ctx, host.DClient, PortCheckContainer, host.Address); err != nil { <nl> return err <nl> } <nl> @@ -568,12 +571,14 @@ func checkPlaneTCPPortsFromHost(ctx context.Context, host *hosts.Host, portList <nl> return err <nl> } <nl> logrus.Debugf(\"[network] Length of containerLog is [%d] on host: %s\", len(containerLog), host.Address) <nl> - if len(containerLog) > 0 { <nl> - portCheckLogs := strings.Join(strings.Split(strings.TrimSpace(containerLog), \"\\n\"), \", \") <nl> - return fmt.Errorf(\"[network] Host [%s] is not able to connect to the following ports: [%s]. Please check network policies and firewall rules\", host.Address, portCheckLogs) <nl> - } <nl> + if len(containerLog) == 0 { <nl> return nil <nl> } <nl> + portCheckLogs = strings.Join(strings.Split(strings.TrimSpace(containerLog), \"\\n\"), \", \") <nl> + time.Sleep(5 * time.Second) <nl> + } <nl> + return fmt.Errorf(\"[network] Host [%s] is not able to connect to the following ports: [%s]. Please check network policies and firewall rules\", host.Address, portCheckLogs) <nl> +} <nl> func getPortBindings(hostAddress string, portList []string) []nat.PortBinding { <nl> portBindingList := []nat.PortBinding{} <nl> ", "msg": "Add retry to TCP port check"}
{"diff_id": 1987, "repo": "rancher/rke", "sha": "313f3635a64b73e7b807a956ed1fd92176fb8d42", "time": "24.03.2021 21:23:34", "diff": "mmm a / services/etcd.go <nl> ppp b / services/etcd.go <nl>@@ -449,14 +449,20 @@ func DownloadEtcdSnapshotFromS3(ctx context.Context, etcdHost *hosts.Host, prsMa <nl> \"--name\", name, <nl> \"--s3-backup=true\", <nl> \"--s3-endpoint=\" + s3Backend.Endpoint, <nl> - \"--s3-accessKey=\" + s3Backend.AccessKey, <nl> - \"--s3-secretKey=\" + s3Backend.SecretKey, <nl> \"--s3-bucketName=\" + s3Backend.BucketName, <nl> \"--s3-region=\" + s3Backend.Region, <nl> }, <nl> Image: etcdSnapshotImage, <nl> Env: es.ExtraEnv, <nl> } <nl> + // Base64 encoding S3 accessKey and secretKey before add them as env variables <nl> + if len(s3Backend.AccessKey) > 0 || len(s3Backend.SecretKey) > 0 { <nl> + env := []string{ <nl> + \"S3_ACCESS_KEY=\" + base64.StdEncoding.EncodeToString([]byte(s3Backend.AccessKey)), <nl> + \"S3_SECRET_KEY=\" + base64.StdEncoding.EncodeToString([]byte(s3Backend.SecretKey)), <nl> + } <nl> + imageCfg.Env = append(imageCfg.Env, env...) <nl> + } <nl> s3Logline := fmt.Sprintf(\"[etcd] Snapshot [%s] will be downloaded on host [%s] from S3 compatible backend at [%s] from bucket [%s] using accesskey [%s]\", name, etcdHost.Address, s3Backend.Endpoint, s3Backend.BucketName, s3Backend.AccessKey) <nl> if s3Backend.Region != \"\" { <nl> s3Logline += fmt.Sprintf(\" and using region [%s]\", s3Backend.Region) <nl> @@ -581,11 +587,17 @@ func RunEtcdSnapshotRemove(ctx context.Context, etcdHost *hosts.Host, prsMap map <nl> s3cmd := []string{ <nl> \"--s3-backup\", <nl> \"--s3-endpoint=\" + es.BackupConfig.S3BackupConfig.Endpoint, <nl> - \"--s3-accessKey=\" + es.BackupConfig.S3BackupConfig.AccessKey, <nl> - \"--s3-secretKey=\" + es.BackupConfig.S3BackupConfig.SecretKey, <nl> \"--s3-bucketName=\" + es.BackupConfig.S3BackupConfig.BucketName, <nl> \"--s3-region=\" + es.BackupConfig.S3BackupConfig.Region, <nl> } <nl> + // Base64 encoding S3 accessKey and secretKey before add them as env variables <nl> + if len(es.BackupConfig.S3BackupConfig.AccessKey) > 0 || len(es.BackupConfig.S3BackupConfig.SecretKey) > 0 { <nl> + env := []string{ <nl> + \"S3_ACCESS_KEY=\" + base64.StdEncoding.EncodeToString([]byte(es.BackupConfig.S3BackupConfig.AccessKey)), <nl> + \"S3_SECRET_KEY=\" + base64.StdEncoding.EncodeToString([]byte(es.BackupConfig.S3BackupConfig.SecretKey)), <nl> + } <nl> + imageCfg.Env = append(imageCfg.Env, env...) <nl> + } <nl> if es.BackupConfig.S3BackupConfig.CustomCA != \"\" { <nl> caStr := base64.StdEncoding.EncodeToString([]byte(es.BackupConfig.S3BackupConfig.CustomCA)) <nl> s3cmd = append(s3cmd, \"--s3-endpoint-ca=\"+caStr) <nl> @@ -671,11 +683,17 @@ func configS3BackupImgCmd(ctx context.Context, imageCfg *container.Config, bc *v <nl> cmd = append(cmd, []string{ <nl> \"--s3-backup=true\", <nl> \"--s3-endpoint=\" + bc.S3BackupConfig.Endpoint, <nl> - \"--s3-accessKey=\" + bc.S3BackupConfig.AccessKey, <nl> - \"--s3-secretKey=\" + bc.S3BackupConfig.SecretKey, <nl> \"--s3-bucketName=\" + bc.S3BackupConfig.BucketName, <nl> \"--s3-region=\" + bc.S3BackupConfig.Region, <nl> }...) <nl> + // Base64 encoding S3 accessKey and secretKey before add them as env variables <nl> + if len(bc.S3BackupConfig.AccessKey) > 0 || len(bc.S3BackupConfig.SecretKey) > 0 { <nl> + env := []string{ <nl> + \"S3_ACCESS_KEY=\" + base64.StdEncoding.EncodeToString([]byte(bc.S3BackupConfig.AccessKey)), <nl> + \"S3_SECRET_KEY=\" + base64.StdEncoding.EncodeToString([]byte(bc.S3BackupConfig.SecretKey)), <nl> + } <nl> + imageCfg.Env = append(imageCfg.Env, env...) <nl> + } <nl> s3Logline := fmt.Sprintf(\"[etcd] Snapshots configured to S3 compatible backend at [%s] to bucket [%s] using accesskey [%s]\", bc.S3BackupConfig.Endpoint, bc.S3BackupConfig.BucketName, bc.S3BackupConfig.AccessKey) <nl> if bc.S3BackupConfig.Region != \"\" { <nl> s3Logline += fmt.Sprintf(\" and using region [%s]\", bc.S3BackupConfig.Region) <nl> ", "msg": "Base64 encoding etcd backup S3 accessKey and secretKey and passing them as env variables to rke-tools"}
{"diff_id": 1991, "repo": "rancher/rke", "sha": "9381a255bf56f5aedcd1f3ebf744c265942edcc9", "time": "19.05.2021 08:32:36", "diff": "mmm a / pki/util.go <nl> ppp b / pki/util.go <nl>@@ -592,7 +592,7 @@ func ReadCertsAndKeysFromDir(certDir string) (map[string]CertificatePKI, error) <nl> for _, file := range files { <nl> logrus.Debugf(\"[certificates] reading file %s from directory [%s]\", file.Name(), certDir) <nl> - if !strings.HasSuffix(file.Name(), \"-key.pem\") && !strings.HasSuffix(file.Name(), \"-csr.pem\") { <nl> + if strings.HasSuffix(file.Name(), \".pem\") && !strings.HasSuffix(file.Name(), \"-key.pem\") && !strings.HasSuffix(file.Name(), \"-csr.pem\") { <nl> // fetching cert <nl> cert, err := getCertFromFile(certDir, file.Name()) <nl> if err != nil { <nl> ", "msg": "Update cert filename validation to *.pem\nUsers leveraging custom certs directories face errors when deploying\nif the directory contains any files that do not end in .pem. This\nchange adds additional validation to ensure files are *.pem before\nattempting further logic."}
{"diff_id": 1995, "repo": "rancher/rke", "sha": "57a2ba00a00d23ff9ab17aab5d521627788c3de4", "time": "10.06.2021 15:58:04", "diff": "mmm a / pki/services.go <nl> ppp b / pki/services.go <nl>@@ -419,9 +419,12 @@ func GenerateEtcdCSRs(ctx context.Context, certs map[string]CertificatePKI, rkeC <nl> for _, host := range etcdHosts { <nl> etcdName := GetCrtNameForHost(host, EtcdCertName) <nl> etcdCrt := certs[etcdName].Certificate <nl> - etcdCSRPEM := certs[etcdName].CSRPEM <nl> - if etcdCSRPEM != \"\" { <nl> - return nil <nl> + etcdCsr := certs[etcdName].CSR <nl> + if etcdCsr != nil { <nl> + if reflect.DeepEqual(etcdAltNames.DNSNames, etcdCsr.DNSNames) && <nl> + DeepEqualIPsAltNames(etcdAltNames.IPs, etcdCsr.IPAddresses) { <nl> + continue <nl> + } <nl> } <nl> logrus.Infof(\"[certificates] Generating etcd-%s csr and key\", host.InternalAddress) <nl> etcdCSR, etcdKey, err := GenerateCertSigningRequestAndKey(true, EtcdCertName, etcdAltNames, certs[etcdName].Key, nil) <nl> @@ -532,7 +535,7 @@ func GenerateKubeletCSR(ctx context.Context, certs map[string]CertificatePKI, rk <nl> if oldKubeletCSR != nil && <nl> reflect.DeepEqual(kubeletAltNames.DNSNames, oldKubeletCSR.DNSNames) && <nl> DeepEqualIPsAltNames(kubeletAltNames.IPs, oldKubeletCSR.IPAddresses) { <nl> - return nil <nl> + continue <nl> } <nl> logrus.Infof(\"[certificates] Generating %s Kubernetes Kubelet csr\", kubeletName) <nl> kubeletCSR, kubeletKey, err := GenerateCertSigningRequestAndKey(true, kubeletName, kubeletAltNames, certs[kubeletName].Key, nil) <nl> ", "msg": "fix custom certs csr generation to do deep compairson for etcd and compare for all nodes and not just first"}
{"diff_id": 1999, "repo": "rancher/rke", "sha": "a0c564b6a71dd123859c68ec687bbf687ee83949", "time": "24.06.2021 10:51:50", "diff": "mmm a / types/rke_types.go <nl> ppp b / types/rke_types.go <nl>@@ -34,7 +34,7 @@ type RancherKubernetesEngineConfig struct { <nl> // Enable/disable strict docker version checking <nl> IgnoreDockerVersion *bool `yaml:\"ignore_docker_version\" json:\"ignoreDockerVersion\" norman:\"default=true\"` <nl> // Enable/disable using cri-dockerd <nl> - EnableCRIDockerd *bool `yaml:\"enable_cri_dockerd\" json:\"enableCRIDockerd\" norman:\"default=false\"` <nl> + EnableCRIDockerd *bool `yaml:\"enable_cri_dockerd\" json:\"enableCriDockerd\" norman:\"default=false\"` <nl> // Kubernetes version to use (if kubernetes image is specified, image version takes precedence) <nl> Version string `yaml:\"kubernetes_version\" json:\"kubernetesVersion,omitempty\"` <nl> // List of private registries and their credentials <nl> ", "msg": "Fix case for CRI Dockerd JSON key"}
{"diff_id": 2006, "repo": "rancher/rke", "sha": "0404dba56dc5651f8e300cdca3a51eb3dc12f303", "time": "21.09.2021 23:33:20", "diff": "mmm a / cluster/encryption.go <nl> ppp b / cluster/encryption.go <nl>@@ -23,8 +23,6 @@ import ( <nl> v1 \"k8s.io/api/core/v1\" <nl> apierrors \"k8s.io/apimachinery/pkg/api/errors\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> - \"k8s.io/apimachinery/pkg/runtime\" <nl> - \"k8s.io/apimachinery/pkg/runtime/serializer\" <nl> apiserverconfig \"k8s.io/apiserver/pkg/apis/config\" <nl> apiserverconfigv1 \"k8s.io/apiserver/pkg/apis/config/v1\" <nl> \"k8s.io/client-go/kubernetes\" <nl> @@ -556,26 +554,12 @@ func parseCustomConfig(customConfig map[string]interface{}) (*apiserverconfigv1. <nl> if err != nil { <nl> return nil, fmt.Errorf(\"error marshalling: %v\", err) <nl> } <nl> - scheme := runtime.NewScheme() <nl> - err = apiserverconfig.AddToScheme(scheme) <nl> - if err != nil { <nl> - return nil, fmt.Errorf(\"error adding to scheme: %v\", err) <nl> - } <nl> - err = apiserverconfigv1.AddToScheme(scheme) <nl> - if err != nil { <nl> - return nil, fmt.Errorf(\"error adding to scheme: %v\", err) <nl> - } <nl> - codecs := serializer.NewCodecFactory(scheme) <nl> - decoder := codecs.UniversalDecoder() <nl> - decodedObj, objType, err := decoder.Decode(data, nil, nil) <nl> + // apiserverconfigv1.EncryptionConfiguration struct has json tags defined, using JSON Unmarshal instead of runtime serializer <nl> + decodedConfig := &apiserverconfigv1.EncryptionConfiguration{} <nl> + err = json.Unmarshal(data, decodedConfig) <nl> if err != nil { <nl> - return nil, fmt.Errorf(\"error decoding data: %v\", err) <nl> - } <nl> - <nl> - decodedConfig, ok := decodedObj.(*apiserverconfigv1.EncryptionConfiguration) <nl> - if !ok { <nl> - return nil, fmt.Errorf(\"unexpected type: %T\", objType) <nl> + return nil, fmt.Errorf(\"error unmarshalling: %v\", err) <nl> } <nl> return decodedConfig, nil <nl> } <nl> ", "msg": "Update cluster.parseCustomConfig function to proper decode custom EncryptionConfiguration"}
{"diff_id": 2013, "repo": "rancher/rke", "sha": "f1776413730aef778c9aa869197c258ba233172b", "time": "02.03.2022 16:43:47", "diff": "mmm a / cmd/up.go <nl> ppp b / cmd/up.go <nl>@@ -103,9 +103,11 @@ func ClusterUp(ctx context.Context, dialersOptions hosts.DialersOptions, flags c <nl> return rebuildClusterWithRotatedCertificates(ctx, dialersOptions, flags, svcOptionsData) <nl> } <nl> // if we need to rotate the encryption key, do so and then return <nl> + // note that we rotate the encryption key only when updating an existing cluster that has secret encryption enabled <nl> + // all other cases will be handled later by reconciling the encryption provider config <nl> if kubeCluster.RancherKubernetesEngineConfig.RotateEncryptionKey { <nl> - // rotate the encryption key only when updating an existing cluster <nl> - if clusterState.CurrentState.RancherKubernetesEngineConfig != nil { <nl> + appliedConfig := clusterState.CurrentState.RancherKubernetesEngineConfig <nl> + if appliedConfig != nil && appliedConfig.Services.KubeAPI.SecretsEncryptionConfig != nil && appliedConfig.Services.KubeAPI.SecretsEncryptionConfig.Enabled { <nl> return RotateEncryptionKey(ctx, clusterState.CurrentState.RancherKubernetesEngineConfig.DeepCopy(), dialersOptions, flags) <nl> } <nl> } <nl> ", "msg": "rotate the encryption key only when updating an existing cluster that has secret encryption enabled"}
{"diff_id": 2020, "repo": "rancher/rke", "sha": "3b8991e34577270473d3b94b67d75560c63c8087", "time": "04.08.2022 12:52:39", "diff": "mmm a / cluster/plan.go <nl> ppp b / cluster/plan.go <nl>@@ -512,7 +512,8 @@ func (c *Cluster) BuildKubeletProcess(host *hosts.Host, serviceOptions v3.Kubern <nl> // cri-dockerd must be enabled if the cluster version is 1.24 and higher <nl> if parsedRangeAtLeast124(parsedVersion) { <nl> CommandArgs[\"container-runtime-endpoint\"] = \"unix:///var/run/cri-dockerd.sock\" <nl> - Binds = []string{fmt.Sprintf(\"%s:/var/lib/cri-dockerd:z\", path.Join(host.PrefixPath, \"/var/lib/cri-dockerd\"))} <nl> + Binds = []string{fmt.Sprintf(\"%s:/var/lib/cri-dockerd:z\", path.Join(host.PrefixPath, \"/var/lib/cri-dockerd\")), <nl> + fmt.Sprintf(\"%s:%s\", path.Join(host.PrefixPath, KubeletDockerConfigPath), \"/.docker/config.json\")} <nl> } <nl> } <nl> ", "msg": "bind rke kubelet docker config path to default docker config path\nrke stores auth info in /var/lib/kubelet/config.json but cri-dockerd\nrelies on k8.io credential provider which uses only default config\nprovider, this allows cri-dockerd to pull sandbox pause image using\nprivate registry"}
{"diff_id": 2022, "repo": "rancher/rke", "sha": "9ce38db01264f7f3385e39912306838fbcb9f58e", "time": "21.09.2022 14:05:59", "diff": "mmm a / pki/cert/cert.go <nl> ppp b / pki/cert/cert.go <nl>@@ -149,7 +149,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS <nl> CommonName: fmt.Sprintf(\"%s-ca@%d\", host, time.Now().Unix()), <nl> }, <nl> NotBefore: time.Now(), <nl> - NotAfter: time.Now().Add(time.Hour * 24 * 365), <nl> + NotAfter: time.Now().Add(duration365d), <nl> KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, <nl> BasicConstraintsValid: true, <nl> @@ -177,7 +177,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS <nl> CommonName: fmt.Sprintf(\"%s@%d\", host, time.Now().Unix()), <nl> }, <nl> NotBefore: time.Now(), <nl> - NotAfter: time.Now().Add(time.Hour * 24 * 365), <nl> + NotAfter: time.Now().Add(duration365d), <nl> KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, <nl> ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth}, <nl> ", "msg": "Use constant instead of value for cert duration"}
{"diff_id": 2027, "repo": "rancher/rke", "sha": "bd3fe59b0de61cc36750b8651eb155e491d3fe0d", "time": "29.12.2022 17:15:38", "diff": "mmm a / cluster/validation.go <nl> ppp b / cluster/validation.go <nl>@@ -681,7 +681,7 @@ func validatePodSecurityPolicy(c *Cluster) error { <nl> kubeClient, err := k8s.NewClient(c.LocalKubeConfigPath, c.K8sWrapTransport) <nl> if err != nil { <nl> // we can not tell this is invoked when creating a new cluster or updating an existing one, so skip this check <nl> - logrus.Warnf(\"Skip the check for PSP resource due to the failire of initializing the kubernetes client\") <nl> + logrus.Debugf(\"Skip the check for PSP resource due to the failire of initializing the kubernetes client\") <nl> return nil <nl> } <nl> pspList, _ := k8s.GetPSPList(kubeClient) <nl> ", "msg": "change Warning to Debug to reduce noise in logs"}
{"diff_id": 2031, "repo": "kubermatic/kubeone", "sha": "2cf6f1393603fb67d88996600eac676408609481", "time": "08.11.2018 11:35:24", "diff": "mmm a / None <nl> ppp b / pkg/templates/machinecontroller.go <nl>+package templates <nl> + <nl> +import ( <nl> + appsv1 \"k8s.io/api/apps/v1\" <nl> + corev1 \"k8s.io/api/core/v1\" <nl> + rbac_v1beta1 \"k8s.io/api/rbac/v1beta1\" <nl> + apiextensionsv1beta1 \"k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1beta1\" <nl> + metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> + \"k8s.io/apimachinery/pkg/util/intstr\" <nl> + <nl> + \"github.com/kubermatic/kubeone/pkg/manifest\" <nl> +) <nl> + <nl> +const ( <nl> + MachineControllerAppLabelKey = \"app\" <nl> + MachineControllerAppLabelValue = \"machine-controller\" <nl> + MachineControllerTag = \"v0.9.9\" <nl> +) <nl> + <nl> +func MachineControllerConfiguration(manifest *manifest.Manifest, instance int) (string, error) { <nl> + items := []interface{}{ <nl> + machineControllerClusterRole(), <nl> + machineControllerClusterRoleBinding(), <nl> + nodeBootstrapperClusterRoleBinding(), <nl> + machineControllerKubeSystemRole(), <nl> + machineControllerKubePublicRole(), <nl> + machineControllerDefaultRole(), <nl> + machineControllerClusterInfoRole(), <nl> + machineControllerKubeSystemRoleBinding(), <nl> + machineControllerKubePublicRoleBinding(), <nl> + machineControllerDefaultRoleBinding(), <nl> + machineControllerClusterInfoRoleBinding(), <nl> + machineControllerMachineCRD(), <nl> + machineControllerClusterCRD(), <nl> + machineControllerMachineSetCRD(), <nl> + machineControllerMachineDeploymentCRD(), <nl> + machineControllerDeployment(), <nl> + } <nl> + <nl> + return kubernetesToYAML(items) <nl> +} <nl> + <nl> +func machineControllerClusterRole() rbac_v1beta1.ClusterRole { <nl> + role := rbac_v1beta1.ClusterRole{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"system:kubermatic-machine-controller\", <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Rules: []rbac_v1beta1.PolicyRule{ <nl> + { <nl> + APIGroups: []string{\"apiextensions.k8s.io\"}, <nl> + Resources: []string{\"customresourcedefinitions\"}, <nl> + Verbs: []string{\"get\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"apiextensions.k8s.io\"}, <nl> + Resources: []string{\"customresourcedefinitions\"}, <nl> + ResourceNames: []string{\"machines.machine.k8s.io\"}, <nl> + Verbs: []string{\"delete\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"apiextensions.k8s.io\"}, <nl> + Resources: []string{\"customresourcedefinitions\"}, <nl> + ResourceNames: []string{\"machines.machine.k8s.io\"}, <nl> + Verbs: []string{\"*\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"machine.k8s.io\"}, <nl> + Resources: []string{\"machines\"}, <nl> + Verbs: []string{\"*\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"nodes\"}, <nl> + Verbs: []string{\"*\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"pods\"}, <nl> + Verbs: []string{\"list\", \"get\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"pods/eviction\"}, <nl> + Verbs: []string{\"create\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"events\"}, <nl> + Verbs: []string{\"create\", \"patch\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"cluster.k8s.io\"}, <nl> + Resources: []string{\"machines\", \"machinesets\", \"machinesets/status\", \"machinedeployments\", \"machinedeployments/status\", \"clusters\", \"clusters/status\"}, <nl> + Verbs: []string{\"*\"}, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return role <nl> +} <nl> + <nl> +func machineControllerClusterRoleBinding() rbac_v1beta1.ClusterRoleBinding { <nl> + crb := rbac_v1beta1.ClusterRoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"system:kubermatic-machine-controller:controller\", <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"system:kubermatic-machine-controller\", <nl> + Kind: \"ClusterRole\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"User\", <nl> + Name: \"machine-controller\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return crb <nl> +} <nl> + <nl> +func nodeBootstrapperClusterRoleBinding() rbac_v1beta1.ClusterRoleBinding { <nl> + crb := rbac_v1beta1.ClusterRoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"system:kubermatic-machine-controller:kubelet-bootstrap\", <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"system:node-bootstrapper\", <nl> + Kind: \"ClusterRole\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"Group\", <nl> + Name: \"system:bootstrappers:machine-controller:default-node-token\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return crb <nl> +} <nl> + <nl> +func nodeSignerClusterRoleBinding() rbac_v1beta1.ClusterRoleBinding { <nl> + crb := rbac_v1beta1.ClusterRoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"system:kubermatic-machine-controller:node-signer\", <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"system:certificates.k8s.io:certificatesigningrequests:nodeclient\", <nl> + Kind: \"ClusterRole\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"Group\", <nl> + Name: \"system:bootstrappers:machine-controller:default-node-token\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return crb <nl> +} <nl> + <nl> +func machineControllerKubeSystemRole() rbac_v1beta1.Role { <nl> + role := rbac_v1beta1.Role{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceSystem, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Rules: []rbac_v1beta1.PolicyRule{ <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"secrets\"}, <nl> + Verbs: []string{ <nl> + \"create\", <nl> + \"update\", <nl> + \"list\", <nl> + \"watch\", <nl> + }, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"endpoints\"}, <nl> + ResourceNames: []string{\"machine-controller\"}, <nl> + Verbs: []string{\"*\"}, <nl> + }, <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"endpoints\"}, <nl> + Verbs: []string{\"create\"}, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return role <nl> +} <nl> + <nl> +func machineControllerKubePublicRole() rbac_v1beta1.Role { <nl> + role := rbac_v1beta1.Role{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespacePublic, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Rules: []rbac_v1beta1.PolicyRule{ <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"configmaps\"}, <nl> + Verbs: []string{ <nl> + \"get\", <nl> + \"list\", <nl> + \"watch\", <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return role <nl> +} <nl> + <nl> +func machineControllerDefaultRole() rbac_v1beta1.Role { <nl> + role := rbac_v1beta1.Role{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceDefault, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Rules: []rbac_v1beta1.PolicyRule{ <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + Resources: []string{\"endpoints\"}, <nl> + Verbs: []string{ <nl> + \"get\", <nl> + \"list\", <nl> + \"watch\", <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return role <nl> +} <nl> + <nl> +func machineControllerClusterInfoRole() rbac_v1beta1.Role { <nl> + role := rbac_v1beta1.Role{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"cluster-info\", <nl> + Namespace: metav1.NamespacePublic, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Rules: []rbac_v1beta1.PolicyRule{ <nl> + { <nl> + APIGroups: []string{\"\"}, <nl> + ResourceNames: []string{\"cluster-info\"}, <nl> + Resources: []string{\"configmaps\"}, <nl> + Verbs: []string{\"get\"}, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return role <nl> +} <nl> + <nl> +func machineControllerKubeSystemRoleBinding() rbac_v1beta1.RoleBinding { <nl> + rb := rbac_v1beta1.RoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceSystem, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"machine-controller\", <nl> + Kind: \"Role\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"User\", <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceSystem, <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return rb <nl> +} <nl> + <nl> +func machineControllerKubePublicRoleBinding() rbac_v1beta1.RoleBinding { <nl> + rb := rbac_v1beta1.RoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespacePublic, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"machine-controller\", <nl> + Kind: \"Role\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"User\", <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespacePublic, <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return rb <nl> +} <nl> + <nl> +func machineControllerDefaultRoleBinding() rbac_v1beta1.RoleBinding { <nl> + rb := rbac_v1beta1.RoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceDefault, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"machine-controller\", <nl> + Kind: \"Role\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"User\", <nl> + Name: \"machine-controller\", <nl> + Namespace: metav1.NamespaceDefault, <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return rb <nl> +} <nl> + <nl> +func machineControllerClusterInfoRoleBinding() rbac_v1beta1.RoleBinding { <nl> + rb := rbac_v1beta1.RoleBinding{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"cluster-info\", <nl> + Namespace: metav1.NamespacePublic, <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + RoleRef: rbac_v1beta1.RoleRef{ <nl> + Name: \"cluster-info\", <nl> + Kind: \"Role\", <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + Subjects: []rbac_v1beta1.Subject{ <nl> + { <nl> + Kind: \"User\", <nl> + Name: \"cluster-info\", <nl> + Namespace: metav1.NamespacePublic, <nl> + APIGroup: rbac_v1beta1.GroupName, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return rb <nl> +} <nl> + <nl> +func machineControllerMachineCRD() apiextensionsv1beta1.CustomResourceDefinition { <nl> + crd := apiextensionsv1beta1.CustomResourceDefinition{ <nl> + Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec{ <nl> + Group: \"cluster.k8s.io\", <nl> + Version: \"v1alpha1\", <nl> + Scope: apiextensionsv1beta1.NamespaceScoped, <nl> + Names: apiextensionsv1beta1.CustomResourceDefinitionNames{ <nl> + Kind: \"Machine\", <nl> + ListKind: \"MachineList\", <nl> + Plural: \"machines\", <nl> + Singular: \"machine\", <nl> + }, <nl> + }, <nl> + } <nl> + crd.Name = \"machines.cluster.k8s.io\" <nl> + <nl> + return crd <nl> +} <nl> + <nl> +func machineControllerClusterCRD() apiextensionsv1beta1.CustomResourceDefinition { <nl> + crd := apiextensionsv1beta1.CustomResourceDefinition{ <nl> + Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec{ <nl> + Group: \"cluster.k8s.io\", <nl> + Version: \"v1alpha1\", <nl> + Scope: apiextensionsv1beta1.NamespaceScoped, <nl> + Names: apiextensionsv1beta1.CustomResourceDefinitionNames{ <nl> + Kind: \"Cluster\", <nl> + ListKind: \"ClusterList\", <nl> + Plural: \"clusters\", <nl> + Singular: \"cluster\", <nl> + }, <nl> + }, <nl> + } <nl> + crd.Name = \"clusters.cluster.k8s.io\" <nl> + <nl> + return crd <nl> +} <nl> + <nl> +func machineControllerMachineSetCRD() apiextensionsv1beta1.CustomResourceDefinition { <nl> + crd := apiextensionsv1beta1.CustomResourceDefinition{ <nl> + Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec{ <nl> + Group: \"cluster.k8s.io\", <nl> + Version: \"v1alpha1\", <nl> + Scope: apiextensionsv1beta1.NamespaceScoped, <nl> + Names: apiextensionsv1beta1.CustomResourceDefinitionNames{ <nl> + Kind: \"MachineSet\", <nl> + ListKind: \"MachineSetList\", <nl> + Plural: \"machinesets\", <nl> + Singular: \"machineset\", <nl> + }, <nl> + }, <nl> + } <nl> + crd.Name = \"machinesets.cluster.k8s.io\" <nl> + <nl> + return crd <nl> +} <nl> + <nl> +func machineControllerMachineDeploymentCRD() apiextensionsv1beta1.CustomResourceDefinition { <nl> + crd := apiextensionsv1beta1.CustomResourceDefinition{ <nl> + Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec{ <nl> + Group: \"cluster.k8s.io\", <nl> + Version: \"v1alpha1\", <nl> + Scope: apiextensionsv1beta1.NamespaceScoped, <nl> + Names: apiextensionsv1beta1.CustomResourceDefinitionNames{ <nl> + Kind: \"MachineDeployment\", <nl> + ListKind: \"MachineDeploymentList\", <nl> + Plural: \"machinedeployments\", <nl> + Singular: \"machinedeployment\", <nl> + }, <nl> + }, <nl> + } <nl> + crd.Name = \"machinedeployments.cluster.k8s.io\" <nl> + <nl> + return crd <nl> +} <nl> + <nl> +func machineControllerDeployment() appsv1.Deployment { <nl> + var replicas int32 <nl> + var readMode int32 <nl> + replicas = 1 <nl> + readMode = 0444 <nl> + <nl> + deployment := appsv1.Deployment{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"machine-controller\", <nl> + Labels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Spec: appsv1.DeploymentSpec{ <nl> + Replicas: &replicas, <nl> + Selector: &metav1.LabelSelector{ <nl> + MatchLabels: map[string]string{ <nl> + MachineControllerAppLabelKey: MachineControllerAppLabelValue, <nl> + }, <nl> + }, <nl> + Strategy: appsv1.DeploymentStrategy{ <nl> + Type: appsv1.RollingUpdateDeploymentStrategyType, <nl> + RollingUpdate: &appsv1.RollingUpdateDeployment{ <nl> + MaxSurge: &intstr.IntOrString{ <nl> + Type: intstr.Int, <nl> + IntVal: 1, <nl> + }, <nl> + MaxUnavailable: &intstr.IntOrString{ <nl> + Type: intstr.Int, <nl> + IntVal: 0, <nl> + }, <nl> + }, <nl> + }, <nl> + Template: corev1.PodTemplateSpec{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Annotations: map[string]string{ <nl> + \"prometheus.io/scrape\": \"true\", <nl> + \"prometheus.io/path\": \"/metrics\", <nl> + \"prometheus.io/port\": \"8085\", <nl> + }, <nl> + }, <nl> + Spec: corev1.PodSpec { <nl> + Tolerations: []corev1.Toleration{ <nl> + { <nl> + Key: \"node-role.kubernetes.io/master\", <nl> + Operator: corev1.TolerationOpExists, <nl> + Effect: corev1.TaintEffectNoSchedule, <nl> + }, <nl> + }, <nl> + Containers: []corev1.Container{ <nl> + { <nl> + Name: \"machine-controller\", <nl> + Image: \"docker.io/kubermatic/machine-controller:\" + MachineControllerTag, <nl> + ImagePullPolicy: corev1.PullIfNotPresent, <nl> + Command: []string{\"/usr/local/bin/machine-controller\"}, <nl> + Args: []string{ <nl> + \"-logtostderr\", <nl> + \"-v\", \"4\", <nl> + \"-internal-listen-address\", \"0.0.0.0:8085\", <nl> + }, <nl> + // TODO(xmudrii): check what do to with vars. <nl> + //Env: getEnvVars(data), <nl> + TerminationMessagePath: corev1.TerminationMessagePathDefault, <nl> + TerminationMessagePolicy: corev1.TerminationMessageReadFile, <nl> + ReadinessProbe: &corev1.Probe{ <nl> + Handler: corev1.Handler{ <nl> + HTTPGet: &corev1.HTTPGetAction{ <nl> + Path: \"/ready\", <nl> + Port: intstr.FromInt(8085), <nl> + }, <nl> + }, <nl> + FailureThreshold: 3, <nl> + PeriodSeconds: 10, <nl> + SuccessThreshold: 1, <nl> + TimeoutSeconds: 15, <nl> + }, <nl> + LivenessProbe: &corev1.Probe{ <nl> + FailureThreshold: 8, <nl> + Handler: corev1.Handler{ <nl> + HTTPGet: &corev1.HTTPGetAction{ <nl> + Path: \"/live\", <nl> + Port: intstr.FromInt(8085), <nl> + }, <nl> + }, <nl> + InitialDelaySeconds: 15, <nl> + PeriodSeconds: 10, <nl> + SuccessThreshold: 1, <nl> + TimeoutSeconds: 15, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> + return deployment <nl> +} <nl> ", "msg": "Add deployment template for Machine Controller"}
{"diff_id": 2044, "repo": "kubermatic/kubeone", "sha": "0abf10435d4959bd5f67287924457ff1a91c73fa", "time": "23.11.2018 09:58:31", "diff": "mmm a / pkg/command/install.go <nl> ppp b / pkg/command/install.go <nl>@@ -79,6 +79,16 @@ func createInstallerOptions(clusterFile string, cluster *config.Cluster, ctx *cl <nl> backupFile = filepath.Join(filepath.Dir(fullPath), fmt.Sprintf(\"%s.tar.gz\", clusterName)) <nl> } <nl> + // refuse to overwrite existing backups (NB: since we attempt to <nl> + // write to the file later on to check for write permissions, we <nl> + // inadvertently create a zero byte file even if the first step <nl> + // of the installer fails; for this reason it's okay to find an <nl> + // existing, zero byte backup) <nl> + stat, err := os.Stat(backupFile) <nl> + if err != nil && stat.Size() > 0 { <nl> + return nil, fmt.Errorf(\"backup %s already exists, refusing to overwrite\", backupFile) <nl> + } <nl> + <nl> // try to write to the file before doing anything else <nl> f, err := os.OpenFile(backupFile, os.O_RDWR|os.O_CREATE, 0600) <nl> if err != nil { <nl> ", "msg": "check for existing backups to prevent accidental overwrites"}
{"diff_id": 2047, "repo": "kubermatic/kubeone", "sha": "1ee5dee1b5e48bf928d7fd87429c16d9f0f817cf", "time": "28.11.2018 11:12:37", "diff": "mmm a / pkg/templates/machinecontroller/webhook.go <nl> ppp b / pkg/templates/machinecontroller/webhook.go <nl>@@ -71,6 +71,7 @@ func WebhookDeployment(cluster *config.Cluster) (*appsv1.Deployment, error) { <nl> } <nl> dep.Name = \"machine-controller-webhook\" <nl> + dep.Namespace = WebhookNamespace <nl> dep.Labels = map[string]string{ <nl> WebhookAppLabelKey: WebhookAppLabelValue, <nl> } <nl> @@ -168,6 +169,7 @@ func Service() (*corev1.Service, error) { <nl> } <nl> se.Name = \"machine-controller-webhook\" <nl> + se.Namespace = WebhookNamespace <nl> se.Labels = map[string]string{ <nl> WebhookAppLabelKey: WebhookAppLabelValue, <nl> } <nl> @@ -209,6 +211,7 @@ func TLSServingCertificate(ca *triple.KeyPair) (*corev1.Secret, error) { <nl> } <nl> se.Name = \"machinecontroller-webhook-serving-cert\" <nl> + se.Namespace = WebhookNamespace <nl> se.Data = map[string][]byte{} <nl> commonName := fmt.Sprintf(\"%s.%s.svc.cluster.local.\", WebbhookName, WebhookNamespace) <nl> @@ -236,11 +239,13 @@ func TLSServingCertificate(ca *triple.KeyPair) (*corev1.Secret, error) { <nl> func MutatingwebhookConfiguration(ca *triple.KeyPair) (*admissionregistrationv1beta1.MutatingWebhookConfiguration, error) { <nl> cfg := &admissionregistrationv1beta1.MutatingWebhookConfiguration{ <nl> TypeMeta: metav1.TypeMeta{ <nl> - APIVersion: \"v1alpha1\", <nl> - Kind: \"admissionregistration.k8s.io\", <nl> + APIVersion: \"admissionregistration.k8s.io/v1beta1\", <nl> + Kind: \"MutatingWebhookConfiguration\", <nl> }, <nl> } <nl> + <nl> cfg.Name = \"machine-controller.kubermatic.io\" <nl> + cfg.Namespace = WebhookNamespace <nl> cfg.Webhooks = []admissionregistrationv1beta1.Webhook{ <nl> { <nl> ", "msg": "make sure the MC is deployed consistently into the kube-system namespace"}
{"diff_id": 2048, "repo": "kubermatic/kubeone", "sha": "16c12254ab3457b182fd4f56449a6705b4a8c8f6", "time": "28.11.2018 11:13:05", "diff": "mmm a / pkg/templates/machinecontroller/webhook.go <nl> ppp b / pkg/templates/machinecontroller/webhook.go <nl>@@ -104,6 +104,14 @@ func WebhookDeployment(cluster *config.Cluster) (*appsv1.Deployment, error) { <nl> }, <nl> } <nl> + dep.Spec.Template.Spec.Tolerations = []corev1.Toleration{ <nl> + { <nl> + Key: \"node-role.kubernetes.io/master\", <nl> + Operator: corev1.TolerationOpExists, <nl> + Effect: corev1.TaintEffectNoSchedule, <nl> + }, <nl> + } <nl> + <nl> dep.Spec.Template.Spec.Containers = []corev1.Container{ <nl> { <nl> Name: \"machine-controller-webhook\", <nl> ", "msg": "make the webhook actually be able to be scheduled in a master-only cluster"}
{"diff_id": 2051, "repo": "kubermatic/kubeone", "sha": "96a8b52705efcb4a39e2e5ad0b7a36220e4f78ca", "time": "07.12.2018 13:23:18", "diff": "mmm a / pkg/terraform/config.go <nl> ppp b / pkg/terraform/config.go <nl>@@ -108,16 +108,16 @@ func (c *Config) Validate() error { <nl> // Apply adds the terraform configuration options to the given <nl> // cluster config. <nl> -func (c *Config) Apply(m *config.Cluster) error { <nl> +func (c *Config) Apply(cluster *config.Cluster) error { <nl> if c.KubeOneAPI.Value.Endpoint != \"\" { <nl> - m.APIServer.Address = c.KubeOneAPI.Value.Endpoint <nl> + cluster.APIServer.Address = c.KubeOneAPI.Value.Endpoint <nl> } <nl> hosts := make([]config.HostConfig, 0) <nl> cp := c.KubeOneHosts.Value.ControlPlane[0] <nl> sshPort, _ := strconv.Atoi(cp.SSHPort) <nl> - m.Name = cp.ClusterName <nl> + cluster.Name = cp.ClusterName <nl> // build up a list of master nodes <nl> for i, publicIP := range cp.PublicAddress { <nl> @@ -142,99 +142,96 @@ func (c *Config) Apply(m *config.Cluster) error { <nl> } <nl> if len(hosts) > 0 { <nl> - m.Hosts = hosts <nl> + cluster.Hosts = hosts <nl> } <nl> - // if there's a cloud provider specific configuration, <nl> - // apply it to the worker nodes <nl> - if len(c.KubeOneWorkers.Value) > 0 { <nl> - var ( <nl> - err error <nl> - workerConfigs []config.WorkerConfig <nl> - ) <nl> + // walk through each of the configured workersets and <nl> + // see if they reference a worker config output from <nl> + // terraform <nl> + for idx, workerset := range cluster.Workers { <nl> + // do we have a matching terraform worker section? <nl> + cloudConfRaw, found := c.KubeOneWorkers.Value[workerset.Name] <nl> + if !found { <nl> + continue <nl> + } <nl> + <nl> + if len(cloudConfRaw) != 1 { <nl> + // TODO: log warning? error? <nl> + continue <nl> + } <nl> - switch m.Provider.Name { <nl> + var err error <nl> + <nl> + switch cluster.Provider.Name { <nl> case config.ProviderNameAWS: <nl> - workerConfigs, err = c.updateAWSWorkers(m.Workers) <nl> + err = c.updateAWSWorkerset(&workerset, cloudConfRaw[0]) <nl> case config.ProviderNameDigitalOcean: <nl> - workerConfigs, err = c.updateDigitalOceanWorkers(m.Workers) <nl> + err = c.updateDigitalOceanWorkerset(&workerset, cloudConfRaw[0]) <nl> case config.ProviderNameHetzner: <nl> - workerConfigs, err = c.updateHetznerWorkers(m.Workers) <nl> + err = c.updateHetznerWorkerset(&workerset, cloudConfRaw[0]) <nl> case config.ProviderNameOpenStack: <nl> - workerConfigs, err = c.updateOpenStackWorkers(m.Workers) <nl> + err = c.updateOpenStackWorkerset(&workerset, cloudConfRaw[0]) <nl> case config.ProviderNameVSphere: <nl> - workerConfigs, err = c.updateVSphereWorkers(m.Workers) <nl> + err = c.updateVSphereWorkerset(&workerset, cloudConfRaw[0]) <nl> default: <nl> - return errors.New(\"unknown provider\") <nl> + return fmt.Errorf(\"unknown provider %v\", cluster.Provider.Name) <nl> } <nl> + <nl> if err != nil { <nl> return err <nl> } <nl> - m.Workers = workerConfigs <nl> + cluster.Workers[idx] = workerset <nl> } <nl> return nil <nl> } <nl> -func (c *Config) updateAWSWorkers(workers []config.WorkerConfig) ([]config.WorkerConfig, error) { <nl> - for idx, workerset := range workers { <nl> - cloudConfRaw, found := c.KubeOneWorkers.Value[workerset.Name] <nl> - if !found { <nl> - continue <nl> - } <nl> - if len(cloudConfRaw) != 1 { <nl> - // TODO: log warning? error? <nl> - continue <nl> - } <nl> - <nl> +func (c *Config) updateAWSWorkerset(workerset *config.WorkerConfig, cfg json.RawMessage) error { <nl> var awsCloudConfig awsWorkerConfig <nl> - if err := json.Unmarshal(cloudConfRaw[0], &awsCloudConfig); err != nil { <nl> - return nil, err <nl> + if err := json.Unmarshal(cfg, &awsCloudConfig); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"ami\", awsCloudConfig.AMI); err != nil { <nl> - return nil, err <nl> - } <nl> - if err := setWorkersetFlag(&workerset, \"availabilityZone\", awsCloudConfig.AvailabilityZone); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"ami\", awsCloudConfig.AMI); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"instanceProfile\", awsCloudConfig.InstanceProfile); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"availabilityZone\", awsCloudConfig.AvailabilityZone); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"region\", awsCloudConfig.Region); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"instanceProfile\", awsCloudConfig.InstanceProfile); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"securityGroupIDs\", awsCloudConfig.SecurityGroupIDs); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"region\", awsCloudConfig.Region); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"subnetId\", awsCloudConfig.SubnetID); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"securityGroupIDs\", awsCloudConfig.SecurityGroupIDs); err != nil { <nl> + return err <nl> } <nl> - if err := setWorkersetFlag(&workerset, \"vpcId\", awsCloudConfig.VPCID); err != nil { <nl> - return nil, err <nl> + if err := setWorkersetFlag(workerset, \"subnetId\", awsCloudConfig.SubnetID); err != nil { <nl> + return err <nl> } <nl> - <nl> - workers[idx] = workerset <nl> + if err := setWorkersetFlag(workerset, \"vpcId\", awsCloudConfig.VPCID); err != nil { <nl> + return err <nl> } <nl> - return workers, nil <nl> + return nil <nl> } <nl> -func (c *Config) updateDigitalOceanWorkers(workers []config.WorkerConfig) ([]config.WorkerConfig, error) { <nl> - return nil, errors.New(\"DigitalOcean is not implemented yet\") <nl> +func (c *Config) updateDigitalOceanWorkerset(workerset *config.WorkerConfig, cfg json.RawMessage) error { <nl> + return errors.New(\"DigitalOcean is not implemented yet\") <nl> } <nl> -func (c *Config) updateHetznerWorkers(workers []config.WorkerConfig) ([]config.WorkerConfig, error) { <nl> - return nil, errors.New(\"Hetzner is not implemented yet\") <nl> +func (c *Config) updateHetznerWorkerset(workerset *config.WorkerConfig, cfg json.RawMessage) error { <nl> + return errors.New(\"Hetzner is not implemented yet\") <nl> } <nl> -func (c *Config) updateOpenStackWorkers(workers []config.WorkerConfig) ([]config.WorkerConfig, error) { <nl> - return nil, errors.New(\"OpenStack is not implemented yet\") <nl> +func (c *Config) updateOpenStackWorkerset(workerset *config.WorkerConfig, cfg json.RawMessage) error { <nl> + return errors.New(\"OpenStack is not implemented yet\") <nl> } <nl> -func (c *Config) updateVSphereWorkers(workers []config.WorkerConfig) ([]config.WorkerConfig, error) { <nl> - return nil, errors.New(\"VSphere is not implemented yet\") <nl> +func (c *Config) updateVSphereWorkerset(workerset *config.WorkerConfig, cfg json.RawMessage) error { <nl> + return errors.New(\"VSphere is not implemented yet\") <nl> } <nl> func setWorkersetFlag(w *config.WorkerConfig, name string, value interface{}) error { <nl> ", "msg": "refactor terraform Apply function so we would not need have to replicate the loop over workersets into each cloud provider function"}
{"diff_id": 2058, "repo": "kubermatic/kubeone", "sha": "38f89b9bddb1f4b26ab378333810dd165bf6e2a2", "time": "18.12.2018 22:03:47", "diff": "mmm a / pkg/installer/version/kube112/reset.go <nl> ppp b / pkg/installer/version/kube112/reset.go <nl>@@ -42,14 +42,14 @@ func resetNode(ctx *util.Context, _ *config.HostConfig, conn ssh.Connection) err <nl> } <nl> const destroyScript = ` <nl> -if kubectl cluster-info > /dev/null; then <nl> - kubectl annotate --all --overwrite node kubermatic.io/skip-eviction=true <nl> - kubectl delete machinedeployment -n \"{{ .MACHINE_NS }}\" --all <nl> - kubectl delete machineset -n \"{{ .MACHINE_NS }}\" --all <nl> - kubectl delete machine -n \"{{ .MACHINE_NS }}\" --all <nl> +if sudo kubectl cluster-info > /dev/null; then <nl> + sudo kubectl annotate --all --overwrite node kubermatic.io/skip-eviction=true <nl> + sudo kubectl delete machinedeployment -n \"{{ .MACHINE_NS }}\" --all <nl> + sudo kubectl delete machineset -n \"{{ .MACHINE_NS }}\" --all <nl> + sudo kubectl delete machine -n \"{{ .MACHINE_NS }}\" --all <nl> for try in {1..30}; do <nl> - if kubectl get machine -n \"{{ .MACHINE_NS }}\" 2>&1 | grep -q 'No resources found.'; then <nl> + if sudo kubectl get machine -n \"{{ .MACHINE_NS }}\" 2>&1 | grep -q 'No resources found.'; then <nl> exit 0 <nl> fi <nl> sleep 10s <nl> ", "msg": "run kubectl as sudo user when destroying workers\nkubectl needs access to /etc/kubeneters/admin.conf\nwhich is not accessbile for normal users"}
{"diff_id": 2063, "repo": "kubermatic/kubeone", "sha": "5c2230aca8658a0b575e61365592fbb3cebb8e9e", "time": "10.01.2019 19:31:36", "diff": "mmm a / pkg/templates/kubeadm/v1beta1/kubeadm.go <nl> ppp b / pkg/templates/kubeadm/v1beta1/kubeadm.go <nl>@@ -37,6 +37,10 @@ func NewConfig(ctx *util.Context, host *config.HostConfig) ([]runtime.Object, er <nl> } <nl> controlPlaneEndpoint := fmt.Sprintf(\"%s:6443\", cluster.APIServer.Address) <nl> + hostAdvertiseAddress := host.PrivateAddress <nl> + if hostAdvertiseAddress == \"\" { <nl> + hostAdvertiseAddress = host.PublicAddress <nl> + } <nl> initConfig := &kubeadmv1beta1.InitConfiguration{ <nl> TypeMeta: metav1.TypeMeta{ <nl> @@ -44,6 +48,9 @@ func NewConfig(ctx *util.Context, host *config.HostConfig) ([]runtime.Object, er <nl> Kind: \"InitConfiguration\", <nl> }, <nl> BootstrapTokens: []kubeadmv1beta1.BootstrapToken{{Token: bootstrapToken}}, <nl> + LocalAPIEndpoint: kubeadmv1beta1.APIEndpoint{ <nl> + AdvertiseAddress: hostAdvertiseAddress, <nl> + }, <nl> } <nl> joinConfig := &kubeadmv1beta1.JoinConfiguration{ <nl> @@ -53,7 +60,7 @@ func NewConfig(ctx *util.Context, host *config.HostConfig) ([]runtime.Object, er <nl> }, <nl> ControlPlane: &kubeadmv1beta1.JoinControlPlane{ <nl> LocalAPIEndpoint: kubeadmv1beta1.APIEndpoint{ <nl> - AdvertiseAddress: host.PrivateAddress, <nl> + AdvertiseAddress: hostAdvertiseAddress, <nl> }, <nl> }, <nl> Discovery: kubeadmv1beta1.Discovery{ <nl> ", "msg": "Fix etcd init on cluster with more then 1 interface"}
{"diff_id": 2069, "repo": "kubermatic/kubeone", "sha": "2a86e7a7e4d4bc2939986238d1dc93f86d001d84", "time": "24.04.2019 15:02:53", "diff": "mmm a / pkg/installer/installation/install.go <nl> ppp b / pkg/installer/installation/install.go <nl>@@ -43,6 +43,7 @@ func Install(ctx *util.Context) error { <nl> {Fn: util.BuildKubernetesClientset, ErrMsg: \"unable to build kubernetes clientset\", Retries: 3}, <nl> {Fn: features.Activate, ErrMsg: \"unable to activate features\"}, <nl> {Fn: applyCanalCNI, ErrMsg: \"failed to install cni plugin canal\", Retries: 3}, <nl> + {Fn: patchCoreDNS, ErrMsg: \"failed to patch CoreDNS\", Retries: 3}, <nl> {Fn: machinecontroller.EnsureMachineController, ErrMsg: \"failed to install machine-controller\", Retries: 3}, <nl> {Fn: machinecontroller.WaitReady, ErrMsg: \"failed to wait for machine-controller\", Retries: 3}, <nl> {Fn: createWorkerMachines, ErrMsg: \"failed to create worker machines\", Retries: 3}, <nl> ", "msg": "patch coreDNS deployment for external cloud provider"}
{"diff_id": 2070, "repo": "kubermatic/kubeone", "sha": "f05009a5ec1af2048416bdc89f1f6d2419863b55", "time": "09.05.2019 13:06:20", "diff": "mmm a / pkg/templates/metricsserver/deployment.go <nl> ppp b / pkg/templates/metricsserver/deployment.go <nl>@@ -208,6 +208,7 @@ func metricsServerDeployment() *appsv1.Deployment { <nl> ImagePullPolicy: corev1.PullIfNotPresent, <nl> Args: []string{ <nl> \"--kubelet-insecure-tls\", <nl> + \"--kubelet-preferred-address-types=InternalIP,InternalDNS,ExternalDNS,ExternalIP\", <nl> }, <nl> VolumeMounts: []corev1.VolumeMount{ <nl> { <nl> ", "msg": "Fix metrics-server for all providers"}
{"diff_id": 2077, "repo": "kubermatic/kubeone", "sha": "ae21bb7b058442b85eb7abdf36ea2747e7548ca3", "time": "03.07.2019 16:32:14", "diff": "mmm a / pkg/templates/weave/weave-net.go <nl> ppp b / pkg/templates/weave/weave-net.go <nl>@@ -101,7 +101,7 @@ func Deploy(ctx *util.Context) error { <nl> peers = append(peers, h.PrivateAddress) <nl> } <nl> - ds := daemonSet(ctx.Cluster.ClusterNetwork.CNI.Encrypted, strings.Join(peers, \" \")) <nl> + ds := daemonSet(ctx.Cluster.ClusterNetwork.CNI.Encrypted, strings.Join(peers, \" \"), ctx.Cluster.ClusterNetwork.PodSubnet) <nl> if err := simpleCreateOrUpdate(bgCtx, ctx.DynamicClient, ds); err != nil { <nl> return errors.Wrap(err, \"failed to ensure weave DaemonSet\") <nl> } <nl> @@ -245,7 +245,7 @@ func secret(pass string) *corev1.Secret { <nl> } <nl> } <nl> -func dsEnv(passwordRef bool, peers string) []corev1.EnvVar { <nl> +func dsEnv(passwordRef bool, peers string, podsubnet string) []corev1.EnvVar { <nl> env := []corev1.EnvVar{ <nl> { <nl> Name: \"HOSTNAME\", <nl> @@ -268,6 +268,10 @@ func dsEnv(passwordRef bool, peers string) []corev1.EnvVar { <nl> Name: \"KUBE_PEERS\", <nl> Value: peers, <nl> }, <nl> + { <nl> + Name: \"IPALLOC_RANGE\", <nl> + Value: podsubnet, <nl> + }, <nl> } <nl> if passwordRef { <nl> @@ -287,7 +291,7 @@ func dsEnv(passwordRef bool, peers string) []corev1.EnvVar { <nl> return env <nl> } <nl> -func daemonSet(passwordRef bool, peers string) *appsv1.DaemonSet { <nl> +func daemonSet(passwordRef bool, peers string, podsubnet string) *appsv1.DaemonSet { <nl> var ( <nl> priviledged = true <nl> fileOrCreate = corev1.HostPathFileOrCreate <nl> @@ -322,7 +326,7 @@ func daemonSet(passwordRef bool, peers string) *appsv1.DaemonSet { <nl> { <nl> Name: \"weave\", <nl> Command: []string{\"/home/weave/launch.sh\"}, <nl> - Env: dsEnv(passwordRef, peers), <nl> + Env: dsEnv(passwordRef, peers, podsubnet), <nl> Image: weaveKubeImage + version, <nl> ReadinessProbe: &corev1.Probe{ <nl> Handler: corev1.Handler{ <nl> ", "msg": "Fix weave-net CNI plugin to respect podSubnet"}
{"diff_id": 2082, "repo": "kubermatic/kubeone", "sha": "d6924e9a4aa51fef8136534f1bce0eb3cee0a48b", "time": "20.08.2019 16:41:24", "diff": "mmm a / hack/tools.go <nl> ppp b / hack/tools.go <nl>@@ -14,14 +14,13 @@ See the License for the specific language governing permissions and <nl> limitations under the License. <nl> */ <nl> - <nl> // +build tools <nl> package tools <nl> import ( <nl> - _ \"k8s.io/code-generator/cmd/deepcopy-gen\" <nl> + _ \"k8s.io/code-generator\" <nl> _ \"k8s.io/code-generator/cmd/conversion-gen\" <nl> + _ \"k8s.io/code-generator/cmd/deepcopy-gen\" <nl> _ \"k8s.io/code-generator/cmd/defaulter-gen\" <nl> - _ \"k8s.io/code-generator\" <nl> ) <nl> ", "msg": "Run gofmt for hack/tools.go"}
{"diff_id": 2095, "repo": "kubermatic/kubeone", "sha": "832ca6be90f83efc6661d41d56a03278c1fee9f4", "time": "17.10.2019 15:19:40", "diff": "mmm a / pkg/cmd/root.go <nl> ppp b / pkg/cmd/root.go <nl>@@ -74,7 +74,7 @@ func newRoot() *cobra.Command { <nl> fs := rootCmd.PersistentFlags() <nl> fs.StringVarP(&opts.TerraformState, globalTerraformFlagName, \"t\", \"\", <nl> - \"Source for terrafor output JSON. - to read from stdin. If path is file, contents will be used. If path is dictionary, `terraform output -json` is executed in this path\") <nl> + \"Source for terraform output in JSON - to read from stdin. If path is a file, contents will be used. If path is a dictionary, `terraform output -json` is executed in this path\") <nl> fs.StringVarP(&opts.CredentialsFilePath, globalCredentialsFlagName, \"c\", \"\", \"File to source credentials and secrets from\") <nl> fs.BoolVarP(&opts.Verbose, globalVerboseFlagName, \"v\", false, \"verbose\") <nl> fs.BoolVarP(&opts.Debug, globalDebugFlagName, \"d\", false, \"debug\") <nl> ", "msg": "pkg/cmd: Fix typo in help text of flag --tfjson\nThis commit also improves the grammatical structure of the help text."}
{"diff_id": 2106, "repo": "kubermatic/kubeone", "sha": "88c89e08306fe9b0a43e7013025c3478998fe058", "time": "24.02.2020 14:03:45", "diff": "mmm a / pkg/apis/kubeone/scheme/scheme.go <nl> ppp b / pkg/apis/kubeone/scheme/scheme.go <nl>@@ -31,7 +31,7 @@ import ( <nl> var Scheme = runtime.NewScheme() <nl> // Codecs is a CodecFactory object used to provide encoding and decoding for the scheme <nl> -var Codecs = serializer.NewCodecFactory(Scheme) <nl> +var Codecs = serializer.NewCodecFactory(Scheme, serializer.EnableStrict) <nl> func init() { <nl> metav1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: \"v1\"}) <nl> ", "msg": "Enable Strict mode to decode KubeOne API"}
{"diff_id": 2115, "repo": "kubermatic/kubeone", "sha": "149aa5e6748b2d4c299c426781946233dfa059e6", "time": "14.07.2020 15:32:00", "diff": "mmm a / pkg/cmd/apply.go <nl> ppp b / pkg/cmd/apply.go <nl>@@ -177,9 +177,6 @@ func runApply(opts *applyOpts) error { <nl> } <nl> } <nl> - fmt.Println(\"The following actions will be taken: \") <nl> - fmt.Println(\"Run with --verbose flag for more information.\") <nl> - <nl> // Reconcile the cluster based on the probe status <nl> if !s.LiveCluster.IsProvisioned() { <nl> return runApplyInstall(s, opts) <nl> @@ -193,15 +190,18 @@ func runApply(opts *applyOpts) error { <nl> s.Logger.Warnf(\"Hosts must be removed in a correct order to preserve the Etcd quorum.\") <nl> s.Logger.Warnf(\"Loss of the Etcd quorum can cause loss of all data!!!\") <nl> - s.Logger.Warnf(\"After removing recommended hosts, run 'kubeone apply' before removing any other host.\") <nl> - s.Logger.Warnf(\"The recommended removal order:\") <nl> + s.Logger.Warnf(\"After removing the recommended hosts, run 'kubeone apply' before removing any other host.\") <nl> safeToDelete := s.LiveCluster.SafeToDeleteHosts() <nl> + if len(safeToDelete) > 0 { <nl> + s.Logger.Warnf(\"The recommended removal order:\") <nl> for _, safe := range safeToDelete { <nl> s.Logger.Warnf(\"- %q\", safe) <nl> } <nl> + } else { <nl> + s.Logger.Warnf(\"No other broken node can be removed without losing quorum.\") <nl> + } <nl> } <nl> - // TODO: Should we return at the beginning after install? <nl> for _, node := range s.LiveCluster.ControlPlane { <nl> if !node.IsInCluster { <nl> return runApplyInstall(s, opts) <nl> @@ -218,6 +218,9 @@ func runApply(opts *applyOpts) error { <nl> } <nl> func runApplyInstall(s *state.State, opts *applyOpts) error { // Print the expected changes <nl> + fmt.Println(\"The following actions will be taken: \") <nl> + fmt.Println(\"Run with --verbose flag for more information.\") <nl> + <nl> for _, node := range s.LiveCluster.ControlPlane { <nl> if !node.IsInCluster { <nl> if node.Config.IsLeader { <nl> @@ -262,6 +265,9 @@ func runApplyInstall(s *state.State, opts *applyOpts) error { // Print the expec <nl> } <nl> func runApplyUpgradeIfNeeded(s *state.State, opts *applyOpts) error { <nl> + fmt.Println(\"The following actions will be taken: \") <nl> + fmt.Println(\"Run with --verbose flag for more information.\") <nl> + <nl> upgradeNeeded, err := s.LiveCluster.UpgradeNeeded() <nl> if err != nil { <nl> s.Logger.Errorf(\"Upgrade not allowed: %v\\n\", err) <nl> ", "msg": "Improve apply repair output"}
{"diff_id": 2129, "repo": "kubermatic/kubeone", "sha": "1cdfac1c63ade2b1a34731e64f835fdcf9fed765", "time": "28.09.2020 14:15:50", "diff": "mmm a / pkg/cmd/apply.go <nl> ppp b / pkg/cmd/apply.go <nl>@@ -89,7 +89,8 @@ func applyCmd(rootFlags *pflag.FlagSet) *cobra.Command { <nl> Use: \"apply\", <nl> Short: \"Reconcile the cluster\", <nl> Long: ` <nl> -Reconcile (Install/Upgrade/Repair/Restore) Kubernetes cluster on pre-existing machines <nl> +Reconcile (Install/Upgrade/Repair/Restore) Kubernetes cluster on pre-existing machines. MachineDeployments get <nl> +initialized but won't get modified by default, see '--upgrade-machine-deployments'. <nl> This command takes KubeOne manifest which contains information about hosts and how the cluster should be provisioned. <nl> It's possible to source information about hosts from Terraform output, using the '--tfjson' flag. <nl> ", "msg": "add info for apply, that machine deployments won't get modified"}
{"diff_id": 2134, "repo": "kubermatic/kubeone", "sha": "b07f49a84379ae3d620f127fe52c7adc1b6104e8", "time": "07.12.2020 18:08:38", "diff": "mmm a / pkg/cmd/proxy.go <nl> ppp b / pkg/cmd/proxy.go <nl>@@ -26,6 +26,7 @@ import ( <nl> \"github.com/spf13/cobra\" <nl> \"github.com/spf13/pflag\" <nl> + \"k8c.io/kubeone/pkg/ssh\" <nl> \"k8c.io/kubeone/pkg/state\" <nl> ) <nl> @@ -70,6 +71,13 @@ func setupProxyTunnel(opts *proxyOpts) error { <nl> return err <nl> } <nl> + // Check if we can authenticate via ssh <nl> + tunn, err := s.Connector.Tunnel(s.Cluster.RandomHost()) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + defer tunn.Close() <nl> + <nl> server := &http.Server{ <nl> Addr: opts.ListenAddr, <nl> Handler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { <nl> @@ -78,7 +86,7 @@ func setupProxyTunnel(opts *proxyOpts) error { <nl> return <nl> } <nl> - if err := handleTunneling(w, r, s); err != nil { <nl> + if terr := handleTunneling(w, r, s, tunn); terr != nil { <nl> code := http.StatusInternalServerError <nl> if err1, ok := err.(*httpError); ok { <nl> code = err1.code <nl> @@ -90,6 +98,7 @@ func setupProxyTunnel(opts *proxyOpts) error { <nl> fmt.Println(\"SSH tunnel started, please open another terminal and setup environment\") <nl> fmt.Printf(\"export HTTPS_PROXY=http://%s\\n\", opts.ListenAddr) <nl> + <nl> return server.ListenAndServe() <nl> } <nl> @@ -102,12 +111,7 @@ func (e *httpError) Error() string { <nl> return fmt.Sprintf(\"error: %s, code: %d\", e.err, e.code) <nl> } <nl> -func handleTunneling(w http.ResponseWriter, r *http.Request, s *state.State) error { <nl> - tunn, err := s.Connector.Tunnel(s.Cluster.RandomHost()) <nl> - if err != nil { <nl> - return &httpError{err: err, code: http.StatusServiceUnavailable} <nl> - } <nl> - <nl> +func handleTunneling(w http.ResponseWriter, r *http.Request, s *state.State, tunn ssh.Tunneler) error { <nl> destConn, err := tunn.TunnelTo(s.Context, \"tcp4\", r.Host) <nl> if err != nil { <nl> tunn.Close() <nl> ", "msg": "use a single shared ssh connection for proxying individual tcp connections\nthis also has the advantage of failing early when ssh authentication fails\ndon't shadow outer err when tunneling a connection\nincorporate reviewer suggestion"}
{"diff_id": 2144, "repo": "kubermatic/kubeone", "sha": "78211863023b80cb2c2583d5b67e525d25f36f16", "time": "09.02.2022 17:57:06", "diff": "mmm a / pkg/addons/ensure.go <nl> ppp b / pkg/addons/ensure.go <nl>@@ -113,6 +113,8 @@ func collectAddons(s *state.State) (addonsToDeploy []addonAction) { <nl> }) <nl> } <nl> + k8sVersion := semver.MustParse(s.Cluster.Versions.Kubernetes) <nl> + <nl> // Special case for AWS <nl> if s.Cluster.CloudProvider.AWS != nil { <nl> if s.Cluster.CloudProvider.External { <nl> @@ -124,12 +126,10 @@ func collectAddons(s *state.State) (addonsToDeploy []addonAction) { <nl> name: resources.AddonCSIAwsEBS, <nl> }, <nl> ) <nl> - } else { <nl> - // since k8s 1.23 CSIMigrationAWS is turn by default and require installation of AWS EBS CSI driver even if <nl> + } else if greaterThan23.Check(k8sVersion) { <nl> + // since k8s 1.23 CSIMigrationAWS is turned on by default and requires installation of AWS EBS CSI driver even if <nl> // we don't use external ccm (https://github.com/kubernetes/kubernetes/pull/106098) <nl> - k8sVersion := semver.MustParse(s.Cluster.Versions.Kubernetes) <nl> - if greaterThan23.Check(k8sVersion) { <nl> addonsToDeploy = append(addonsToDeploy, <nl> addonAction{ <nl> name: resources.AddonCSIAwsEBS, <nl> @@ -137,14 +137,10 @@ func collectAddons(s *state.State) (addonsToDeploy []addonAction) { <nl> ) <nl> } <nl> } <nl> - } <nl> - <nl> - if !s.Cluster.CloudProvider.External { <nl> - return <nl> - } <nl> - switch { <nl> - case s.Cluster.CloudProvider.Azure != nil: <nl> + // Special case for AZURE <nl> + if s.Cluster.CloudProvider.Azure != nil { <nl> + if s.Cluster.CloudProvider.External { <nl> addonsToDeploy = append(addonsToDeploy, <nl> addonAction{ <nl> name: resources.AddonCCMAzure, <nl> @@ -156,6 +152,27 @@ func collectAddons(s *state.State) (addonsToDeploy []addonAction) { <nl> name: resources.AddonCSIAzureFile, <nl> }, <nl> ) <nl> + } else if greaterThan23.Check(k8sVersion) { <nl> + // since k8s 1.23 CSIMigrationAzureDisk is turned on by default and requires installation of AzureDisk CSI driver <nl> + // even if we don't use external ccm <nl> + // note: in 1.23 CSIMigrationAzureFile is still turn off by default <nl> + <nl> + addonsToDeploy = append(addonsToDeploy, <nl> + addonAction{ <nl> + name: resources.AddonCSIAzureDisk, <nl> + }, <nl> + addonAction{ <nl> + name: resources.AddonCSIAzureFile, <nl> + }, <nl> + ) <nl> + } <nl> + } <nl> + <nl> + if !s.Cluster.CloudProvider.External { <nl> + return <nl> + } <nl> + <nl> + switch { <nl> case s.Cluster.CloudProvider.DigitalOcean != nil: <nl> addonsToDeploy = append(addonsToDeploy, <nl> addonAction{ <nl> ", "msg": "Azure: install AzureDisk CSI driver if k8s >=1.23 even if cloud.external=false\nsince k8s 1.23 CSIMigrationAzureDisk is turn by default and require installation of AzureDisk CSI driver even if we don't use external ccm\nnote: in 1.23 CSIMigrationAzureFile is still turn off by default"}
{"diff_id": 2148, "repo": "kubermatic/kubeone", "sha": "1974e340efcb1943d39e27a306fa5abc2ff7eb03", "time": "03.05.2022 18:46:44", "diff": "mmm a / pkg/terraform/v1beta2/config.go <nl> ppp b / pkg/terraform/v1beta2/config.go <nl>@@ -178,6 +178,7 @@ func NewConfigFromJSON(buf []byte) (*Config, error) { <nl> KubeoneAPI interface{} `json:\"kubeone_api\"` <nl> KubeoneHosts interface{} `json:\"kubeone_hosts\"` <nl> KubeoneWorkers interface{} `json:\"kubeone_workers\"` <nl> + KubeoneStaticWorkers interface{} `json:\"kubeone_static_workers\"` <nl> }{} <nl> // cat off all the excessive fields from the terraform output JSON that will prevent otherwise strict unmarshalling <nl> ", "msg": "Fixed missing reading of the static workers defined in terraform"}
{"diff_id": 2160, "repo": "docker/app", "sha": "836c959f832c5e861abe9e2b73b4a7e8d0d06eb3", "time": "10.04.2018 10:37:02", "diff": "mmm a / packager/render.go <nl> ppp b / packager/render.go <nl>@@ -45,7 +45,7 @@ func merge(res map[string]interface{}, src map[interface{}]interface{}) { <nl> } <nl> merge(res[kk].(map[string]interface{}), vv) <nl> default: <nl> - res[kk] = fmt.Sprintf(\"%v\", v) <nl> + res[kk] = vv <nl> } <nl> } <nl> } <nl> @@ -103,7 +103,12 @@ func Render(appname string, composeFiles []string, settingsFile []string, env ma <nl> val[s] = make(map[interface{}]interface{}) <nl> val = val[s].(map[interface{}]interface{}) <nl> } <nl> - val[ss[len(ss)-1]] = v <nl> + var converted interface{} <nl> + err = yaml.Unmarshal([]byte(v), &converted) <nl> + if err != nil { <nl> + return \"\", err <nl> + } <nl> + val[ss[len(ss)-1]] = converted <nl> merge(settings, valroot) <nl> } <nl> // flatten settings for variable expension <nl> @@ -131,7 +136,8 @@ func Render(appname string, composeFiles []string, settingsFile []string, env ma <nl> } <nl> configFiles = append(configFiles, composetypes.ConfigFile{Config: parsed}) <nl> } <nl> - fmt.Printf(\"ENV: %v\\n\", finalEnv) <nl> + //fmt.Printf(\"ENV: %v\\n\", finalEnv) <nl> + //fmt.Printf(\"MAPENV: %#v\\n\", settings) <nl> rendered, err := loader.Load(composetypes.ConfigDetails { <nl> WorkingDir: \".\", <nl> ConfigFiles: configFiles, <nl> ", "msg": "render: fix value typing."}
{"diff_id": 2174, "repo": "docker/app", "sha": "f25c7b29cb1c42d832f5c4f066d0c2c05cf6f73e", "time": "28.04.2018 13:03:08", "diff": "mmm a / e2e/binary_test.go <nl> ppp b / e2e/binary_test.go <nl>@@ -25,28 +25,38 @@ func getBinary(t *testing.T) string { <nl> if dockerApp != \"\" { <nl> return dockerApp <nl> } <nl> - dockerApp = os.Getenv(\"DOCKERAPP_BINARY\") <nl> - if dockerApp == \"\" { <nl> - binName := \"docker-app\" <nl> - if runtime.GOOS == \"windows\" { <nl> - binName += \".exe\" <nl> + binName := findBinary() <nl> + if binName == \"\" { <nl> + t.Error(\"cannot locate docker-app binary\") <nl> } <nl> - locations := []string{\".\", \"../_build\"} <nl> - for _, l := range locations { <nl> - b := path.Join(l, binName) <nl> - if _, err := os.Stat(b); err == nil { <nl> - dockerApp = b <nl> - break <nl> + cmd := exec.Command(binName, \"version\") <nl> + err := cmd.Run() <nl> + assert.NilError(t, err, \"failed to execute %s\", binName) <nl> + dockerApp = binName <nl> + return dockerApp <nl> } <nl> + <nl> +func findBinary() string { <nl> + binNames := []string{ <nl> + os.Getenv(\"DOCKERAPP_BINARY\"), <nl> + \"./docker-app-\" + runtime.GOOS + binExt(), <nl> + \"./docker-app\" + binExt(), <nl> + \"../_build/docker-app-\" + runtime.GOOS + binExt(), <nl> + \"../_build/docker_app\" + binExt(), <nl> } <nl> + for _, binName := range binNames { <nl> + if _, err := os.Stat(binName); err == nil { <nl> + return binName <nl> } <nl> - if dockerApp == \"\" { <nl> - t.Error(\"cannot locate docker-app binary\") <nl> } <nl> - cmd := exec.Command(dockerApp, \"version\") <nl> - _, err := cmd.CombinedOutput() <nl> - assert.NilError(t, err, \"failed to execute docker-app binary\") <nl> - return dockerApp <nl> + return \"\" <nl> +} <nl> + <nl> +func binExt() string { <nl> + if runtime.GOOS == \"windows\" { <nl> + return \".exe\" <nl> + } <nl> + return \"\" <nl> } <nl> func TestRenderBinary(t *testing.T) { <nl> ", "msg": "Fix docker-app search for e2e on CI\nThe executable name has the platform as suffix on CI."}
{"diff_id": 2179, "repo": "docker/app", "sha": "d0940506931737a33d8f423f2d6f9dc591751f85", "time": "30.04.2018 10:56:41", "diff": "mmm a / renderer/render.go <nl> ppp b / renderer/render.go <nl>@@ -80,6 +80,7 @@ func applyRenderers(data []byte, renderers []string, settings map[string]interfa <nl> if err != nil { <nl> return nil, err <nl> } <nl> + tmpl.Option(\"missingkey=error\") <nl> yaml := bytes.NewBuffer(nil) <nl> err = tmpl.Execute(yaml, settings) <nl> if err != nil { <nl> ", "msg": "renderer: set gotemplate to err on missing variable."}
{"diff_id": 2205, "repo": "docker/app", "sha": "b4cde150faaa1c3bea1e1825c7c58323adca316c", "time": "29.06.2018 16:06:12", "diff": "mmm a / internal/packager/extract.go <nl> ppp b / internal/packager/extract.go <nl>@@ -118,7 +118,7 @@ func ExtractWithOrigin(appname string) (ExtractedApp, error) { <nl> } <nl> } <nl> originalAppname := appname <nl> - <nl> + appname = filepath.Clean(appname) <nl> // try appending our extension <nl> appname = internal.DirNameFromAppName(appname) <nl> s, err := os.Stat(appname) <nl> ", "msg": "Extract: clean path, mainly to remove eventual trailing separator."}
{"diff_id": 2229, "repo": "docker/app", "sha": "d286fb3da704433006259e7837468b1c85e1ea6c", "time": "25.09.2018 15:10:07", "diff": "mmm a / e2e/commands_test.go <nl> ppp b / e2e/commands_test.go <nl>@@ -261,8 +261,8 @@ func TestSplitMerge(t *testing.T) { <nl> result = icmd.RunCommand(dockerApp, \"inspect\", \"remerged\").Assert(t, icmd.Success) <nl> assert.Assert(t, golden.String(result.Combined(), \"envvariables-inspect.golden\")) <nl> // test inplace <nl> - icmd.RunCommand(dockerApp, \"merge\", \"split\") <nl> - icmd.RunCommand(dockerApp, \"split\", \"split\") <nl> + icmd.RunCommand(dockerApp, \"merge\", \"split\").Assert(t, icmd.Success) <nl> + icmd.RunCommand(dockerApp, \"split\", \"split\").Assert(t, icmd.Success) <nl> } <nl> func TestURL(t *testing.T) { <nl> ", "msg": "Add missing asserts to split/merge test (missed off as part of a previous refactor)"}
{"diff_id": 2250, "repo": "docker/app", "sha": "2e4b6d88f20db13fe73756e439f1107210b2e670", "time": "05.04.2019 15:56:12", "diff": "mmm a / internal/packager/packing.go <nl> ppp b / internal/packager/packing.go <nl>@@ -14,10 +14,15 @@ import ( <nl> \"github.com/pkg/errors\" <nl> ) <nl> -var dockerFile = `FROM docker/cnab-app-base:` + internal.Version + ` <nl> -COPY . .` <nl> +const ( <nl> + // CNABBaseImageName is the name of the base invocation image. <nl> + CNABBaseImageName = \"docker/cnab-app-base\" <nl> + <nl> + dockerIgnore = \"Dockerfile\" <nl> +) <nl> -const dockerIgnore = \"Dockerfile\" <nl> +var dockerFile = `FROM ` + CNABBaseImageName + `:` + internal.Version + ` <nl> +COPY . .` <nl> func tarAdd(tarout *tar.Writer, path, file string) error { <nl> payload, err := ioutil.ReadFile(file) <nl> ", "msg": "Use const for base invocation image name"}
{"diff_id": 2252, "repo": "docker/app", "sha": "46de296ba798571c0201d68c8bf5fecd3b1d972d", "time": "16.04.2019 15:05:41", "diff": "mmm a / internal/commands/validate.go <nl> ppp b / internal/commands/validate.go <nl>@@ -9,30 +9,29 @@ import ( <nl> \"github.com/spf13/cobra\" <nl> ) <nl> -var ( <nl> - validateParametersFile []string <nl> - validateEnv []string <nl> -) <nl> +type validateOptions struct { <nl> + parametersOptions <nl> +} <nl> func validateCmd() *cobra.Command { <nl> + var opts validateOptions <nl> cmd := &cobra.Command{ <nl> Use: \"validate [<app-name>] [-s key=value...] [-f parameters-file...]\", <nl> Short: \"Checks the rendered application is syntactically correct\", <nl> Args: cli.RequiresMaxArgs(1), <nl> RunE: func(cmd *cobra.Command, args []string) error { <nl> app, err := packager.Extract(firstOrEmpty(args), <nl> - types.WithParametersFiles(validateParametersFile...), <nl> + types.WithParametersFiles(opts.parametersFiles...), <nl> ) <nl> if err != nil { <nl> return err <nl> } <nl> defer app.Cleanup() <nl> - argParameters := cliopts.ConvertKVStringsToMap(validateEnv) <nl> + argParameters := cliopts.ConvertKVStringsToMap(opts.overrides) <nl> _, err = render.Render(app, argParameters, nil) <nl> return err <nl> }, <nl> } <nl> - cmd.Flags().StringArrayVarP(&validateParametersFile, \"parameters-files\", \"f\", []string{}, \"Override with parameters from files\") <nl> - cmd.Flags().StringArrayVarP(&validateEnv, \"set\", \"s\", []string{}, \"Override parameters values\") <nl> + opts.parametersOptions.addFlags(cmd.Flags()) <nl> return cmd <nl> } <nl> ", "msg": "Use parametersOptions for validate command"}
{"diff_id": 2254, "repo": "docker/app", "sha": "9140af045c54c78d1baa187b79aa5aad19081372", "time": "10.04.2019 11:46:30", "diff": "mmm a / types/parameters/merge_test.go <nl> ppp b / types/parameters/merge_test.go <nl>package parameters <nl> import ( <nl> + \"fmt\" <nl> \"testing\" <nl> \"gotest.tools/assert\" <nl> @@ -42,12 +43,15 @@ func TestMerge(t *testing.T) { <nl> assert.NilError(t, err) <nl> parameters, err := Merge(m1, m2, m3) <nl> assert.NilError(t, err) <nl> + fmt.Println(parameters) <nl> assert.Check(t, is.DeepEqual(parameters.Flatten(), map[string]string{ <nl> \"foo\": \"bar\", <nl> \"bar.baz\": \"boz\", <nl> \"bar.port\": \"10\", <nl> \"bar.foo\": \"toto\", <nl> - \"baz.0\": \"c\", <nl> + \"baz.0\": \"a\", <nl> + \"baz.1\": \"b\", <nl> + \"baz.2\": \"c\", <nl> \"banana\": \"monkey\", <nl> })) <nl> } <nl> ", "msg": "Update unit test since mergo 0.3.7 changed slice merge order (https://github.com/imdario/mergo/pull/102)"}
{"diff_id": 2259, "repo": "docker/app", "sha": "96d4d9bec487e747ef3a55ba8b30fba4c9039d27", "time": "30.04.2019 16:25:35", "diff": "mmm a / types/metadata/metadata.go <nl> ppp b / types/metadata/metadata.go <nl>@@ -2,6 +2,8 @@ package metadata <nl> import ( <nl> \"strings\" <nl> + <nl> + \"github.com/deislabs/cnab-go/bundle\" <nl> ) <nl> // Maintainer represents one of the apps's maintainers <nl> @@ -38,3 +40,19 @@ type AppMetadata struct { <nl> Description string `json:\"description,omitempty\"` <nl> Maintainers Maintainers `json:\"maintainers,omitempty\"` <nl> } <nl> + <nl> +// Metadata extracts the docker-app metadata from the bundle <nl> +func FromBundle(bndl *bundle.Bundle) AppMetadata { <nl> + meta := AppMetadata{ <nl> + Name: bndl.Name, <nl> + Version: bndl.Version, <nl> + Description: bndl.Description, <nl> + } <nl> + for _, m := range bndl.Maintainers { <nl> + meta.Maintainers = append(meta.Maintainers, Maintainer{ <nl> + Name: m.Name, <nl> + Email: m.Email, <nl> + }) <nl> + } <nl> + return meta <nl> +} <nl> ", "msg": "metadata: Add helper to extract docker-app metadata from a bundle."}
{"diff_id": 2265, "repo": "docker/app", "sha": "d2f1fce9a63fd54cafb8c1ade3db05930395c2c8", "time": "11.06.2019 18:07:44", "diff": "mmm a / internal/commands/cnab.go <nl> ppp b / internal/commands/cnab.go <nl>@@ -54,6 +54,9 @@ func addNamedCredentialSets(credStore appstore.CredentialStore, namedCredentials <nl> c, err = credentials.Load(file) <nl> } else { <nl> c, err = credStore.Read(file) <nl> + if os.IsNotExist(err) { <nl> + err = e <nl> + } <nl> } <nl> if err != nil { <nl> return err <nl> ", "msg": "Print the original error message, the one checking the file locally, instead of the one looking for the credential set in the credential store."}
{"diff_id": 2273, "repo": "docker/app", "sha": "c85f3d0186451ccdbbcce5134f8d943ad298e3dd", "time": "24.09.2019 14:57:59", "diff": "mmm a / internal/packager/init.go <nl> ppp b / internal/packager/init.go <nl>@@ -45,11 +45,6 @@ func Init(name string, composeFile string) (string, error) { <nl> return \"\", err <nl> } <nl> - if composeFile == \"\" { <nl> - if _, err := os.Stat(internal.ComposeFileName); err == nil { <nl> - composeFile = internal.ComposeFileName <nl> - } <nl> - } <nl> if composeFile == \"\" { <nl> err = initFromScratch(name) <nl> } else { <nl> ", "msg": "Do not search implicitly for a compose file\nThe user needs to explicitly give the path to the docker-compose file\nthat they want to base the app on."}
{"diff_id": 2277, "repo": "docker/app", "sha": "eeb3270e10ed28439f55eccfae1939ade3bc6a62", "time": "02.10.2019 18:01:56", "diff": "mmm a / e2e/images_test.go <nl> ppp b / e2e/images_test.go <nl>@@ -99,6 +99,11 @@ func TestImageTag(t *testing.T) { <nl> dir := fs.NewDir(t, \"\") <nl> defer dir.Remove() <nl> + dockerAppImageTag := func(args ...string) { <nl> + cmdArgs := append([]string{\"app\", \"image\", \"tag\"}, args...) <nl> + cmd.Command = dockerCli.Command(cmdArgs...) <nl> + } <nl> + <nl> // given a first available image <nl> cmd.Command = dockerCli.Command(\"app\", \"bundle\", filepath.Join(\"testdata\", \"simple\", \"simple.dockerapp\"), \"--tag\", \"a-simple-app\", \"--output\", dir.Join(\"simple-bundle.json\")) <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> @@ -109,35 +114,35 @@ a-simple-app:latest simple <nl> expectImageListOutput(t, cmd, singleImageExpectation) <nl> // with no argument <nl> - cmd.Command = dockerCli.Command(\"app\", \"bundle\", \"tag\") <nl> + dockerAppImageTag() <nl> icmd.RunCmd(cmd).Assert(t, icmd.Expected{ <nl> ExitCode: 1, <nl> Err: `\"docker app image tag\" requires exactly 2 arguments.`, <nl> }) <nl> // with one argument <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app\") <nl> + dockerAppImageTag(\"a-simple-app\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Expected{ <nl> ExitCode: 1, <nl> Err: `\"docker app image tag\" requires exactly 2 arguments.`, <nl> }) <nl> // with invalid src reference <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app$2\", \"b-simple-app\") <nl> + dockerAppImageTag(\"a-simple-app$2\", \"b-simple-app\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Expected{ <nl> ExitCode: 1, <nl> Err: `could not parse 'a-simple-app$2' as a valid reference: invalid reference format`, <nl> }) <nl> // with invalid target reference <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app\", \"b@simple-app\") <nl> + dockerAppImageTag(\"a-simple-app\", \"b@simple-app\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Expected{ <nl> ExitCode: 1, <nl> Err: `could not parse 'b@simple-app' as a valid reference: invalid reference format`, <nl> }) <nl> // tag image with only names <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app\", \"b-simple-app\") <nl> + dockerAppImageTag(\"a-simple-app\", \"b-simple-app\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> expectImageListOutput(t, cmd, `APP IMAGE APP NAME <nl> a-simple-app:latest simple <nl> @@ -145,7 +150,7 @@ b-simple-app:latest simple <nl> `) <nl> // target tag <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app\", \"a-simple-app:0.1\") <nl> + dockerAppImageTag(\"a-simple-app\", \"a-simple-app:0.1\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> expectImageListOutput(t, cmd, `APP IMAGE APP NAME <nl> a-simple-app:0.1 simple <nl> @@ -154,7 +159,7 @@ b-simple-app:latest simple <nl> `) <nl> // source tag <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app:0.1\", \"c-simple-app\") <nl> + dockerAppImageTag(\"a-simple-app:0.1\", \"c-simple-app\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> expectImageListOutput(t, cmd, `APP IMAGE APP NAME <nl> a-simple-app:0.1 simple <nl> @@ -164,7 +169,7 @@ c-simple-app:latest simple <nl> `) <nl> // source and target tags <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"a-simple-app:0.1\", \"b-simple-app:0.2\") <nl> + dockerAppImageTag(\"a-simple-app:0.1\", \"b-simple-app:0.2\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> expectImageListOutput(t, cmd, `APP IMAGE APP NAME <nl> a-simple-app:0.1 simple <nl> @@ -187,7 +192,7 @@ push-pull:latest push-pull <nl> `) <nl> // can be tagged to an existing tag <nl> - cmd.Command = dockerCli.Command(\"app\", \"image\", \"tag\", \"push-pull\", \"b-simple-app:0.2\") <nl> + dockerAppImageTag(\"push-pull\", \"b-simple-app:0.2\") <nl> icmd.RunCmd(cmd).Assert(t, icmd.Success) <nl> expectImageListOutput(t, cmd, `APP IMAGE APP NAME <nl> a-simple-app:0.1 simple <nl> ", "msg": "refactor: extract `docker app image tag` command\nThis allows to remove duplication and avoid to write the wrong command."}
{"diff_id": 2279, "repo": "docker/app", "sha": "631171a1a348e2194c1af2c58dd2dfd3555a382c", "time": "03.10.2019 15:56:50", "diff": "mmm a / internal/commands/image/rm.go <nl> ppp b / internal/commands/image/rm.go <nl>@@ -14,8 +14,9 @@ import ( <nl> func rmCmd() *cobra.Command { <nl> return &cobra.Command{ <nl> - Use: \"rm [APP_IMAGE] [APP_IMAGE...]\", <nl> Short: \"Remove an application image\", <nl> + Use: \"rm [APP_IMAGE] [APP_IMAGE...]\", <nl> + Aliases: []string{\"remove\"}, <nl> Args: cli.RequiresMinArgs(1), <nl> Example: `$ docker app image rm myapp <nl> $ docker app image rm myapp:1.0.0 <nl> ", "msg": "Add \"remove\" as alias for app image rm"}
{"diff_id": 2280, "repo": "docker/app", "sha": "67f946e5f7fd0333ec40ffcb6170f92bea6ade87", "time": "25.09.2019 17:19:05", "diff": "mmm a / internal/commands/build.go <nl> ppp b / internal/commands/build.go <nl>@@ -73,7 +73,7 @@ func runBuild(dockerCli command.Cli, application string, opt buildOptions) error <nl> return err <nl> } <nl> - for k, t := range targets { <nl> + for service, t := range targets { <nl> if strings.HasPrefix(*t.Context, \".\") { <nl> // Relative path in compose file under x.dockerapp refers to parent folder <nl> // FIXME docker app init should maybe udate them ? <nl> @@ -82,8 +82,8 @@ func runBuild(dockerCli command.Cli, application string, opt buildOptions) error <nl> return err <nl> } <nl> t.Context = &path <nl> - t.Tags = []string{fmt.Sprintf(\"%s:%s\", bundle.Name, bundle.Version)} <nl> - targets[k] = t <nl> + t.Tags = []string{fmt.Sprintf(\"%s:%s-%s\", bundle.Name, bundle.Version, service)} <nl> + targets[service] = t <nl> } <nl> } <nl> @@ -117,21 +117,22 @@ func runBuild(dockerCli command.Cli, application string, opt buildOptions) error <nl> // FIXME add invocation image as another build target <nl> pw := progress.NewPrinter(ctx2, os.Stderr, opt.progress) <nl> - resp, err := build.Build(ctx2, driverInfo, buildopts, dockerAPI(dockerCli), dockerCli.ConfigFile(), pw) <nl> + _, err = build.Build(ctx2, driverInfo, buildopts, dockerAPI(dockerCli), dockerCli.ConfigFile(), pw) <nl> if err != nil { <nl> return err <nl> } <nl> fmt.Println(\"Successfully built service images\") <nl> - for k, r := range resp { <nl> + <nl> + /* FIXME Build should tell us everything we need to know about digests https://github.com/docker/buildx/issues/149 <nl> + for service, r := range resp { <nl> digest := r.ExporterResponse[\"containerimage.digest\"] <nl> - image := bundle.Images[k] <nl> + image := bundle.Images[service] <nl> image.ImageType = cnab.ImageTypeDocker <nl> - image.Digest = digest <nl> - bundle.Images[k] = image <nl> - fmt.Printf(\" - %s : %s\\n\", k, image.Digest) <nl> + image.Image = fmt.Sprintf(\"%s@%s\", bundle.Name, digest) <nl> + bundle.Images[service] = image <nl> + fmt.Printf(\" - %s : %s\\n\", service, image.Digest) <nl> } <nl> - <nl> // -- debug <nl> dt, err = json.MarshalIndent(resp, \"\", \" \") <nl> if err != nil { <nl> @@ -139,6 +140,22 @@ func runBuild(dockerCli command.Cli, application string, opt buildOptions) error <nl> } <nl> fmt.Fprintln(dockerCli.Out(), string(dt)) <nl> // -- debug <nl> + */ <nl> + // FIXME as a workaround, inspect image we've just built to get digest <nl> + for service, _ := range targets { <nl> + ref := fmt.Sprintf(\"%s:%s-%s\", bundle.Name, bundle.Version, service) <nl> + inspect, _, err := dockerCli.Client().ImageInspectWithRaw(ctx, ref) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + image := bundle.Images[service] <nl> + image.ImageType = cnab.ImageTypeDocker <nl> + image.Image = fmt.Sprintf(\"%s:%s-%s\", bundle.Name, bundle.Version, service) <nl> + image.Digest = inspect.ID <nl> + bundle.Images[service] = image <nl> + fmt.Printf(\" - %s : %s:%s-%s (%s)\\n\", service, bundle.Name, bundle.Version, service, inspect.ID) <nl> + } <nl> + <nl> if opt.tag == \"\" { <nl> opt.tag = bundle.Name + \":\" + bundle.Version <nl> ", "msg": "Workaround buildx#149 to get built images digests"}
{"diff_id": 2286, "repo": "docker/app", "sha": "c9b862782a67bf57e8af763759b6bb5147114451", "time": "14.10.2019 11:16:56", "diff": "mmm a / internal/commands/init.go <nl> ppp b / internal/commands/init.go <nl>@@ -17,7 +17,7 @@ func initCmd(dockerCli command.Cli) *cobra.Command { <nl> cmd := &cobra.Command{ <nl> Use: \"init APP_NAME [--compose-file COMPOSE_FILE] [OPTIONS]\", <nl> Short: \"Initialize Docker Application definition\", <nl> - Long: `Start building a Docker Application package. If there is a docker-compose.yml file in the current directory it will be copied and used.`, <nl> + Long: `Start building a Docker Application package.`, <nl> Example: `$ docker app init myapp`, <nl> Args: cli.ExactArgs(1), <nl> RunE: func(cmd *cobra.Command, args []string) error { <nl> ", "msg": "Fix init help message.\nDocker App doesn't take a docker-compose file implicitly any more."}
{"diff_id": 2300, "repo": "docker/app", "sha": "42218d2f214f26177b64d872dfe7e45224669fb1", "time": "28.10.2019 08:00:47", "diff": "mmm a / internal/commands/image/inspect.go <nl> ppp b / internal/commands/image/inspect.go <nl>@@ -87,7 +87,7 @@ func runInspect(dockerCli command.Cli, appname string, opts inspectOptions) erro <nl> installation.SetParameter(internal.ParameterInspectFormatName, format) <nl> - if err := a.Run(&installation.Claim, nil, nil); err != nil { <nl> + if err := a.Run(&installation.Claim, nil); err != nil { <nl> return fmt.Errorf(\"inspect failed: %s\\n%s\", err, errBuf) <nl> } <nl> return nil <nl> ", "msg": "cnab-go OperationConfigs panic if we pass a nil function"}
{"diff_id": 2304, "repo": "docker/app", "sha": "d83eb35ed1c036a00bf3d56b276cb7040aedbc80", "time": "12.11.2019 15:37:48", "diff": "mmm a / internal/commands/run.go <nl> ppp b / internal/commands/run.go <nl>@@ -73,7 +73,7 @@ func runCmd(dockerCli command.Cli) *cobra.Command { <nl> cmd.Flags().StringArrayVar(&opts.labels, \"label\", nil, \"Label to add to services\") <nl> //nolint:errcheck <nl> - cmd.Flags().SetAnnotation(\"cnab-bundle-json\", \"experimental\", []string{\"true\"}) <nl> + cmd.Flags().SetAnnotation(\"cnab-bundle-json\", \"experimentalCLI\", []string{\"true\"}) <nl> return cmd <nl> } <nl> ", "msg": "Experimental flags are hidden by experimentalCLI\nThe code had the (wrong) annotation \"experimental\""}
{"diff_id": 2311, "repo": "docker/app", "sha": "9b4f291649d6cf1b6377b28683ff3fff1e5b7fee", "time": "13.11.2019 16:55:51", "diff": "mmm a / internal/commands/image/list.go <nl> ppp b / internal/commands/image/list.go <nl>@@ -2,12 +2,16 @@ package image <nl> import ( <nl> \"bytes\" <nl> + \"encoding/json\" <nl> \"fmt\" <nl> \"io\" <nl> \"strings\" <nl> \"text/tabwriter\" <nl> \"time\" <nl> + \"github.com/docker/cli/templates\" <nl> + \"github.com/pkg/errors\" <nl> + <nl> \"github.com/docker/app/internal/packager\" <nl> \"github.com/docker/app/internal/relocated\" <nl> \"github.com/docker/app/internal/store\" <nl> @@ -22,11 +26,12 @@ import ( <nl> type imageListOption struct { <nl> quiet bool <nl> digests bool <nl> + template string <nl> } <nl> type imageListColumn struct { <nl> header string <nl> - value func(p pkg) string <nl> + value func(desc imageDesc) string <nl> } <nl> func listCmd(dockerCli command.Cli) *cobra.Command { <nl> @@ -52,6 +57,8 @@ func listCmd(dockerCli command.Cli) *cobra.Command { <nl> flags := cmd.Flags() <nl> flags.BoolVarP(&options.quiet, \"quiet\", \"q\", false, \"Only show numeric IDs\") <nl> flags.BoolVarP(&options.digests, \"digests\", \"\", false, \"Show image digests\") <nl> + cmd.Flags().StringVarP(&options.template, \"format\", \"f\", \"\", \"Format the output using the given syntax or Go template\") <nl> + cmd.Flags().SetAnnotation(\"format\", \"experimentalCLI\", []string{\"true\"}) //nolint:errcheck <nl> return cmd <nl> } <nl> @@ -94,10 +101,32 @@ func getPackages(bundleStore store.BundleStore, references []reference.Reference <nl> func printImages(dockerCli command.Cli, refs []pkg, options imageListOption) error { <nl> w := tabwriter.NewWriter(dockerCli.Out(), 0, 0, 1, ' ', 0) <nl> + <nl> + list := []imageDesc{} <nl> + for _, ref := range refs { <nl> + list = append(list, getImageDesc(ref)) <nl> + } <nl> + <nl> + if options.template == \"json\" { <nl> + bytes, err := json.MarshalIndent(list, \"\", \" \") <nl> + if err != nil { <nl> + return errors.Errorf(\"Failed to marshall json: %s\", err) <nl> + } <nl> + _, err = dockerCli.Out().Write(bytes) <nl> + return err <nl> + } <nl> + if options.template != \"\" { <nl> + tmpl, err := templates.Parse(options.template) <nl> + if err != nil { <nl> + return errors.Errorf(\"Template parsing error: %s\", err) <nl> + } <nl> + return tmpl.Execute(dockerCli.Out(), list) <nl> + } <nl> + <nl> listColumns := getImageListColumns(options) <nl> printHeaders(w, listColumns) <nl> - for _, ref := range refs { <nl> - printValues(w, ref, listColumns) <nl> + for _, desc := range list { <nl> + printValues(w, desc, listColumns) <nl> } <nl> return w.Flush() <nl> @@ -137,55 +166,87 @@ func printHeaders(w io.Writer, listColumns []imageListColumn) { <nl> fmt.Fprintln(w, strings.Join(headers, \"\\t\")) <nl> } <nl> -func printValues(w io.Writer, ref pkg, listColumns []imageListColumn) { <nl> +func printValues(w io.Writer, desc imageDesc, listColumns []imageListColumn) { <nl> var values []string <nl> for _, column := range listColumns { <nl> - values = append(values, column.value(ref)) <nl> + values = append(values, column.value(desc)) <nl> } <nl> fmt.Fprintln(w, strings.Join(values, \"\\t\")) <nl> } <nl> +type imageDesc struct { <nl> + ID string `json:\"id,omitempty\"` <nl> + Name string `json:\"name,omitempty\"` <nl> + Repository string `json:\"repository,omitempty\"` <nl> + Tag string `json:\"tag,omitempty\"` <nl> + Digest string `json:\"digest,omitempty\"` <nl> + Created time.Duration `json:\"created,omitempty\"` <nl> +} <nl> + <nl> +func getImageDesc(p pkg) imageDesc { <nl> + var id string <nl> + id, _ = getImageID(p) <nl> + var repository string <nl> + if n, ok := p.ref.(reference.Named); ok { <nl> + repository = reference.FamiliarName(n) <nl> + } <nl> + var tag string <nl> + if t, ok := p.ref.(reference.Tagged); ok { <nl> + tag = t.Tag() <nl> + } <nl> + var digest string <nl> + if t, ok := p.ref.(reference.Digested); ok { <nl> + digest = t.Digest().String() <nl> + } <nl> + var created time.Duration <nl> + if payload, err := packager.CustomPayload(p.bundle.Bundle); err == nil { <nl> + if createdPayload, ok := payload.(packager.CustomPayloadCreated); ok { <nl> + created = time.Now().UTC().Sub(createdPayload.CreatedTime()) <nl> + } <nl> + } <nl> + return imageDesc{ <nl> + ID: id, <nl> + Name: p.bundle.Name, <nl> + Repository: repository, <nl> + Tag: tag, <nl> + Digest: digest, <nl> + Created: created, <nl> + } <nl> +} <nl> + <nl> func getImageListColumns(options imageListOption) []imageListColumn { <nl> columns := []imageListColumn{ <nl> - {\"REPOSITORY\", func(p pkg) string { <nl> - if n, ok := p.ref.(reference.Named); ok { <nl> - return reference.FamiliarName(n) <nl> + {\"REPOSITORY\", func(desc imageDesc) string { <nl> + if desc.Repository != \"\" { <nl> + return desc.Repository <nl> } <nl> return \"<none>\" <nl> }}, <nl> - {\"TAG\", func(p pkg) string { <nl> - if t, ok := p.ref.(reference.Tagged); ok { <nl> - return t.Tag() <nl> + {\"TAG\", func(desc imageDesc) string { <nl> + if desc.Tag != \"\" { <nl> + return desc.Tag <nl> } <nl> return \"<none>\" <nl> }}, <nl> } <nl> if options.digests { <nl> - columns = append(columns, imageListColumn{\"DIGEST\", func(p pkg) string { <nl> - if t, ok := p.ref.(reference.Digested); ok { <nl> - return t.Digest().String() <nl> + columns = append(columns, imageListColumn{\"DIGEST\", func(desc imageDesc) string { <nl> + if desc.Digest != \"\" { <nl> + return desc.Digest <nl> } <nl> return \"<none>\" <nl> }}) <nl> } <nl> columns = append(columns, <nl> - imageListColumn{\"APP IMAGE ID\", func(p pkg) string { <nl> - id, err := getImageID(p) <nl> - if err != nil { <nl> - return \"\" <nl> - } <nl> - return id <nl> + imageListColumn{\"APP IMAGE ID\", func(desc imageDesc) string { <nl> + return desc.ID <nl> }}, <nl> - imageListColumn{\"APP NAME\", func(p pkg) string { <nl> - return p.bundle.Name <nl> + imageListColumn{\"APP NAME\", func(desc imageDesc) string { <nl> + return desc.Name <nl> }}, <nl> - imageListColumn{\"CREATED\", func(p pkg) string { <nl> - payload, err := packager.CustomPayload(p.bundle.Bundle) <nl> - if err != nil { <nl> - return \"\" <nl> - } <nl> - if createdPayload, ok := payload.(packager.CustomPayloadCreated); ok { <nl> - return units.HumanDuration(time.Now().UTC().Sub(createdPayload.CreatedTime())) + \" ago\" <nl> + imageListColumn{\"CREATED\", func(desc imageDesc) string { <nl> + if desc.Created > 0 { <nl> + return units.HumanDuration(desc.Created) + \" ago\" <nl> } <nl> return \"\" <nl> }}, <nl> ", "msg": "Introduce --format option for docker app image ls"}
{"diff_id": 2318, "repo": "docker/app", "sha": "d26c9ad7f5a20a161e9d5d7df7d46c06c69ac565", "time": "26.11.2019 17:14:55", "diff": "mmm a / internal/commands/build/build.go <nl> ppp b / internal/commands/build/build.go <nl>@@ -23,6 +23,7 @@ import ( <nl> \"github.com/docker/cli/cli\" <nl> \"github.com/docker/cli/cli/command\" <nl> compose \"github.com/docker/cli/cli/compose/types\" <nl> + \"github.com/docker/cli/cli/streams\" <nl> \"github.com/docker/cnab-to-oci/remotes\" <nl> \"github.com/docker/distribution/reference\" <nl> \"github.com/moby/buildkit/client\" <nl> @@ -75,6 +76,30 @@ func Cmd(dockerCli command.Cli) *cobra.Command { <nl> return cmd <nl> } <nl> +// FIXME: DO NOT SET THIS VARIABLE DIRECTLY! Use `getOutputFile` <nl> +// This global var prevents the file to be garbage collected and by that invalidated <nl> +// A an alternative fix for this would be writing the output to a bytes buffer and flushing to stdout. <nl> +// The impossibility here is that os.File is not an interface that a buffer can implement. <nl> +// Maybe `progress.NewPrinter` should implement an \"os.File-like\" interface just for its needs. <nl> +// See https://github.com/golang/go/issues/14106 <nl> +var _outputFile *os.File <nl> + <nl> +func getOutputFile(realOut *streams.Out, quiet bool) (*os.File, error) { <nl> + if _outputFile != nil { <nl> + return _outputFile, nil <nl> + } <nl> + if quiet { <nl> + var err error <nl> + _outputFile, err = os.Create(os.DevNull) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + return _outputFile, nil <nl> + } <nl> + _outputFile = os.NewFile(realOut.FD(), os.Stdout.Name()) <nl> + return _outputFile, nil <nl> +} <nl> + <nl> func runBuild(dockerCli command.Cli, contextPath string, opt buildOptions) error { <nl> err := checkMinimalEngineVersion(dockerCli) <nl> if err != nil { <nl> @@ -160,14 +185,10 @@ func buildImageUsingBuildx(app *types.App, contextPath string, opt buildOptions, <nl> }, <nl> } <nl> - var out *os.File <nl> - if opt.quiet { <nl> - if out, err = os.Create(os.DevNull); err != nil { <nl> + out, err := getOutputFile(dockerCli.Out(), opt.quiet) <nl> + if err != nil { <nl> return nil, err <nl> } <nl> - } else { <nl> - out = os.NewFile(dockerCli.Out().FD(), \"/dev/stdout\") <nl> - } <nl> pw := progress.NewPrinter(ctx, out, opt.progress) <nl> ", "msg": "Create a global variable to hold output file\nWith a function scoped `os.File`, next time the GC passes\nthe instance is collected, calling the finalizer and triggering\nthe invalidation of the FD, that cannot be used anymore."}
{"diff_id": 2320, "repo": "docker/app", "sha": "3cebbc2d85daa402514eac867fe85f443e9f5e5d", "time": "02.12.2019 14:10:53", "diff": "mmm a / internal/commands/remove.go <nl> ppp b / internal/commands/remove.go <nl>@@ -36,7 +36,7 @@ func removeCmd(dockerCli command.Cli, installerContext *cliopts.InstallerContext <nl> }, <nl> } <nl> opts.credentialOptions.addFlags(cmd.Flags()) <nl> - cmd.Flags().BoolVar(&opts.force, \"force\", false, \"Force the removal of a running App\") <nl> + cmd.Flags().BoolVarP(&opts.force, \"force\", \"f\", false, \"Force the removal of a running App\") <nl> return cmd <nl> } <nl> ", "msg": "Add -f shorthand to app rm --force\nMakes -f the shorthand for --force in `docker app rm`, same as in `docker rm` and `docker app image rm`"}
{"diff_id": 2327, "repo": "openshift/machine-config-operator", "sha": "f3504e41577d18a491e59d556184bdcf6f02ed95", "time": "08.08.2018 14:20:25", "diff": "mmm a / cmd/machine-config-daemon/start.go <nl> ppp b / cmd/machine-config-daemon/start.go <nl>@@ -2,6 +2,7 @@ package main <nl> import ( <nl> \"flag\" <nl> + \"os\" <nl> \"github.com/golang/glog\" <nl> \"github.com/openshift/machine-config-operator/pkg/daemon\" <nl> @@ -33,8 +34,8 @@ func init() { <nl> rootCmd.AddCommand(startCmd) <nl> startCmd.PersistentFlags().StringVar(&startOpts.kubeconfig, \"kubeconfig\", \"\", \"Kubeconfig file to access a remote cluster (testing only)\") <nl> startCmd.PersistentFlags().StringVar(&startOpts.nodeName, \"node-name\", \"\", \"kubernetes node name daemon is managing.\") <nl> - startCmd.PersistentFlags().StringVar(&startOpts.nodeName, \"target-namespace\", \"openshift-machine-config\", \"namespace is where the daemon looks for machineconfigs.\") <nl> - startCmd.PersistentFlags().StringVar(&startOpts.nodeName, \"root-prefix\", \"/rootfs\", \"where the nodes root filesystem is mounted, for the file stage.\") <nl> + startCmd.PersistentFlags().StringVar(&startOpts.targetNamespace, \"target-namespace\", \"openshift-machine-config\", \"namespace is where the daemon looks for machineconfigs.\") <nl> + startCmd.PersistentFlags().StringVar(&startOpts.rootPrefix, \"root-prefix\", \"/rootfs\", \"where the nodes root filesystem is mounted, for the file stage.\") <nl> } <nl> func runStartCmd(cmd *cobra.Command, args []string) { <nl> @@ -45,8 +46,12 @@ func runStartCmd(cmd *cobra.Command, args []string) { <nl> glog.Infof(\"Version: %+v\", version.Version) <nl> if startOpts.nodeName == \"\" { <nl> + name, ok := os.LookupEnv(\"NODE_NAME\") <nl> + if !ok || name == \"\" { <nl> glog.Fatalf(\"node-name is required\") <nl> } <nl> + startOpts.nodeName = name <nl> + } <nl> cb, err := newClientBuilder(startOpts.kubeconfig) <nl> if err != nil { <nl> ", "msg": "cmd/machine-config-daemon: update flags to specify node name from env"}
{"diff_id": 2331, "repo": "openshift/machine-config-operator", "sha": "691142bca1c9c40910f39d74d92f742e2f514b07", "time": "24.08.2018 17:23:34", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -86,11 +86,11 @@ func (dn *Daemon) Run(stop <-chan struct{}) error { <nl> // syncOnce only completes once. <nl> func (dn *Daemon) syncOnce() error { <nl> // validate that the machine correctly made it to the target state <nl> - status, err := dn.validate() <nl> + isDesired, err := dn.isDesiredMachineState() <nl> if err != nil { <nl> return err <nl> } <nl> - if !status { <nl> + if !isDesired { <nl> return dn.triggerUpdate() <nl> } <nl> @@ -143,11 +143,11 @@ func (dn *Daemon) triggerUpdate() error { <nl> return dn.update(currentConfig, desiredConfig) <nl> } <nl> -// validate confirms that the node is actually in the state that it wants to be <nl> -// in. it does this by looking at the elements in the target config and checks <nl> -// if all are present on the node. if any file/unit is missing or there is a <nl> -// mismatch, it re-triggers the update. <nl> -func (dn *Daemon) validate() (bool, error) { <nl> +// isDesiredMachineState confirms that the node is actually in the state that it <nl> +// wants to be in. It does this by looking at the elements in the target config <nl> +// and checks if all are present on the node. Returns true iff there are no <nl> +// mismatches (e.g. files, units... XXX: but not yet OS version). <nl> +func (dn *Daemon) isDesiredMachineState() (bool, error) { <nl> ccAnnotation, err := getNodeAnnotation(dn.kubeClient.CoreV1().Nodes(), dn.name, CurrentMachineConfigAnnotationKey) <nl> if err != nil { <nl> return false, err <nl> ", "msg": "pkg/daemon: clarify function naming and description\nThe `validate()` function was documented as retriggering the update when\nreally it just determined whether an update was needed. Fix the\ndocumentation there. Also rename it to something more explicit to make\nthings easier to grok."}
{"diff_id": 2342, "repo": "openshift/machine-config-operator", "sha": "e7f132defe4cc552142c9e58f135adf87fb4902b", "time": "06.09.2018 16:39:14", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -262,7 +262,7 @@ func (dn *Daemon) writeFiles(files []ignv2_2types.File) error { <nl> } <nl> // set chown if file information is provided <nl> - if f.User.ID != nil || f.User.Name != \"\" { <nl> + if f.User != nil || f.Group != nil { <nl> uid, gid, err := getFileOwnership(f) <nl> if err != nil { <nl> return fmt.Errorf(\"Failed to retrieve file ownership for file %q: %v\", f.Path, err) <nl> @@ -286,19 +286,30 @@ func (dn *Daemon) writeFiles(files []ignv2_2types.File) error { <nl> return nil <nl> } <nl> +// This is essentially ResolveNodeUidAndGid() from Ignition; XXX should dedupe <nl> func getFileOwnership(file ignv2_2types.File) (int, int, error) { <nl> - var uid, gid int <nl> - if file.User.ID != nil && file.Group.ID != nil { <nl> + uid, gid := 0, 0 // default to root <nl> + if file.User != nil { <nl> + if file.User.ID != nil { <nl> uid = *file.User.ID <nl> - gid = *file.Group.ID <nl> } else if file.User.Name != \"\" { <nl> osUser, err := user.Lookup(file.User.Name) <nl> if err != nil { <nl> return uid, gid, fmt.Errorf(\"Failed to retrieve UserID for username: %s\", file.User.Name) <nl> } <nl> - <nl> uid, _ = strconv.Atoi(osUser.Uid) <nl> - gid, _ = strconv.Atoi(osUser.Gid) <nl> + } <nl> + } <nl> + if file.Group != nil { <nl> + if file.Group.ID != nil { <nl> + gid = *file.Group.ID <nl> + } else if file.Group.Name != \"\" { <nl> + osGroup, err := user.LookupGroup(file.Group.Name) <nl> + if err != nil { <nl> + return uid, gid, fmt.Errorf(\"Failed to retrieve GroupID for group: %s\", file.Group.Name) <nl> + } <nl> + gid, _ = strconv.Atoi(osGroup.Gid) <nl> + } <nl> } <nl> return uid, gid, nil <nl> } <nl> ", "msg": "daemon: strengthen user/group handling\nWe were previously dereferencing `f.User` without checking if it was\n`nil` first. Just lift `ResolveNodeUidAndGid()` from Ignition (and make\na note that we should dedupe stuff down the line)."}
{"diff_id": 2343, "repo": "openshift/machine-config-operator", "sha": "02abda0299048c9107c5aaaad93509fb19cce6c2", "time": "06.09.2018 17:09:14", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -80,13 +80,34 @@ func (dn *Daemon) Run(stop <-chan struct{}) error { <nl> glog.Info(\"Starting MachineConfigDaemon\") <nl> defer glog.Info(\"Shutting down MachineConfigDaemon\") <nl> - err := dn.syncOnce() <nl> + err := dn.process() <nl> if err != nil { <nl> + glog.Errorf(\"Marking degraded due to: %v\", err) <nl> return setUpdateDegraded(dn.kubeClient.CoreV1().Nodes(), dn.name) <nl> } <nl> + <nl> return nil <nl> } <nl> +// process starts the main loop that actually does all the work. <nl> +func (dn *Daemon) process() error { <nl> + <nl> + // do a first pass before entering the main loop <nl> + if err := dn.syncOnce(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + for { <nl> + glog.V(2).Infof(\"Watching for node annotation updates on %q\", dn.name) <nl> + if err := waitUntilUpdate(dn.kubeClient.CoreV1().Nodes(), dn.name); err != nil { <nl> + return err <nl> + } <nl> + if err := dn.syncOnce(); err != nil { <nl> + return err <nl> + } <nl> + } <nl> +} <nl> + <nl> // syncOnce only completes once. <nl> func (dn *Daemon) syncOnce() error { <nl> // validate that the machine correctly made it to the target state <nl> @@ -94,8 +115,17 @@ func (dn *Daemon) syncOnce() error { <nl> if err != nil { <nl> return err <nl> } <nl> + <nl> if !isDesired { <nl> - return dn.triggerUpdate() <nl> + // this currently doesn't return, but may return in the future if the <nl> + // update could be done without rebooting <nl> + if err := dn.triggerUpdate(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + glog.V(2).Infof(\"Successfully updated without reboot\") <nl> + } else { <nl> + glog.V(2).Infof(\"Node is up to date\") <nl> } <nl> if err := setUpdateDone(dn.kubeClient.CoreV1().Nodes(), dn.name); err != nil { <nl> @@ -111,13 +141,7 @@ func (dn *Daemon) syncOnce() error { <nl> return err <nl> } <nl> - glog.V(2).Infof(\"Watching for node annotation updates\") <nl> - err = waitUntilUpdate(dn.kubeClient.CoreV1().Nodes(), dn.name) <nl> - if err != nil { <nl> - return fmt.Errorf(\"Failed to wait until update request: %v\", err) <nl> - } <nl> - <nl> - return dn.triggerUpdate() <nl> + return nil <nl> } <nl> // triggerUpdate starts the update using the current and the target config. <nl> ", "msg": "pkg/daemon: run main loop\nChange the higher level logic to run in a main loop that continuously\nwatches for updates and re-applies config changes. I.e., we don't want\nto reboot if a config change doesn't actually requires it."}
{"diff_id": 2350, "repo": "openshift/machine-config-operator", "sha": "3a076d03ae938cbd4b0bdd4157f13f85f1446727", "time": "24.09.2018 10:05:29", "diff": "mmm a / pkg/daemon/rpm-ostree.go <nl> ppp b / pkg/daemon/rpm-ostree.go <nl>@@ -6,12 +6,14 @@ import ( <nl> \"strings\" <nl> ) <nl> +// RpmOstreeState houses zero or more RpmOstreeDeployments <nl> // Subset of `rpm-ostree status --json` <nl> // https://github.com/projectatomic/rpm-ostree/blob/bce966a9812df141d38e3290f845171ec745aa4e/src/daemon/rpmostreed-deployment-utils.c#L227 <nl> type RpmOstreeState struct { <nl> Deployments []RpmOstreeDeployment <nl> } <nl> +// RpmOstreeDeployment represents a single deployment on a node <nl> type RpmOstreeDeployment struct { <nl> Id string `json:\"id\"` <nl> OSName string `json:\"osname\"` <nl> ", "msg": "daemon/rpm-ostree.go: Add doc for exported structs"}
{"diff_id": 2355, "repo": "openshift/machine-config-operator", "sha": "c5c7d1e731ec6df5c46bd1e8f2bf99b973f0e325", "time": "25.09.2018 13:43:35", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -438,21 +438,7 @@ func getFileOwnership(file ignv2_2types.File) (int, int, error) { <nl> // updateOS updates the system OS to the one specified in newConfig <nl> func (dn *Daemon) updateOS(oldConfig, newConfig *mcfgv1.MachineConfig) error { <nl> - // XXX(jl): don't re-ask rpm-ostree here, just cache from checkOS() <nl> - bootedOSImageURL, _, err := getBootedOSImageURL() <nl> - if err != nil { <nl> - glog.Warningf(\"Cannot retrieve bootedOSImageURL.\") <nl> - return err <nl> - } <nl> - glog.V(2).Infof(\"Retrieved Booted OS Image URL: %s\", bootedOSImageURL) <nl> - <nl> - // see similar block in checkOS() <nl> - if bootedOSImageURL == \"\" { <nl> - bootedOSImageURL = \"://dummy\" <nl> - glog.V(2).Infof(\"Assigned empty Booted OS Image URL to: %s\", bootedOSImageURL) <nl> - } <nl> - <nl> - if newConfig.Spec.OSImageURL == bootedOSImageURL { <nl> + if newConfig.Spec.OSImageURL == dn.bootedOSImageURL { <nl> return nil <nl> } <nl> ", "msg": "cmd/daemon: simplify OSImageURL querying\nThe booted OSImageURL is clearly not going to change during the same run\nof the daemon, so query it just once at the beginning instead. This\nsimplifies comparisons later on against new machine configs and avoids\nrunning `rpm-ostree status --json` twice."}
{"diff_id": 2362, "repo": "openshift/machine-config-operator", "sha": "e1339056647ba5173ef0e7bdfc745f14306aaecd", "time": "10.10.2018 12:15:11", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -134,6 +134,16 @@ func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) (bool <nl> return false, nil <nl> } <nl> + // Special case files append: if the new config wants us to append, then we <nl> + // have to force a reprovision since it's not idempotent <nl> + for _, f := range newIgn.Storage.Files { <nl> + if f.Append { <nl> + glog.Warningf(\"daemon can't reconcile state!\") <nl> + glog.Warningf(\"Ignition files includes append\") <nl> + return false, nil <nl> + } <nl> + } <nl> + <nl> // Systemd section <nl> // we can reconcile any state changes in the systemd section. <nl> ", "msg": "daemon: don't try to reconcile file appends\nIt's inherently non-idempotent. See upstream Ignition issue on this:"}
{"diff_id": 2363, "repo": "openshift/machine-config-operator", "sha": "79643e9076cc51fb1c649dd847289eb7b814de1c", "time": "12.10.2018 16:04:46", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -20,6 +20,7 @@ import ( <nl> ) <nl> const ( <nl> + DefaultDirectoryPermissions os.FileMode = 0755 <nl> DefaultFilePermissions os.FileMode = 0644 <nl> ) <nl> @@ -271,7 +272,7 @@ func (dn *Daemon) writeUnits(units []ignv2_2types.Unit) error { <nl> for i := range u.Dropins { <nl> glog.Infof(\"Writing systemd unit dropin %q\", u.Dropins[i].Name) <nl> path = filepath.Join(pathSystemd, u.Name+\".d\", u.Dropins[i].Name) <nl> - if err := dn.fileSystemClient.MkdirAll(filepath.Dir(path), os.FileMode(0655)); err != nil { <nl> + if err := dn.fileSystemClient.MkdirAll(filepath.Dir(path), DefaultDirectoryPermissions); err != nil { <nl> return fmt.Errorf(\"Failed to create directory %q: %v\", filepath.Dir(path), err) <nl> } <nl> glog.V(2).Infof(\"Created directory: %s\", path) <nl> @@ -289,7 +290,7 @@ func (dn *Daemon) writeUnits(units []ignv2_2types.Unit) error { <nl> glog.Infof(\"Writing systemd unit %q\", u.Name) <nl> path = filepath.Join(pathSystemd, u.Name) <nl> - if err := dn.fileSystemClient.MkdirAll(filepath.Dir(path), os.FileMode(0655)); err != nil { <nl> + if err := dn.fileSystemClient.MkdirAll(filepath.Dir(path), DefaultDirectoryPermissions); err != nil { <nl> return fmt.Errorf(\"Failed to create directory %q: %v\", filepath.Dir(path), err) <nl> } <nl> glog.V(2).Infof(\"Created directory: %s\", path) <nl> @@ -355,7 +356,7 @@ func (dn *Daemon) writeFiles(files []ignv2_2types.File) error { <nl> for _, f := range files { <nl> glog.Infof(\"Writing file %q\", f.Path) <nl> // create any required directories for the file <nl> - if err := dn.fileSystemClient.MkdirAll(filepath.Dir(f.Path), os.FileMode(0655)); err != nil { <nl> + if err := dn.fileSystemClient.MkdirAll(filepath.Dir(f.Path), DefaultDirectoryPermissions); err != nil { <nl> return fmt.Errorf(\"Failed to create directory %q: %v\", filepath.Dir(f.Path), err) <nl> } <nl> ", "msg": "daemon: create leading dirs with 0755\nThis matches the same mode Ignition uses to create leading directories."}
{"diff_id": 2367, "repo": "openshift/machine-config-operator", "sha": "88c4a8b680beaf4dbebd69a33b92a5666d763cb1", "time": "18.10.2018 14:50:41", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -3,6 +3,7 @@ package daemon <nl> import ( <nl> \"fmt\" <nl> \"io/ioutil\" <nl> + \"net/http\" <nl> \"os\" <nl> \"path/filepath\" <nl> \"strings\" <nl> @@ -11,6 +12,7 @@ import ( <nl> ignv2_2types \"github.com/coreos/ignition/config/v2_2/types\" <nl> \"github.com/golang/glog\" <nl> drain \"github.com/openshift/kubernetes-drain\" <nl> + \"github.com/openshift/machine-config-operator/lib/resourceread\" <nl> mcfgv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" <nl> mcfgclientset \"github.com/openshift/machine-config-operator/pkg/generated/clientset/versioned\" <nl> mcfgclientv1 \"github.com/openshift/machine-config-operator/pkg/generated/clientset/versioned/typed/machineconfiguration.openshift.io/v1\" <nl> @@ -420,6 +422,38 @@ func (dn *Daemon) Close() { <nl> dn.loginClient.Close() <nl> } <nl> +// getMachineConfigFromFile parses a valid machine config file in yaml format and returns <nl> +// a MachineConfig struct. <nl> +func (dn *Daemon) getMachineConfigFromFile(filePath string) (*mcfgv1.MachineConfig, error) { <nl> + data, err := dn.fileSystemClient.ReadFile(filePath) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + config := resourceread.ReadMachineConfigV1OrDie(data) <nl> + return config, nil <nl> +} <nl> + <nl> +// getMachineConfigFromURL reads a remote MC in yaml format and returns a MachineConfig struct. <nl> +func (dn *Daemon) getMachineConfigFromURL(url string) (*mcfgv1.MachineConfig, error) { <nl> + // Make a request to the remote URL <nl> + resp, err := http.Get(url) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + defer resp.Body.Close() <nl> + <nl> + // Read the body content from the request <nl> + body, err := dn.fileSystemClient.ReadAll(resp.Body) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + <nl> + // Unmarshal the body into the machineConfig <nl> + config := resourceread.ReadMachineConfigV1OrDie(body) <nl> + <nl> + return config, nil <nl> +} <nl> + <nl> func getMachineConfig(client mcfgclientv1.MachineConfigInterface, name string) (*mcfgv1.MachineConfig, error) { <nl> return client.Get(name, metav1.GetOptions{}) <nl> } <nl> ", "msg": "daemon: More methods for getting MCs"}
{"diff_id": 2372, "repo": "openshift/machine-config-operator", "sha": "10057300ec039b03733db85c98f5c7215e6c42ce", "time": "07.11.2018 12:07:24", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -80,6 +80,12 @@ func (dn *Daemon) update(oldConfig, newConfig *mcfgv1.MachineConfig) error { <nl> func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) (bool, error) { <nl> glog.Info(\"Checking if configs are reconcilable\") <nl> + // We skip out of reconcilable if there is no Kind and we are in runOnce mode. The <nl> + // reason is that there is a good chance a previous state is not available to match against. <nl> + if oldConfig.Kind == \"\" && dn.onceFrom != \"\" { <nl> + glog.Infof(\"Missing kind in old config. Assuming reconcilable with new.\") <nl> + return true, nil <nl> + } <nl> oldIgn := oldConfig.Spec.Config <nl> newIgn := newConfig.Spec.Config <nl> ", "msg": "daemon/update: Allow reconcile skip\nWhen we are in runOnce mode AND the previous MachineConfig does\nnot have a Kind we can assume that there was no previous config\nto check against."}
{"diff_id": 2378, "repo": "openshift/machine-config-operator", "sha": "2dc1d4f0f5e6e544db1a1ca6011b5e0284728111", "time": "14.11.2018 10:22:47", "diff": "mmm a / cmd/machine-config-daemon/start.go <nl> ppp b / cmd/machine-config-daemon/start.go <nl>@@ -73,7 +73,7 @@ func runStartCmd(cmd *cobra.Command, args []string) { <nl> // If we are asked to run once and it's a valid file system path use <nl> // the bare Daemon <nl> - if startOpts.onceFrom != \"\" && daemon.ValidPath(startOpts.onceFrom) { <nl> + if startOpts.onceFrom != \"\" { <nl> dn, err = daemon.New( <nl> startOpts.rootMount, <nl> startOpts.nodeName, <nl> ", "msg": "MCD: Fix once-from start\nCurrently, if an invalid path is provided to mcd with\nonce-from, mcd will attempt to start in normal mode.\nWe validate the path later in the process, this commit\nensures we don't start mcd in normal mode by mistake."}
{"diff_id": 2379, "repo": "openshift/machine-config-operator", "sha": "7c1c618f550583cd0c527281a44f9e4ca87c299b", "time": "16.11.2018 11:30:24", "diff": "mmm a / pkg/operator/operator.go <nl> ppp b / pkg/operator/operator.go <nl>@@ -270,7 +270,9 @@ func (optr *Operator) getCAsFromConfigMap(namespace, name, key string) ([]byte, <nl> } else if d, dok := cm.Data[key]; dok { <nl> raw, err := base64.StdEncoding.DecodeString(d) <nl> if err != nil { <nl> - return nil, err <nl> + // this is actually the result of a bad assumption. configmap values are not encoded. <nl> + // After the installer pull merges, this entire attempt to decode can go away. <nl> + return []byte(d), nil <nl> } <nl> return raw, nil <nl> } else { <nl> ", "msg": "accept unencoded configmaps\nconfigmap values are raw strings, not base64 encoded.\nThis makes the MCO bilingual until the installer pull merges"}
{"diff_id": 2380, "repo": "openshift/machine-config-operator", "sha": "9e54fdf191898fb8a45af2c4e92b99a468b303e0", "time": "16.11.2018 14:26:28", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -15,7 +15,6 @@ import ( <nl> ignv2_2types \"github.com/coreos/ignition/config/v2_2/types\" <nl> \"github.com/golang/glog\" <nl> drain \"github.com/openshift/kubernetes-drain\" <nl> - \"github.com/openshift/machine-config-operator/lib/resourceread\" <nl> mcfgv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" <nl> mcfgclientset \"github.com/openshift/machine-config-operator/pkg/generated/clientset/versioned\" <nl> mcfgclientv1 \"github.com/openshift/machine-config-operator/pkg/generated/clientset/versioned/typed/machineconfiguration.openshift.io/v1\" <nl> @@ -653,48 +652,6 @@ func (dn *Daemon) Close() { <nl> dn.loginClient.Close() <nl> } <nl> -// getMachineConfigFromFile parses a valid machine config file in yaml format and returns <nl> -// a MachineConfig struct. <nl> -func (dn *Daemon) getMachineConfigFromFile(filePath string) (*mcfgv1.MachineConfig, error) { <nl> - data, err := dn.fileSystemClient.ReadFile(filePath) <nl> - if err != nil { <nl> - return nil, err <nl> - } <nl> - config := resourceread.ReadMachineConfigV1OrDie(data) <nl> - return config, nil <nl> -} <nl> - <nl> -// getIgnitionConfigFromFile parses an Ignition file and returns a usable Ignition config <nl> -func (dn *Daemon) getIgnitionConfigFromFile(filePath string) (ignv2_2types.Config, error) { <nl> - data, err := dn.fileSystemClient.ReadFile(filePath) <nl> - if err != nil { <nl> - return ignv2_2types.Config{}, err <nl> - } <nl> - config, _, err := ignv2.Parse(data) <nl> - return config, err <nl> -} <nl> - <nl> -// getMachineConfigFromURL reads a remote MC in yaml format and returns a MachineConfig struct. <nl> -func (dn *Daemon) getMachineConfigFromURL(url string) (*mcfgv1.MachineConfig, error) { <nl> - // Make a request to the remote URL <nl> - resp, err := http.Get(url) <nl> - if err != nil { <nl> - return nil, err <nl> - } <nl> - defer resp.Body.Close() <nl> - <nl> - // Read the body content from the request <nl> - body, err := dn.fileSystemClient.ReadAll(resp.Body) <nl> - if err != nil { <nl> - return nil, err <nl> - } <nl> - <nl> - // Unmarshal the body into the machineConfig <nl> - config := resourceread.ReadMachineConfigV1OrDie(body) <nl> - <nl> - return config, nil <nl> -} <nl> - <nl> func getMachineConfig(client mcfgclientv1.MachineConfigInterface, name string) (*mcfgv1.MachineConfig, error) { <nl> return client.Get(name, metav1.GetOptions{}) <nl> } <nl> ", "msg": "daemon: Remove dead methods\nThe following were removed from Daemon as they are no longer used:\ngetMachineConfigFromURL\ngetIgnitionConfigFromFile\ngetMachineConfigFromURL"}
{"diff_id": 2382, "repo": "openshift/machine-config-operator", "sha": "a84ecb37a97f411160201abc89e5470dcd9c8a68", "time": "30.11.2018 09:29:50", "diff": "mmm a / cmd/machine-config-daemon/start.go <nl> ppp b / cmd/machine-config-daemon/start.go <nl>@@ -100,7 +100,7 @@ func runStartCmd(cmd *cobra.Command, args []string) { <nl> } else { <nl> cb, err := common.NewClientBuilder(startOpts.kubeconfig) <nl> if err != nil { <nl> - glog.Fatalf(\"failed to initialize daemon: %v\", err) <nl> + glog.Fatalf(\"failed to initialize ClientBuilder: %v\", err) <nl> } <nl> ctx = common.CreateControllerContext(cb, stopCh, componentName) <nl> // create the daemon instance. this also initializes kube client items <nl> ", "msg": "daemon: tweak log message\nWe had two \"failed to initialize daemon\" log messages in the\n`runStartCmd()` function. One can still distinguish which one was\nemitted since glog does print the line number too, but really that first\none wasn't accurate, so tweak it."}
{"diff_id": 2383, "repo": "openshift/machine-config-operator", "sha": "c368609431253b6fea380a9bede406cde36352b4", "time": "30.11.2018 11:27:22", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -426,7 +426,7 @@ func (dn *Daemon) executeUpdateFromClusterWithMachineConfig(desiredConfig *mcfgv <nl> if err := dn.triggerUpdateWithMachineConfig(desiredConfig); err != nil { <nl> glog.Errorf(\"Marking degraded due to: %v\", err) <nl> if errSet := dn.nodeWriter.SetUpdateDegraded(dn.kubeClient.CoreV1().Nodes(), dn.name); errSet != nil { <nl> - glog.Errorf(\"Futher error attempting to set the node to degraded: %v\", errSet) <nl> + glog.Errorf(\"Further error attempting to set the node to degraded: %v\", errSet) <nl> } <nl> // reboot the node, which will catch the degraded state and sleep <nl> dn.reboot() <nl> ", "msg": "daemon: fix typo in error message"}
{"diff_id": 2384, "repo": "openshift/machine-config-operator", "sha": "39d00a06fbdc14fa905a34c14916ae3e23d8b4d8", "time": "30.11.2018 11:27:44", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -23,6 +23,7 @@ import ( <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> \"k8s.io/apimachinery/pkg/runtime\" <nl> \"k8s.io/apimachinery/pkg/runtime/serializer\" <nl> + \"k8s.io/apimachinery/pkg/util/wait\" <nl> coreinformersv1 \"k8s.io/client-go/informers/core/v1\" <nl> \"k8s.io/client-go/kubernetes\" <nl> corelisterv1 \"k8s.io/client-go/listers/core/v1\" <nl> @@ -653,7 +654,18 @@ func (dn *Daemon) Close() { <nl> } <nl> func getMachineConfig(client mcfgclientv1.MachineConfigInterface, name string) (*mcfgv1.MachineConfig, error) { <nl> - return client.Get(name, metav1.GetOptions{}) <nl> + // Retry for 5 minutes to get a MachineConfig in case of transient errors. <nl> + var mc *mcfgv1.MachineConfig = nil <nl> + err := wait.PollImmediate(10*time.Second, 5*time.Minute, func() (bool, error) { <nl> + var err error <nl> + mc, err = client.Get(name, metav1.GetOptions{}) <nl> + if err == nil { <nl> + return true, nil <nl> + } <nl> + glog.Infof(\"While getting MachineConfig %s, got: %v. Retrying...\", name, err) <nl> + return false, nil <nl> + }) <nl> + return mc, err <nl> } <nl> // ValidPath attempts to see if the path provided is indeed an acceptable <nl> ", "msg": "daemon: retry for 5 minutes to get MachineConfig\nWhenever we try to get a MachineConfig, keep retrying for 5 minutes in\ncase we hit transient errors. One such error for example is the\nMachineConfig inexplicably disappearing (see"}
{"diff_id": 2386, "repo": "openshift/machine-config-operator", "sha": "0a23bb95af9b841755f4261cde6cdcd9bfe80f5c", "time": "04.12.2018 14:50:16", "diff": "mmm a / pkg/daemon/writer.go <nl> ppp b / pkg/daemon/writer.go <nl>@@ -105,7 +105,7 @@ func (nw *NodeWriter) SetUpdateDegradedIgnoreErr(err error, client corev1.NodeIn <nl> // log error here since the caller won't look at it <nl> degraded_err := nw.SetUpdateDegraded(err, client, node) <nl> if degraded_err != nil { <nl> - glog.Error(\"Error while setting degraded: %v\", degraded_err) <nl> + glog.Errorf(\"Error while setting degraded: %v\", degraded_err) <nl> } <nl> return err <nl> } <nl> ", "msg": "daemon: Fix format message for degraded message\nWe were using `%v` so need `f` for format."}
{"diff_id": 2387, "repo": "openshift/machine-config-operator", "sha": "46c0b8f80b65accf5e784466e6eea0d61f087e1c", "time": "05.12.2018 16:28:28", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -491,7 +491,10 @@ func (dn *Daemon) updateOS(oldConfig, newConfig *mcfgv1.MachineConfig) error { <nl> // cleans up the agent's connections, and then sleeps for 7 days. if it wakes up <nl> // and manages to return, it returns a scary error message. <nl> func (dn *Daemon) reboot(rationale string) error { <nl> + // We'll only have a recorder if we're cluster driven <nl> + if (dn.recorder != nil) { <nl> dn.recorder.Eventf(&corev1.Node{ObjectMeta: metav1.ObjectMeta{Name: dn.name}}, corev1.EventTypeNormal, \"Reboot\", rationale) <nl> + } <nl> glog.Infof(\"Rebooting: %s\", rationale) <nl> // reboot <nl> ", "msg": "daemon: Only log to recorder in cluster driven model\nI was looking at this code again and realized this is probably\nan issue."}
{"diff_id": 2397, "repo": "openshift/machine-config-operator", "sha": "a75cce0a93220f080b3351d139ce10b512fc158f", "time": "08.01.2019 09:56:42", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -281,7 +281,7 @@ func (dn *Daemon) deleteStaleData(oldConfig, newConfig *mcfgv1.MachineConfig) { <nl> glog.V(2).Info(\"Removing stale config storage files\") <nl> for _, f := range oldConfig.Spec.Config.Storage.Files { <nl> if _, ok := newFileSet[f.Path]; !ok { <nl> - dn.fileSystemClient.RemoveAll(path) <nl> + dn.fileSystemClient.RemoveAll(f.Path) <nl> } <nl> } <nl> ", "msg": "daemon: fix deletion of stale config files\nMinor regression from We weren't passing the right variable to\n`RemoveAll()`.\nCloses:"}
{"diff_id": 2399, "repo": "openshift/machine-config-operator", "sha": "784759bd54309d8d6def81f9bff5841ce98c1d2f", "time": "08.01.2019 10:00:55", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -277,14 +277,13 @@ func (dn *Daemon) deleteStaleData(oldConfig, newConfig *mcfgv1.MachineConfig) { <nl> newFileSet[f.Path] = struct{}{} <nl> } <nl> - glog.V(2).Info(\"Removing stale config storage files\") <nl> for _, f := range oldConfig.Spec.Config.Storage.Files { <nl> if _, ok := newFileSet[f.Path]; !ok { <nl> + glog.V(2).Infof(\"Deleting stale config file: %s\", f.Path) <nl> dn.fileSystemClient.RemoveAll(f.Path) <nl> } <nl> } <nl> - glog.V(2).Info(\"Removing stale config systemd units\") <nl> newUnitSet := make(map[string]struct{}) <nl> newDropinSet := make(map[string]struct{}) <nl> for _, u := range newConfig.Spec.Config.Systemd.Units { <nl> @@ -300,6 +299,7 @@ func (dn *Daemon) deleteStaleData(oldConfig, newConfig *mcfgv1.MachineConfig) { <nl> for j := range u.Dropins { <nl> path := filepath.Join(pathSystemd, u.Name+\".d\", u.Dropins[j].Name) <nl> if _, ok := newDropinSet[path]; !ok { <nl> + glog.V(2).Infof(\"Deleting stale systemd dropin file: %s\", path) <nl> dn.fileSystemClient.RemoveAll(path) <nl> } <nl> } <nl> @@ -308,6 +308,7 @@ func (dn *Daemon) deleteStaleData(oldConfig, newConfig *mcfgv1.MachineConfig) { <nl> if err := dn.disableUnit(u); err != nil { <nl> glog.Warningf(\"Unable to disable %s: %s\", u.Name, err) <nl> } <nl> + glog.V(2).Infof(\"Deleting stale systemd unit file: %s\", path) <nl> dn.fileSystemClient.RemoveAll(path) <nl> } <nl> } <nl> ", "msg": "daemon: provide better logging during deletion\nAdd more level 2 logging so we can see exactly which files are being\ndeleted."}
{"diff_id": 2403, "repo": "openshift/machine-config-operator", "sha": "82c3272873d020bb7728457c530d8b9212cc5686", "time": "09.01.2019 22:58:19", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -141,9 +141,10 @@ func New( <nl> } <nl> osImageURL := \"\" <nl> + osVersion := \"\" <nl> // Only pull the osImageURL from OSTree when we are on RHCOS <nl> if operatingSystem == MachineConfigDaemonOSRHCOS { <nl> - osImageURL, osVersion, err := nodeUpdaterClient.GetBootedOSImageURL(rootMount) <nl> + osImageURL, osVersion, err = nodeUpdaterClient.GetBootedOSImageURL(rootMount) <nl> if err != nil { <nl> return nil, fmt.Errorf(\"Error reading osImageURL from rpm-ostree: %v\", err) <nl> } <nl> ", "msg": "daemon: Fix load of booted OS\nWe can almost update but go degraded on boot because we had\nincorrect variable scoping here :cry:"}
{"diff_id": 2405, "repo": "openshift/machine-config-operator", "sha": "e69094828c83abd8f76c1c7e249c6bbeadbb0b1a", "time": "11.01.2019 02:26:10", "diff": "mmm a / pkg/daemon/writer.go <nl> ppp b / pkg/daemon/writer.go <nl>@@ -20,6 +20,7 @@ type message struct { <nl> client corev1.NodeInterface <nl> node string <nl> annos map[string]string <nl> + taint *v1.Taint <nl> responseChannel chan error <nl> } <nl> @@ -43,7 +44,11 @@ func (nw *NodeWriter) Run(stop <-chan struct{}) { <nl> case <-stop: <nl> return <nl> case msg := <-nw.writer: <nl> + if msg.annos != nil { <nl> msg.responseChannel <- setNodeAnnotations(msg.client, msg.node, msg.annos) <nl> + } else if msg.taint != nil { <nl> + msg.responseChannel <- addTaint(msg.client, msg.node, msg.taint) <nl> + } <nl> } <nl> } <nl> } <nl> @@ -117,6 +122,18 @@ func (nw *NodeWriter) SetUpdateDegradedMsgIgnoreErr(msg string, client corev1.No <nl> return nw.SetUpdateDegradedIgnoreErr(err, client, node) <nl> } <nl> +// SetTaint takes the specified taint and applies it to the node's taints <nl> +func (nw *NodeWriter) SetTaint(client corev1.NodeInterface, node string, taint *v1.Taint) error { <nl> + respChan := make(chan error, 1) <nl> + nw.writer <- message{ <nl> + client: client, <nl> + node: node, <nl> + taint: taint, <nl> + responseChannel: respChan, <nl> + } <nl> + return <-respChan <nl> +} <nl> + <nl> // updateNodeRetry calls f to update a node object in Kubernetes. <nl> // It will attempt to update the node by applying f to it up to DefaultBackoff <nl> // number of times. <nl> @@ -151,3 +168,11 @@ func setNodeAnnotations(client corev1.NodeInterface, node string, m map[string]s <nl> } <nl> }) <nl> } <nl> + <nl> +// addTaint appends the specified taint to the nodespec <nl> +func addTaint(client corev1.NodeInterface, node string, taint *v1.Taint) error { <nl> + return updateNodeRetry(client, node, func(node *v1.Node) { <nl> + node.Spec.Taints = append(node.Spec.Taints, *taint) <nl> + }) <nl> + <nl> +} <nl> ", "msg": "daemon/writer.go: add taint functionality\nModify message and Run structure to write taints to nodespecs."}
{"diff_id": 2406, "repo": "openshift/machine-config-operator", "sha": "dad109738309c216c13aa032c2dca618acc8049d", "time": "14.01.2019 15:10:23", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -219,6 +219,7 @@ func NewClusterDrivenDaemon( <nl> eventBroadcaster.StartRecordingToSink(&clientsetcorev1.EventSinkImpl{Interface: kubeClient.CoreV1().Events(\"\")}) <nl> dn.recorder = eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: \"machineconfigdaemon\", Host: nodeName}) <nl> + glog.Infof(\"Managing node: %s\", nodeName) <nl> if err = loadNodeAnnotations(dn.kubeClient.CoreV1().Nodes(), nodeName); err != nil { <nl> return nil, err <nl> } <nl> ", "msg": "daemon: Log the node name\nI often look at a MCD log and it'd be helpful to have the node\nright there rather than needing to describe the object.  Unlike\nmost pods, the node we're managing is rather critical information."}
{"diff_id": 2408, "repo": "openshift/machine-config-operator", "sha": "3e31dbef914c0ef75f795aae7745e0163ff9679f", "time": "14.01.2019 19:00:11", "diff": "mmm a / pkg/daemon/node.go <nl> ppp b / pkg/daemon/node.go <nl>@@ -6,6 +6,7 @@ import ( <nl> \"io/ioutil\" <nl> \"time\" <nl> + \"github.com/golang/glog\" <nl> core_v1 \"k8s.io/api/core/v1\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> \"k8s.io/apimachinery/pkg/util/wait\" <nl> @@ -40,6 +41,7 @@ func loadNodeAnnotations(client corev1.NodeInterface, node string) error { <nl> return fmt.Errorf(\"Failed to unmarshal initial annotations: %v\", err) <nl> } <nl> + glog.Infof(\"Setting initial node config: %s\", initial[CurrentMachineConfigAnnotationKey]) <nl> err = setNodeAnnotations(client, node, initial) <nl> if err != nil { <nl> return fmt.Errorf(\"Failed to set initial annotations: %v\", err) <nl> ", "msg": "daemon: Log initial node config\nThis is also helpful to understand the flow; whether we are\nusing an initial config or pulling down a new one."}
{"diff_id": 2409, "repo": "openshift/machine-config-operator", "sha": "d902c1f9326b0d8d85c4b367754750e02f1051cf", "time": "15.01.2019 14:40:26", "diff": "mmm a / pkg/controller/render/render_controller.go <nl> ppp b / pkg/controller/render/render_controller.go <nl>@@ -36,6 +36,10 @@ const ( <nl> // <nl> // 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s <nl> maxRetries = 15 <nl> + <nl> + // renderDelay is a pause to avoid churn in MachineConfigs; see <nl> + // https://github.com/openshift/machine-config-operator/issues/301 <nl> + renderDelay = 5 * time.Second <nl> ) <nl> var ( <nl> @@ -91,7 +95,7 @@ func New( <nl> }) <nl> ctrl.syncHandler = ctrl.syncMachineConfigPool <nl> - ctrl.enqueueMachineConfigPool = ctrl.enqueue <nl> + ctrl.enqueueMachineConfigPool = ctrl.enqueueDefault <nl> ctrl.mcpLister = mcpInformer.Lister() <nl> ctrl.mcLister = mcInformer.Lister() <nl> @@ -321,7 +325,7 @@ func (ctrl *Controller) enqueueRateLimited(pool *mcfgv1.MachineConfigPool) { <nl> } <nl> // enqueueAfter will enqueue a pool after the provided amount of time. <nl> -func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfig, after time.Duration) { <nl> +func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfigPool, after time.Duration) { <nl> key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(pool) <nl> if err != nil { <nl> utilruntime.HandleError(fmt.Errorf(\"Couldn't get key for object %#v: %v\", pool, err)) <nl> @@ -331,6 +335,11 @@ func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfig, after time.Dura <nl> ctrl.queue.AddAfter(key, after) <nl> } <nl> +// enqueueDefault calls a default enqueue function <nl> +func (ctrl *Controller) enqueueDefault(pool *mcfgv1.MachineConfigPool) { <nl> + ctrl.enqueueAfter(pool, renderDelay) <nl> +} <nl> + <nl> // worker runs a worker thread that just dequeues items, processes them, and marks them done. <nl> // It enforces that the syncHandler is never invoked concurrently with the same key. <nl> func (ctrl *Controller) worker() { <nl> ", "msg": "controller: Add a 5s delay before rendering MCs\nTo reduce churn if MCs are being created rapidly - both on general\nprinciple, and also to reduce our exposure to the current bug\nthat a booting node may fail to find a GC'd MachineConfig:"}
{"diff_id": 2418, "repo": "openshift/machine-config-operator", "sha": "1832d525d1d84356b13fb6e9d2b250b060ab981d", "time": "22.01.2019 17:12:03", "diff": "mmm a / pkg/controller/node/node_controller.go <nl> ppp b / pkg/controller/node/node_controller.go <nl>@@ -40,6 +40,10 @@ const ( <nl> // <nl> // 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s <nl> maxRetries = 15 <nl> + <nl> + // updateDelay is a pause to deal with churn in MachineConfigs; see <nl> + // https://github.com/openshift/machine-config-operator/issues/301 <nl> + updateDelay = 5 * time.Second <nl> ) <nl> // controllerKind contains the schema.GroupVersionKind for this controller type. <nl> @@ -99,7 +103,7 @@ func New( <nl> }) <nl> ctrl.syncHandler = ctrl.syncMachineConfigPool <nl> - ctrl.enqueueMachineConfigPool = ctrl.enqueue <nl> + ctrl.enqueueMachineConfigPool = ctrl.enqueueDefault <nl> ctrl.mcpLister = mcpInformer.Lister() <nl> ctrl.nodeLister = nodeInformer.Lister() <nl> @@ -296,7 +300,7 @@ func (ctrl *Controller) enqueueRateLimited(pool *mcfgv1.MachineConfigPool) { <nl> } <nl> // enqueueAfter will enqueue a pool after the provided amount of time. <nl> -func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfig, after time.Duration) { <nl> +func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfigPool, after time.Duration) { <nl> key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(pool) <nl> if err != nil { <nl> utilruntime.HandleError(fmt.Errorf(\"Couldn't get key for object %#v: %v\", pool, err)) <nl> @@ -306,6 +310,11 @@ func (ctrl *Controller) enqueueAfter(pool *mcfgv1.MachineConfig, after time.Dura <nl> ctrl.queue.AddAfter(key, after) <nl> } <nl> +// enqueueDefault calls a default enqueue function <nl> +func (ctrl *Controller) enqueueDefault(pool *mcfgv1.MachineConfigPool) { <nl> + ctrl.enqueueAfter(pool, updateDelay) <nl> +} <nl> + <nl> // worker runs a worker thread that just dequeues items, processes them, and marks them done. <nl> // It enforces that the syncHandler is never invoked concurrently with the same key. <nl> func (ctrl *Controller) worker() { <nl> ", "msg": "controller/node: Also add a 5s delay here responding to pool changes\nThis is like\nbut for the node controller.\nWe really don't need to react *instantly* to start updating and\nrebooting machines, and having a small delay will help avoid\nraces when MCs are created rapidly."}
{"diff_id": 2424, "repo": "openshift/machine-config-operator", "sha": "b5406ff686d065d0410f9a653e3f80d9ffe8ae62", "time": "01.02.2019 19:39:22", "diff": "mmm a / cmd/machine-config-controller/start.go <nl> ppp b / cmd/machine-config-controller/start.go <nl>@@ -79,6 +79,7 @@ func runStartCmd(cmd *cobra.Command, args []string) { <nl> } <nl> func startControllers(ctx *common.ControllerContext) error { <nl> + // Our primary MCs come from here <nl> go template.New( <nl> rootOpts.templates, <nl> ctx.InformerFactory.Machineconfiguration().V1().ControllerConfigs(), <nl> @@ -87,6 +88,18 @@ func startControllers(ctx *common.ControllerContext) error { <nl> ctx.ClientBuilder.MachineConfigClientOrDie(\"template-controller\"), <nl> ).Run(2, ctx.Stop) <nl> + // Add all \"sub-renderers here\" <nl> + go kubeletconfig.New( <nl> + rootOpts.templates, <nl> + ctx.InformerFactory.Machineconfiguration().V1().MachineConfigPools(), <nl> + ctx.InformerFactory.Machineconfiguration().V1().ControllerConfigs(), <nl> + ctx.InformerFactory.Machineconfiguration().V1().KubeletConfigs(), <nl> + ctx.ClientBuilder.KubeClientOrDie(\"kubelet-config-controller\"), <nl> + ctx.ClientBuilder.MachineConfigClientOrDie(\"kubelet-config-controller\"), <nl> + ).Run(2, ctx.Stop) <nl> + <nl> + // The renderer creates \"rendered\" MCs from the MC fragments generated by <nl> + // the above sub-controllers, which are then consumed by the node controller <nl> go render.New( <nl> ctx.InformerFactory.Machineconfiguration().V1().MachineConfigPools(), <nl> ctx.InformerFactory.Machineconfiguration().V1().MachineConfigs(), <nl> @@ -94,6 +107,7 @@ func startControllers(ctx *common.ControllerContext) error { <nl> ctx.ClientBuilder.MachineConfigClientOrDie(\"render-controller\"), <nl> ).Run(2, ctx.Stop) <nl> + // The node controller consumes data written by the above <nl> go node.New( <nl> ctx.InformerFactory.Machineconfiguration().V1().MachineConfigPools(), <nl> ctx.KubeInformerFactory.Core().V1().Nodes(), <nl> @@ -101,14 +115,5 @@ func startControllers(ctx *common.ControllerContext) error { <nl> ctx.ClientBuilder.MachineConfigClientOrDie(\"node-update-controller\"), <nl> ).Run(2, ctx.Stop) <nl> - go kubeletconfig.New( <nl> - rootOpts.templates, <nl> - ctx.InformerFactory.Machineconfiguration().V1().MachineConfigPools(), <nl> - ctx.InformerFactory.Machineconfiguration().V1().ControllerConfigs(), <nl> - ctx.InformerFactory.Machineconfiguration().V1().KubeletConfigs(), <nl> - ctx.ClientBuilder.KubeClientOrDie(\"kubelet-config-controller\"), <nl> - ctx.ClientBuilder.MachineConfigClientOrDie(\"kubelet-config-controller\"), <nl> - ).Run(2, ctx.Stop) <nl> - <nl> return nil <nl> } <nl> ", "msg": "controller: Start sub-controllers in order of dependency\nThe render controller needs kubelet data, so start it first.\nAnd add a bit of docs."}
{"diff_id": 2430, "repo": "openshift/machine-config-operator", "sha": "34300ccb4ab266b5791909f267d0c8d2cafd645f", "time": "08.02.2019 12:54:40", "diff": "mmm a / pkg/operator/sync.go <nl> ppp b / pkg/operator/sync.go <nl>@@ -329,7 +329,7 @@ func (optr *Operator) syncRequiredMachineConfigPools(config renderConfig) error <nl> return err <nl> } <nl> var lastErr error <nl> - if err := wait.Poll(time.Second, 5*time.Minute, func() (bool, error) { <nl> + if err := wait.Poll(time.Second, 10*time.Minute, func() (bool, error) { <nl> pools, err := optr.mcpLister.List(sel) <nl> if apierrors.IsNotFound(err) { <nl> return false, err <nl> @@ -355,7 +355,7 @@ func (optr *Operator) syncRequiredMachineConfigPools(config renderConfig) error <nl> } <nl> return true, nil <nl> }); err != nil { <nl> - if err.Error() == wait.ErrWaitTimeout.Error() { <nl> + if err == wait.ErrWaitTimeout { <nl> return fmt.Errorf(\"%v during syncRequiredMachineConfigPools: %v\", err, lastErr) <nl> } <nl> return err <nl> @@ -365,16 +365,16 @@ func (optr *Operator) syncRequiredMachineConfigPools(config renderConfig) error <nl> const ( <nl> deploymentRolloutPollInterval = time.Second <nl> - deploymentRolloutTimeout = 5 * time.Minute <nl> + deploymentRolloutTimeout = 10 * time.Minute <nl> daemonsetRolloutPollInterval = time.Second <nl> - daemonsetRolloutTimeout = 5 * time.Minute <nl> + daemonsetRolloutTimeout = 10 * time.Minute <nl> customResourceReadyInterval = time.Second <nl> - customResourceReadyTimeout = 5 * time.Minute <nl> + customResourceReadyTimeout = 10 * time.Minute <nl> controllerConfigCompletedInterval = time.Second <nl> - controllerConfigCompletedTimeout = time.Minute <nl> + controllerConfigCompletedTimeout = 5 * time.Minute <nl> ) <nl> func (optr *Operator) waitForCustomResourceDefinition(resource *apiextv1beta1.CustomResourceDefinition) error { <nl> ", "msg": "pkg/operator: wait longer for resources to be available to CVO"}
{"diff_id": 2442, "repo": "openshift/machine-config-operator", "sha": "64b29879b195904278cdb2d685b64a49f404cc51", "time": "17.02.2019 01:42:26", "diff": "mmm a / test/e2e/mcd_test.go <nl> ppp b / test/e2e/mcd_test.go <nl>@@ -12,9 +12,11 @@ import ( <nl> \"k8s.io/apimachinery/pkg/labels\" <nl> \"k8s.io/apimachinery/pkg/util/uuid\" <nl> \"k8s.io/apimachinery/pkg/util/wait\" <nl> + \"k8s.io/client-go/kubernetes\" <nl> \"github.com/openshift/machine-config-operator/cmd/common\" <nl> mcv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" <nl> + \"github.com/openshift/machine-config-operator/pkg/daemon/constants\" <nl> ) <nl> // Test case for https://github.com/openshift/machine-config-operator/issues/358 <nl> @@ -104,7 +106,7 @@ func TestMCDeployed(t *testing.T) { <nl> // grab the latest worker- MC <nl> var newMCName string <nl> - err = wait.Poll(2*time.Second, 5*time.Minute, func() (bool, error) { <nl> + if err := wait.PollImmediate(2*time.Second, 5*time.Minute, func() (bool, error) { <nl> mcp, err := mcClient.MachineconfigurationV1().MachineConfigPools().Get(\"worker\", metav1.GetOptions{}) <nl> if err != nil { <nl> return false, err <nl> @@ -116,33 +118,41 @@ func TestMCDeployed(t *testing.T) { <nl> } <nl> } <nl> return false, nil <nl> - }) <nl> - <nl> - listOptions := metav1.ListOptions{ <nl> - LabelSelector: labels.SelectorFromSet(labels.Set{\"k8s-app\": \"machine-config-daemon\"}).String(), <nl> + }); err != nil { <nl> + t.Errorf(\"machine config hasn't been picked by the pool: %v\", err) <nl> } <nl> - err = wait.Poll(3*time.Second, 5*time.Minute, func() (bool, error) { <nl> - mcdList, err := k.CoreV1().Pods(\"openshift-machine-config-operator\").List(listOptions) <nl> + visited := make(map[string]bool) <nl> + if err := wait.Poll(2*time.Second, 5*time.Minute, func() (bool, error) { <nl> + nodes, err := getNodesByRole(k, \"worker\") <nl> if err != nil { <nl> return false, err <nl> } <nl> - <nl> - for _, pod := range mcdList.Items { <nl> - res, err := k.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &v1.PodLogOptions{}).DoRaw() <nl> - if err != nil { <nl> - // do not error out, we may be rebooting, that's why we list at every iteration <nl> - return false, nil <nl> + for _, node := range nodes { <nl> + if visited[node.Name] { <nl> + continue <nl> } <nl> - for _, line := range strings.Split(string(res), \"\\n\") { <nl> - if strings.Contains(line, \"completed update for config \"+newMCName) { <nl> + if node.Annotations[constants.CurrentMachineConfigAnnotationKey] == newMCName { <nl> + visited[node.Name] = true <nl> + if len(visited) == len(nodes) { <nl> return true, nil <nl> } <nl> + continue <nl> } <nl> } <nl> return false, nil <nl> - }) <nl> - if err != nil { <nl> + }); err != nil { <nl> t.Errorf(\"machine config didn't result in file being on any worker: %v\", err) <nl> } <nl> } <nl> + <nl> +func getNodesByRole(k kubernetes.Interface, role string) ([]v1.Node, error) { <nl> + listOptions := metav1.ListOptions{ <nl> + LabelSelector: labels.SelectorFromSet(labels.Set{fmt.Sprintf(\"node-role.kubernetes.io/%s\", role): \"\"}).String(), <nl> + } <nl> + nodes, err := k.CoreV1().Nodes().List(listOptions) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + return nodes.Items, nil <nl> +} <nl> ", "msg": "test/e2e: check MC deployed on every node\nTestMCDeployed wasn't checking that the MC was indeed deployed on every\nnode (worker). Fix that by checking every node. The test is going to\ntake a little longer but it's more robust as well."}
{"diff_id": 2444, "repo": "openshift/machine-config-operator", "sha": "2a582a91fc53fa35726c84a17229a68849025182", "time": "16.02.2019 18:58:35", "diff": "mmm a / pkg/operator/status_test.go <nl> ppp b / pkg/operator/status_test.go <nl>@@ -576,3 +576,29 @@ func TestOperatorSyncStatus(t *testing.T) { <nl> } <nl> } <nl> } <nl> + <nl> +func TestInClusterBringUpStayOnErr(t *testing.T) { <nl> + optr := &Operator{} <nl> + optr.vStore = newVersionStore() <nl> + optr.vStore.Set(\"operator\", \"test-version\") <nl> + optr.mcpLister = &mockMCPLister{} <nl> + co := &configv1.ClusterOperator{} <nl> + optr.configClient = &mockClusterOperatorsClient{co: co} <nl> + optr.inClusterBringup = true <nl> + <nl> + fn1 := func(config renderConfig) error { <nl> + return errors.New(\"mocked fn1\") <nl> + } <nl> + err := optr.syncAll(renderConfig{}, []syncFunc{{name: \"mock1\", fn: fn1}}) <nl> + assert.NotNil(t, err, \"expected syncAll to fail\") <nl> + <nl> + assert.True(t, optr.inClusterBringup) <nl> + <nl> + fn1 = func(config renderConfig) error { <nl> + return nil <nl> + } <nl> + err = optr.syncAll(renderConfig{}, []syncFunc{{name: \"mock1\", fn: fn1}}) <nl> + assert.Nil(t, err, \"expected syncAll to pass\") <nl> + <nl> + assert.False(t, optr.inClusterBringup) <nl> +} <nl> \\ No newline at end of file <nl> ", "msg": "pkg/operator: add test for inClusterBringUp staying on error"}
{"diff_id": 2451, "repo": "openshift/machine-config-operator", "sha": "fecb35b3ae49750493f11290fd34049099d295b7", "time": "25.02.2019 15:05:05", "diff": "mmm a / cmd/machine-config-operator/bootstrap.go <nl> ppp b / cmd/machine-config-operator/bootstrap.go <nl>@@ -38,6 +38,7 @@ var ( <nl> mcdImage string <nl> etcdImage string <nl> setupEtcdEnvImage string <nl> + infraImage string <nl> destinationDir string <nl> } <nl> ) <nl> @@ -62,6 +63,7 @@ func init() { <nl> bootstrapCmd.MarkFlagRequired(\"etcd-image\") <nl> bootstrapCmd.PersistentFlags().StringVar(&bootstrapOpts.setupEtcdEnvImage, \"setup-etcd-env-image\", \"\", \"Image for Setup Etcd Environment.\") <nl> bootstrapCmd.MarkFlagRequired(\"setup-etcd-env-image\") <nl> + bootstrapCmd.PersistentFlags().StringVar(&bootstrapOpts.infraImage, \"infra-image\", \"quay.io/openshift/origin-pod:v4.0\", \"Image for Infra Containers.\") <nl> bootstrapCmd.PersistentFlags().StringVar(&bootstrapOpts.configFile, \"config-file\", \"\", \"ClusterConfig ConfigMap file.\") <nl> bootstrapCmd.MarkFlagRequired(\"config-file\") <nl> bootstrapCmd.PersistentFlags().StringVar(&bootstrapOpts.infraConfigFile, \"infra-config-file\", \"/assets/manifests/cluster-infrastructure-02-config.yml\", \"File containing infrastructure.config.openshift.io manifest.\") <nl> ", "msg": "Add infra-image flag to bootstrap.go as no-op\nAdding this, so we can get the installer PR in first\nbefore landing in\nThis is from an approved and lgtm'ed PR"}
{"diff_id": 2453, "repo": "openshift/machine-config-operator", "sha": "3488ac8c49e770f0b1bf2eb6fc13fc119075938b", "time": "25.02.2019 13:27:30", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -18,6 +18,7 @@ import ( <nl> \"time\" <nl> ignv2_2types \"github.com/coreos/ignition/config/v2_2/types\" <nl> + \"github.com/coreos/ignition/config/validate\" <nl> \"github.com/golang/glog\" <nl> drain \"github.com/openshift/kubernetes-drain\" <nl> mcfgv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" <nl> @@ -206,6 +207,11 @@ func (dn *Daemon) reconcilable(oldConfig, newConfig *mcfgv1.MachineConfig) error <nl> newIgn := newConfig.Spec.Config <nl> // Ignition section <nl> + // First check if this is a generally valid Ignition Config <nl> + rpt := validate.ValidateWithoutSource(reflect.ValueOf(newIgn)) <nl> + if rpt.IsFatal() { <nl> + return errors.Errorf(\"Invalid Ignition config found: %v\", rpt) <nl> + } <nl> // if the config versions are different, all bets are off. this probably <nl> // shouldn't happen, but if it does, we can't deal with it. <nl> ", "msg": "add ign validation check to reconcilable()\nAdd a check using Ignition's internal validation function to ensure\nthat machineconfigs contain valid Ignition configs. If Ignition config\nis invalid, a message containing the error report will be logged.\nCloses:"}
{"diff_id": 2458, "repo": "openshift/machine-config-operator", "sha": "91451ecd1492c117a00bf487b912581c4823ba15", "time": "08.03.2019 09:57:28", "diff": "mmm a / pkg/controller/kubelet-config/kubelet_config_controller.go <nl> ppp b / pkg/controller/kubelet-config/kubelet_config_controller.go <nl>@@ -149,10 +149,21 @@ func (ctrl *Controller) Run(workers int, stopCh <-chan struct{}) { <nl> <-stopCh <nl> } <nl> +func kubeletConfigTriggerObjectChange(old, new *mcfgv1.KubeletConfig) bool { <nl> + if old.DeletionTimestamp != new.DeletionTimestamp { <nl> + return true <nl> + } <nl> + if !reflect.DeepEqual(old.Spec, new.Spec) { <nl> + return true <nl> + } <nl> + return false <nl> +} <nl> + <nl> func (ctrl *Controller) updateKubeletConfig(old, cur interface{}) { <nl> oldConfig := old.(*mcfgv1.KubeletConfig) <nl> newConfig := cur.(*mcfgv1.KubeletConfig) <nl> - if !reflect.DeepEqual(oldConfig, newConfig) { <nl> + <nl> + if kubeletConfigTriggerObjectChange(oldConfig, newConfig) { <nl> glog.V(4).Infof(\"Update KubeletConfig %s\", oldConfig.Name) <nl> ctrl.enqueueKubeletConfig(newConfig) <nl> } <nl> @@ -281,12 +292,15 @@ func (ctrl *Controller) generateOriginalKubeletConfig(role string) (*ignv2_2type <nl> } <nl> func (ctrl *Controller) syncStatusOnly(cfg *mcfgv1.KubeletConfig, err error, args ...interface{}) error { <nl> - if cfg.GetGeneration() != cfg.Status.ObservedGeneration { <nl> - cfg.Status.ObservedGeneration = cfg.GetGeneration() <nl> - cfg.Status.Conditions = append(cfg.Status.Conditions, wrapErrorWithCondition(err, args...)) <nl> + return retry.RetryOnConflict(updateBackoff, func() error { <nl> + newcfg, err := ctrl.mckLister.Get(cfg.Name) <nl> + if err != nil { <nl> + return err <nl> } <nl> - _, lerr := ctrl.client.MachineconfigurationV1().KubeletConfigs().UpdateStatus(cfg) <nl> + newcfg.Status.Conditions = append(newcfg.Status.Conditions, wrapErrorWithCondition(err, args...)) <nl> + _, lerr := ctrl.client.MachineconfigurationV1().KubeletConfigs().UpdateStatus(newcfg) <nl> return lerr <nl> + }) <nl> } <nl> // syncKubeletConfig will sync the kubeletconfig with the given key. <nl> @@ -416,12 +430,21 @@ func (ctrl *Controller) syncKubeletConfig(key string) error { <nl> } <nl> func (ctrl *Controller) popFinalizerFromKubeletConfig(kc *mcfgv1.KubeletConfig) error { <nl> - curJSON, err := json.Marshal(kc) <nl> + return retry.RetryOnConflict(updateBackoff, func() error { <nl> + newcfg, err := ctrl.mckLister.Get(kc.Name) <nl> + if errors.IsNotFound(err) { <nl> + return nil <nl> + } <nl> if err != nil { <nl> return err <nl> } <nl> - kcTmp := kc.DeepCopy() <nl> + curJSON, err := json.Marshal(newcfg) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + <nl> + kcTmp := newcfg.DeepCopy() <nl> kcTmp.Finalizers = append(kc.Finalizers[:0], kc.Finalizers[1:]...) <nl> modJSON, err := json.Marshal(kcTmp) <nl> @@ -433,34 +456,38 @@ func (ctrl *Controller) popFinalizerFromKubeletConfig(kc *mcfgv1.KubeletConfig) <nl> if err != nil { <nl> return err <nl> } <nl> - <nl> - return retry.RetryOnConflict(updateBackoff, func() error { <nl> - _, err = ctrl.client.MachineconfigurationV1().KubeletConfigs().Patch(kc.Name, types.MergePatchType, patch) <nl> + _, err = ctrl.client.MachineconfigurationV1().KubeletConfigs().Patch(newcfg.Name, types.MergePatchType, patch) <nl> return err <nl> }) <nl> } <nl> func (ctrl *Controller) addFinalizerToKubeletConfig(kc *mcfgv1.KubeletConfig, mc *mcfgv1.MachineConfig) error { <nl> - curJSON, err := json.Marshal(kc) <nl> + return retry.RetryOnConflict(updateBackoff, func() error { <nl> + newcfg, err := ctrl.mckLister.Get(kc.Name) <nl> + if errors.IsNotFound(err) { <nl> + return nil <nl> + } <nl> + if err != nil { <nl> + return err <nl> + } <nl> + <nl> + curJSON, err := json.Marshal(newcfg) <nl> if err != nil { <nl> return err <nl> } <nl> - kcTmp := kc.DeepCopy() <nl> + kcTmp := newcfg.DeepCopy() <nl> kcTmp.Finalizers = append(kcTmp.Finalizers, mc.Name) <nl> modJSON, err := json.Marshal(kcTmp) <nl> if err != nil { <nl> return err <nl> } <nl> - <nl> patch, err := jsonmergepatch.CreateThreeWayJSONMergePatch(curJSON, modJSON, curJSON) <nl> if err != nil { <nl> return err <nl> } <nl> - <nl> - return retry.RetryOnConflict(updateBackoff, func() error { <nl> - _, err := ctrl.client.MachineconfigurationV1().KubeletConfigs().Patch(kc.Name, types.MergePatchType, patch) <nl> + _, err = ctrl.client.MachineconfigurationV1().KubeletConfigs().Patch(newcfg.Name, types.MergePatchType, patch) <nl> return err <nl> }) <nl> } <nl> ", "msg": "kubelet-config: accurately detect object updates\nWe were processing a circular amount of update events (which included\nStatus updates). This patch detects a real change within the\nkubeletconfig.\nThis patch also fetches a fresh config so we limit the 'object has been\nupdated' messages."}
{"diff_id": 2462, "repo": "openshift/machine-config-operator", "sha": "780c918bbe09b9898be4d3a27dcabefd68719657", "time": "13.03.2019 15:25:02", "diff": "mmm a / pkg/daemon/rpm-ostree.go <nl> ppp b / pkg/daemon/rpm-ostree.go <nl>@@ -2,17 +2,16 @@ package daemon <nl> import ( <nl> \"encoding/json\" <nl> - \"errors\" <nl> \"fmt\" <nl> \"io/ioutil\" <nl> \"os\" <nl> + \"os/exec\" <nl> \"path/filepath\" <nl> \"strings\" <nl> \"time\" <nl> - \"github.com/coreos/go-systemd/dbus\" <nl> - \"github.com/coreos/go-systemd/sdjournal\" <nl> \"github.com/golang/glog\" <nl> + \"github.com/pkg/errors\" <nl> \"github.com/openshift/machine-config-operator/pkg/daemon/constants\" <nl> ) <nl> @@ -123,55 +122,9 @@ func (r *RpmOstreeClient) RunPivot(osImageURL string) error { <nl> defer close(journalStopCh) <nl> go followPivotJournalLogs(journalStopCh) <nl> - conn, err := dbus.NewSystemdConnection() <nl> + err := exec.Command(\"systemctl\", \"start\", \"pivot.service\").Run() <nl> if err != nil { <nl> - return fmt.Errorf(\"error creating systemd conn: %v\", err) <nl> - } <nl> - defer conn.Close() <nl> - <nl> - // start job <nl> - status := make(chan string) <nl> - _, err = conn.StartUnit(\"pivot.service\", \"fail\", status) <nl> - if err != nil { <nl> - return fmt.Errorf(\"error starting job: %v\", err) <nl> - } <nl> - <nl> - select { <nl> - case st := <-status: <nl> - if st != \"done\" { <nl> - return fmt.Errorf(\"error queuing start job; got %s\", st) <nl> - } <nl> - case <-time.After(5 * time.Minute): <nl> - return fmt.Errorf(\"timed out waiting for start job\") <nl> - } <nl> - <nl> - // wait until inactive/failed <nl> - var failed bool <nl> - eventsCh, errCh := conn.SubscribeUnits(time.Second) <nl> -Outer: <nl> - for { <nl> - select { <nl> - case e := <-eventsCh: <nl> - if st, ok := e[\"pivot.service\"]; ok { <nl> - // If the service is disabled, systemd won't keep around <nl> - // metadata about whether it failed/passed, etc... The bindings <nl> - // signal this by just using a `nil` status since it dropped out <nl> - // of `ListUnits()`. <nl> - if st == nil { <nl> - return errors.New(\"got nil while waiting for pivot; is the service enabled?\") <nl> - } <nl> - failed = st.ActiveState == \"failed\" <nl> - if failed || st.ActiveState == \"inactive\" { <nl> - break Outer <nl> - } <nl> - } <nl> - case f := <-errCh: <nl> - return fmt.Errorf(\"error while waiting for pivot: %v\", f) <nl> - } <nl> - } <nl> - <nl> - if failed { <nl> - return errors.New(\"pivot service did not exit successfully\") <nl> + return errors.Wrapf(err, \"failed to start pivot.service\") <nl> } <nl> return nil <nl> } <nl> @@ -179,37 +132,17 @@ Outer: <nl> // Proxy pivot and rpm-ostree daemon journal logs until told to stop. Warns if <nl> // we encounter an error. <nl> func followPivotJournalLogs(stopCh <-chan time.Time) { <nl> - reader, err := sdjournal.NewJournalReader( <nl> - sdjournal.JournalReaderConfig{ <nl> - Since: time.Duration(1) * time.Second, <nl> - Matches: []sdjournal.Match{ <nl> - { <nl> - Field: sdjournal.SD_JOURNAL_FIELD_SYSTEMD_UNIT, <nl> - Value: pivotUnit, <nl> - }, <nl> - { <nl> - Field: sdjournal.SD_JOURNAL_FIELD_SYSTEMD_UNIT, <nl> - Value: rpmostreedUnit, <nl> - }, <nl> - }, <nl> - Formatter: func(entry *sdjournal.JournalEntry) (string, error) { <nl> - msg, ok := entry.Fields[\"MESSAGE\"] <nl> - if !ok { <nl> - return \"\", fmt.Errorf(\"missing MESSAGE field in entry\") <nl> - } <nl> - return fmt.Sprintf(\"%s: %s\\n\", entry.Fields[sdjournal.SD_JOURNAL_FIELD_SYSTEMD_UNIT], msg), nil <nl> - }, <nl> - }) <nl> - if err != nil { <nl> - glog.Warningf(\"Failed to open journal: %v\", err) <nl> - return <nl> - } <nl> - defer reader.Close() <nl> - <nl> - // We're kinda abusing the API here. The idea is that the stop channel is <nl> - // used with time.After(), hence the `chan time.Time` type. But really, it <nl> - // works to just never output anything and just close it as we do here. <nl> - if err := reader.Follow(stopCh, os.Stdout); err != sdjournal.ErrExpired { <nl> - glog.Warningf(\"Failed to follow journal: %v\", err) <nl> - } <nl> + cmd := exec.Command(\"journalctl\", \"-f\", \"-b\", \"-o\", \"cat\", <nl> + \"-u\", \"rpm-ostreed\", <nl> + \"-u\", \"pivot\") <nl> + cmd.Stdout = os.Stdout <nl> + cmd.Stderr = os.Stderr <nl> + if err := cmd.Start(); err != nil { <nl> + glog.Fatal(err) <nl> + } <nl> + <nl> + go func() { <nl> + <- stopCh <nl> + cmd.Process.Kill() <nl> + }() <nl> } <nl> ", "msg": "rpmostree: Just exec systemctl\nThis way we're not depending on shared libraries in the host.\nAlso IMO...this is just simpler.  We don't care about being a\n\"pure go\" program, let's not go into using APIs unless we really\nneed to."}
{"diff_id": 2463, "repo": "openshift/machine-config-operator", "sha": "ed537dd4cd4c63e6fe60d7a8486c01315cf50f0a", "time": "14.03.2019 18:45:53", "diff": "mmm a / pkg/controller/render/render_controller_test.go <nl> ppp b / pkg/controller/render/render_controller_test.go <nl>@@ -7,6 +7,7 @@ import ( <nl> ignv2_2types \"github.com/coreos/ignition/config/v2_2/types\" <nl> mcfgv1 \"github.com/openshift/machine-config-operator/pkg/apis/machineconfiguration.openshift.io/v1\" <nl> + ctrlcommon \"github.com/openshift/machine-config-operator/pkg/controller/common\" <nl> \"github.com/openshift/machine-config-operator/pkg/generated/clientset/versioned/fake\" <nl> informers \"github.com/openshift/machine-config-operator/pkg/generated/informers/externalversions\" <nl> corev1 \"k8s.io/api/core/v1\" <nl> @@ -65,12 +66,14 @@ func newMachineConfig(name string, labels map[string]string, osurl string, files <nl> if labels == nil { <nl> labels = map[string]string{} <nl> } <nl> + ignCfg := ctrlcommon.NewIgnConfig() <nl> + ignCfg.Storage.Files = files <nl> return &mcfgv1.MachineConfig{ <nl> TypeMeta: metav1.TypeMeta{APIVersion: mcfgv1.SchemeGroupVersion.String()}, <nl> ObjectMeta: metav1.ObjectMeta{Name: name, Labels: labels, UID: types.UID(utilrand.String(5))}, <nl> Spec: mcfgv1.MachineConfigSpec{ <nl> OSImageURL: osurl, <nl> - Config: ignv2_2types.Config{Storage: ignv2_2types.Storage{Files: files}}, <nl> + Config: ignCfg, <nl> }, <nl> } <nl> } <nl> @@ -323,6 +326,43 @@ func TestDoNothing(t *testing.T) { <nl> f.run(getKey(mcp, t)) <nl> } <nl> +func TestGetMachineConfigsForPool(t *testing.T) { <nl> + masterPool := newMachineConfigPool(\"test-cluster-master\", metav1.AddLabelToSelector(&metav1.LabelSelector{}, \"node-role\", \"master\"), \"\") <nl> + files := []ignv2_2types.File{{ <nl> + Node: ignv2_2types.Node{ <nl> + Path: \"/dummy/0\", <nl> + }, <nl> + }, { <nl> + Node: ignv2_2types.Node{ <nl> + Path: \"/dummy/1\", <nl> + }, <nl> + }, { <nl> + Node: ignv2_2types.Node{ <nl> + Path: \"/dummy/2\", <nl> + }, <nl> + }} <nl> + mcs := []*mcfgv1.MachineConfig{ <nl> + newMachineConfig(\"00-test-cluster-master\", map[string]string{\"node-role\": \"master\"}, \"dummy://\", []ignv2_2types.File{files[0]}), <nl> + newMachineConfig(\"05-extra-master\", map[string]string{\"node-role\": \"master\"}, \"dummy://1\", []ignv2_2types.File{files[1]}), <nl> + newMachineConfig(\"00-test-cluster-worker\", map[string]string{\"node-role\": \"worker\"}, \"dummy://2\", []ignv2_2types.File{files[2]}), <nl> + } <nl> + masterConfigs, err := getMachineConfigsForPool(masterPool, mcs) <nl> + if err != nil { <nl> + t.Fatalf(\"expected no error, got: %v\", err) <nl> + } <nl> + // check that only the master MCs were selected <nl> + if len(masterConfigs) != 2 { <nl> + t.Fatalf(\"expected to select 2 configs for pool master got: %v\", len(masterConfigs)) <nl> + } <nl> + <nl> + // search for a worker config in an array of MCs with no worker configs <nl> + workerPool := newMachineConfigPool(\"test-cluster-worker\", metav1.AddLabelToSelector(&metav1.LabelSelector{}, \"node-role\", \"worker\"), \"\") <nl> + _, err = getMachineConfigsForPool(workerPool, mcs[:2]) <nl> + if err == nil { <nl> + t.Fatalf(\"expected error, no worker configs found\") <nl> + } <nl> +} <nl> + <nl> func getKey(config *mcfgv1.MachineConfigPool, t *testing.T) string { <nl> key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(config) <nl> if err != nil { <nl> ", "msg": "mcc: add unit test for GetMachineConfigsForPool() in render_controller.go"}
{"diff_id": 2473, "repo": "openshift/machine-config-operator", "sha": "584892765db040b1cb655c80da0cc34f5213b0e1", "time": "02.04.2019 12:26:14", "diff": "mmm a / test/e2e/mcd_test.go <nl> ppp b / test/e2e/mcd_test.go <nl>@@ -124,7 +124,7 @@ func TestMCDeployed(t *testing.T) { <nl> if err := wait.Poll(2*time.Second, 10*time.Minute, func() (bool, error) { <nl> nodes, err := getNodesByRole(cs, \"worker\") <nl> if err != nil { <nl> - return false, err <nl> + return false, nil <nl> } <nl> for _, node := range nodes { <nl> if visited[node.Name] { <nl> ", "msg": "test/e2e: skip error check on MC deployed test\nCall can fail if network is off try reaching the nodes as happened in\nso just return  false, nil as we do in other places where we know there\nmay be a network error."}
{"diff_id": 2492, "repo": "openshift/machine-config-operator", "sha": "87f4cc96fc9e8744a1a014674a3739ad6ce090e0", "time": "02.05.2019 15:07:59", "diff": "mmm a / pkg/controller/node/node_controller.go <nl> ppp b / pkg/controller/node/node_controller.go <nl>@@ -200,7 +200,14 @@ func (ctrl *Controller) updateNode(old, cur interface{}) { <nl> if pool == nil { <nl> return <nl> } <nl> + <nl> + // Specifically log when a node has completed an update so the MCC logs are a useful central aggregate of state changes <nl> + if oldNode.Annotations[daemonconsts.CurrentMachineConfigAnnotationKey] != oldNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey] && <nl> + curNode.Annotations[daemonconsts.CurrentMachineConfigAnnotationKey] == curNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey] { <nl> + glog.Infof(\"Pool %s: node %s has completed update to %s\", pool.Name, curNode.Name, curNode.Annotations[daemonconsts.DesiredMachineConfigAnnotationKey]) <nl> + } else { <nl> glog.V(4).Infof(\"Node %s updated\", curNode.Name) <nl> + } <nl> ctrl.enqueueMachineConfigPool(pool) <nl> } <nl> ", "msg": "controller: Log when a node has been updated\nThe MCC logs show when we tell a node to start, but don't show us\nwhen it's done.  This helps the MCC logs be a centralized\naggregate of events.\nDoing this now to aid debugging slowness in updates in our `e2e-aws-op`\ntest suite."}
{"diff_id": 2503, "repo": "openshift/machine-config-operator", "sha": "826f246b82b79533540effda8b3958895d09f2df", "time": "08.05.2019 19:20:07", "diff": "mmm a / pkg/daemon/daemon_test.go <nl> ppp b / pkg/daemon/daemon_test.go <nl>@@ -128,38 +128,6 @@ func TestCompareOSImageURL(t *testing.T) { <nl> } <nl> } <nl> -func TestDaemonOnceFromNoPanic(t *testing.T) { <nl> - if _, err := os.Stat(\"/proc/sys/kernel/random/boot_id\"); os.IsNotExist(err) { <nl> - t.Skip(\"we're not on linux\") <nl> - } <nl> - <nl> - exitCh := make(chan error) <nl> - defer close(exitCh) <nl> - stopCh := make(chan struct{}) <nl> - defer close(stopCh) <nl> - <nl> - // This is how a onceFrom daemon is initialized <nl> - // and it shouldn't panic assuming kubeClient is there <nl> - dn, err := New( <nl> - \"/\", <nl> - \"testnodename\", <nl> - \"testos\", <nl> - NewNodeUpdaterClient(), <nl> - \"\", <nl> - \"test\", <nl> - false, <nl> - nil, <nl> - k8sfake.NewSimpleClientset(), <nl> - false, <nl> - \"\", <nl> - nil, <nl> - exitCh, <nl> - stopCh, <nl> - ) <nl> - require.Nil(t, err) <nl> - require.NotPanics(t, func() { dn.triggerUpdateWithMachineConfig(&mcfgv1.MachineConfig{}, &mcfgv1.MachineConfig{}) }) <nl> -} <nl> - <nl> type fixture struct { <nl> t *testing.T <nl> ", "msg": "daemon: Remove dummy OnceFrom Test\nNow that the daemon tries to talk to the systemd journal, this unit\ntest hangs in my dev environment.  We can't really support this\nwithout mocking a lot more, and it's not really covering much\ninteresting right now.  Delete it."}
{"diff_id": 2507, "repo": "openshift/machine-config-operator", "sha": "382261745e005a93b8d9fd1534bb78a152713492", "time": "14.05.2019 14:03:35", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -778,7 +778,6 @@ func (dn *Daemon) getPendingConfig() (string, error) { <nl> if !os.IsNotExist(err) { <nl> return \"\", errors.Wrapf(err, \"loading transient state\") <nl> } <nl> - dn.logSystem(\"error loading pending config %v\", err) <nl> return \"\", nil <nl> } <nl> var p pendingConfigState <nl> ", "msg": "daemon: Drop unnecessary logSystem\nWe show this log in the normal case where the file exists.\nI don't think we need to `logSystem` this anyways."}
{"diff_id": 2518, "repo": "openshift/machine-config-operator", "sha": "c0f7151977a2bc1994d635bc032dbd94a61918f5", "time": "03.06.2019 09:52:12", "diff": "mmm a / pkg/controller/kubelet-config/kubelet_config_controller.go <nl> ppp b / pkg/controller/kubelet-config/kubelet_config_controller.go <nl>@@ -2,6 +2,7 @@ package kubeletconfig <nl> import ( <nl> \"encoding/json\" <nl> + \"errors\" <nl> \"fmt\" <nl> \"reflect\" <nl> \"time\" <nl> @@ -12,7 +13,7 @@ import ( <nl> \"github.com/vincent-petithory/dataurl\" <nl> v1 \"k8s.io/api/core/v1\" <nl> - \"k8s.io/apimachinery/pkg/api/errors\" <nl> + macherrors \"k8s.io/apimachinery/pkg/api/errors\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> \"k8s.io/apimachinery/pkg/labels\" <nl> \"k8s.io/apimachinery/pkg/types\" <nl> @@ -59,6 +60,8 @@ var updateBackoff = wait.Backoff{ <nl> Jitter: 1.0, <nl> } <nl> +var errCouldNotFindMCPSet = errors.New(\"could not find any MachineConfigPool set for KubeletConfig\") <nl> + <nl> // A list of fields a user cannot set within the KubeletConfig CR. If a user <nl> // were to set these values, then the system may become unrecoverable (ie: not <nl> // recover after a reboot). <nl> @@ -236,7 +239,7 @@ func (ctrl *Controller) cascadeDelete(cfg *mcfgv1.KubeletConfig) error { <nl> } <nl> mcName := cfg.GetFinalizers()[0] <nl> err := ctrl.client.Machineconfiguration().MachineConfigs().Delete(mcName, &metav1.DeleteOptions{}) <nl> - if err != nil && !errors.IsNotFound(err) { <nl> + if err != nil && !macherrors.IsNotFound(err) { <nl> return err <nl> } <nl> if err := ctrl.popFinalizerFromKubeletConfig(cfg); err != nil { <nl> @@ -284,7 +287,7 @@ func (ctrl *Controller) processNextWorkItem() bool { <nl> } <nl> func (ctrl *Controller) handleErr(err error, key interface{}) { <nl> - if err == nil { <nl> + if err == nil || err == errCouldNotFindMCPSet { <nl> ctrl.queue.Forget(key) <nl> return <nl> } <nl> @@ -373,7 +376,7 @@ func (ctrl *Controller) syncKubeletConfig(key string) error { <nl> // Fetch the KubeletConfig <nl> cfg, err := ctrl.mckLister.Get(name) <nl> - if errors.IsNotFound(err) { <nl> + if macherrors.IsNotFound(err) { <nl> glog.V(2).Infof(\"KubeletConfig %v has been deleted\", key) <nl> return nil <nl> } <nl> @@ -415,7 +418,7 @@ func (ctrl *Controller) syncKubeletConfig(key string) error { <nl> } <nl> features, err := ctrl.featLister.Get(clusterFeatureInstanceName) <nl> - if errors.IsNotFound(err) { <nl> + if macherrors.IsNotFound(err) { <nl> features = createNewDefaultFeatureGate() <nl> } else if err != nil { <nl> glog.V(2).Infof(\"%v\", err) <nl> @@ -434,10 +437,10 @@ func (ctrl *Controller) syncKubeletConfig(key string) error { <nl> // Get MachineConfig <nl> managedKey := getManagedKubeletConfigKey(pool) <nl> mc, err := ctrl.client.Machineconfiguration().MachineConfigs().Get(managedKey, metav1.GetOptions{}) <nl> - if err != nil && !errors.IsNotFound(err) { <nl> + if err != nil && !macherrors.IsNotFound(err) { <nl> return ctrl.syncStatusOnly(cfg, err, \"could not find MachineConfig: %v\", managedKey) <nl> } <nl> - isNotFound := errors.IsNotFound(err) <nl> + isNotFound := macherrors.IsNotFound(err) <nl> // Generate the original KubeletConfig <nl> originalKubeletIgn, err := ctrl.generateOriginalKubeletConfig(role) <nl> if err != nil { <nl> @@ -502,7 +505,7 @@ func (ctrl *Controller) syncKubeletConfig(key string) error { <nl> func (ctrl *Controller) popFinalizerFromKubeletConfig(kc *mcfgv1.KubeletConfig) error { <nl> return retry.RetryOnConflict(updateBackoff, func() error { <nl> newcfg, err := ctrl.mckLister.Get(kc.Name) <nl> - if errors.IsNotFound(err) { <nl> + if macherrors.IsNotFound(err) { <nl> return nil <nl> } <nl> if err != nil { <nl> @@ -538,7 +541,7 @@ func (ctrl *Controller) patchKubeletConfigs(name string, patch []byte) error { <nl> func (ctrl *Controller) addFinalizerToKubeletConfig(kc *mcfgv1.KubeletConfig, mc *mcfgv1.MachineConfig) error { <nl> return retry.RetryOnConflict(updateBackoff, func() error { <nl> newcfg, err := ctrl.mckLister.Get(kc.Name) <nl> - if errors.IsNotFound(err) { <nl> + if macherrors.IsNotFound(err) { <nl> return nil <nl> } <nl> if err != nil { <nl> @@ -586,7 +589,7 @@ func (ctrl *Controller) getPoolsForKubeletConfig(config *mcfgv1.KubeletConfig) ( <nl> } <nl> if len(pools) == 0 { <nl> - return nil, fmt.Errorf(\"could not find any MachineConfigPool set for KubeletConfig %s\", config.Name) <nl> + return nil, errCouldNotFindMCPSet <nl> } <nl> return pools, nil <nl> ", "msg": "BZ1714769: fix kubeletconfig zero MCP errors\nDo not requeue the key if there are zero MCPs. Prevents multiple status\nupdates and will not succeed.\nref:"}
{"diff_id": 2568, "repo": "openshift/machine-config-operator", "sha": "ebfd797ff189821f6fe93d30c36cbc0c732c96ef", "time": "15.07.2019 18:53:35", "diff": "mmm a / pkg/controller/render/render_controller_test.go <nl> ppp b / pkg/controller/render/render_controller_test.go <nl>@@ -265,10 +265,12 @@ func TestIgnValidationGenerateRenderedMachineConfig(t *testing.T) { <nl> mcp := helpers.NewMachineConfigPool(\"test-cluster-master\", helpers.MasterSelector, nil, \"\") <nl> files := []igntypes.File{{ <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/0\", <nl> }, <nl> }, { <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/1\", <nl> }, <nl> }} <nl> @@ -279,15 +281,20 @@ func TestIgnValidationGenerateRenderedMachineConfig(t *testing.T) { <nl> cc := newControllerConfig(ctrlcommon.ControllerConfigName) <nl> _, err := generateRenderedMachineConfig(mcp, mcs, cc) <nl> - if err != nil { <nl> - t.Fatalf(\"expected no error. Got: %v\", err) <nl> - } <nl> + require.Nil(t, err) <nl> + // verify that an invalid igntion config (here a config with content and an empty version, <nl> + // will fail validation <nl> mcs[1].Spec.Config.Ignition.Version = \"\" <nl> _, err = generateRenderedMachineConfig(mcp, mcs, cc) <nl> - if err == nil { <nl> - t.Fatalf(\"expected error. mcs contains a machine config with invalid ignconfig version\") <nl> - } <nl> + require.NotNil(t, err) <nl> + <nl> + // verify that a machine config with no ignition content will not fail validation <nl> + mcs[1].Spec.Config = igntypes.Config{} <nl> + mcs[1].Spec.KernelArguments = append(mcs[1].Spec.KernelArguments, \"test1\") <nl> + _, err = generateRenderedMachineConfig(mcp, mcs, cc) <nl> + require.Nil(t, err) <nl> + <nl> } <nl> func TestUpdatesGeneratedMachineConfig(t *testing.T) { <nl> @@ -295,10 +302,12 @@ func TestUpdatesGeneratedMachineConfig(t *testing.T) { <nl> mcp := helpers.NewMachineConfigPool(\"test-cluster-master\", helpers.MasterSelector, nil, \"\") <nl> files := []igntypes.File{{ <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/0\", <nl> }, <nl> }, { <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/1\", <nl> }, <nl> }} <nl> @@ -359,10 +368,12 @@ func TestDoNothing(t *testing.T) { <nl> mcp := helpers.NewMachineConfigPool(\"test-cluster-master\", helpers.MasterSelector, nil, \"\") <nl> files := []igntypes.File{{ <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/0\", <nl> }, <nl> }, { <nl> Node: igntypes.Node{ <nl> + Filesystem: \"root\", <nl> Path: \"/dummy/1\", <nl> }, <nl> }} <nl> ", "msg": "mcc: update rendercontroller unit test to cover empty ign config\nalso update existing unit tests in file to pass validation"}
{"diff_id": 2572, "repo": "openshift/machine-config-operator", "sha": "3f2b858b28d6c2ca906f8cd788d4c915404ef24c", "time": "16.07.2019 15:58:51", "diff": "mmm a / pkg/controller/template/template_controller_test.go <nl> ppp b / pkg/controller/template/template_controller_test.go <nl>@@ -267,7 +267,7 @@ func TestCreatesMachineConfigs(t *testing.T) { <nl> } <nl> rcc := cc.DeepCopy() <nl> rcc.Status.ObservedGeneration = 1 <nl> - rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version 0.0.0-was-not-built-properly\"}} <nl> + rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version v0.0.0-was-not-built-properly\"}} <nl> f.expectUpdateControllerConfigStatus(rcc) <nl> f.expectGetSecretAction(ps) <nl> @@ -278,7 +278,7 @@ func TestCreatesMachineConfigs(t *testing.T) { <nl> ccc := cc.DeepCopy() <nl> ccc.Status.ObservedGeneration = 1 <nl> ccc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{ <nl> - {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version 0.0.0-was-not-built-properly\"}, <nl> + {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version v0.0.0-was-not-built-properly\"}, <nl> {Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionFalse}, <nl> {Type: mcfgv1.TemplateControllerFailing, Status: corev1.ConditionFalse}, <nl> } <nl> @@ -306,7 +306,7 @@ func TestDoNothing(t *testing.T) { <nl> rcc := cc.DeepCopy() <nl> rcc.Status.ObservedGeneration = 1 <nl> - rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version 0.0.0-was-not-built-properly\"}} <nl> + rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version v0.0.0-was-not-built-properly\"}} <nl> f.expectUpdateControllerConfigStatus(rcc) <nl> f.expectGetSecretAction(ps) <nl> for idx := range mcs { <nl> @@ -315,7 +315,7 @@ func TestDoNothing(t *testing.T) { <nl> ccc := cc.DeepCopy() <nl> ccc.Status.ObservedGeneration = 1 <nl> ccc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{ <nl> - {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version 0.0.0-was-not-built-properly\"}, <nl> + {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version v0.0.0-was-not-built-properly\"}, <nl> {Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionFalse}, <nl> {Type: mcfgv1.TemplateControllerFailing, Status: corev1.ConditionFalse}, <nl> } <nl> @@ -343,7 +343,7 @@ func TestRecreateMachineConfig(t *testing.T) { <nl> rcc := cc.DeepCopy() <nl> rcc.Status.ObservedGeneration = 1 <nl> - rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version 0.0.0-was-not-built-properly\"}} <nl> + rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version v0.0.0-was-not-built-properly\"}} <nl> f.expectUpdateControllerConfigStatus(rcc) <nl> f.expectGetSecretAction(ps) <nl> @@ -354,7 +354,7 @@ func TestRecreateMachineConfig(t *testing.T) { <nl> ccc := cc.DeepCopy() <nl> ccc.Status.ObservedGeneration = 1 <nl> ccc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{ <nl> - {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version 0.0.0-was-not-built-properly\"}, <nl> + {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version v0.0.0-was-not-built-properly\"}, <nl> {Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionFalse}, <nl> {Type: mcfgv1.TemplateControllerFailing, Status: corev1.ConditionFalse}, <nl> } <nl> @@ -387,7 +387,7 @@ func TestUpdateMachineConfig(t *testing.T) { <nl> } <nl> rcc := cc.DeepCopy() <nl> rcc.Status.ObservedGeneration = 1 <nl> - rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version 0.0.0-was-not-built-properly\"}} <nl> + rcc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{{Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionTrue, Message: \"syncing towards (1) generation using controller version v0.0.0-was-not-built-properly\"}} <nl> f.expectUpdateControllerConfigStatus(rcc) <nl> f.expectGetSecretAction(ps) <nl> for idx := range expmcs { <nl> @@ -397,7 +397,7 @@ func TestUpdateMachineConfig(t *testing.T) { <nl> ccc := cc.DeepCopy() <nl> ccc.Status.ObservedGeneration = 1 <nl> ccc.Status.Conditions = []mcfgv1.ControllerConfigStatusCondition{ <nl> - {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version 0.0.0-was-not-built-properly\"}, <nl> + {Type: mcfgv1.TemplateControllerCompleted, Status: corev1.ConditionTrue, Message: \"sync completed towards (1) generation using controller version v0.0.0-was-not-built-properly\"}, <nl> {Type: mcfgv1.TemplateControllerRunning, Status: corev1.ConditionFalse}, <nl> {Type: mcfgv1.TemplateControllerFailing, Status: corev1.ConditionFalse}, <nl> } <nl> ", "msg": "mcc: update template controller unit tests for version changes"}
{"diff_id": 2583, "repo": "openshift/machine-config-operator", "sha": "12c7ea7b264340fcbf5d4d3bc42ec782a4092730", "time": "05.08.2019 10:39:17", "diff": "mmm a / cmd/gcp-routes-controller/run.go <nl> ppp b / cmd/gcp-routes-controller/run.go <nl>@@ -82,6 +82,7 @@ func runRunCmd(cmd *cobra.Command, args []string) error { <nl> errCh := make(chan error) <nl> tracker := &healthTracker{ <nl> + state: unknownTrackerState, <nl> ErrCh: errCh, <nl> SuccessThreshold: 2, <nl> FailureThreshold: 10, <nl> ", "msg": "cmd/gcp-routes-controller: init tracker state to unknown\ngolangci-lint doesn't allow deafult enum value to be unsed. :("}
{"diff_id": 2617, "repo": "openshift/machine-config-operator", "sha": "f23f4932b5705dd6f0f8a52b163883313cdb6fb0", "time": "04.11.2019 22:09:15", "diff": "mmm a / pkg/controller/node/status.go <nl> ppp b / pkg/controller/node/status.go <nl>@@ -201,13 +201,13 @@ func checkNodeReady(node *corev1.Node) error { <nl> // - NodeDiskPressure condition status is ConditionFalse, <nl> // - NodeNetworkUnavailable condition status is ConditionFalse. <nl> if cond.Type == corev1.NodeReady && cond.Status != corev1.ConditionTrue { <nl> - return fmt.Errorf(\"node %s is reporting NotReady\", node.Name) <nl> + return fmt.Errorf(\"node %s is reporting NotReady=%v\", node.Name, cond.Status) <nl> } <nl> if cond.Type == corev1.NodeDiskPressure && cond.Status != corev1.ConditionFalse { <nl> - return fmt.Errorf(\"node %s is reporting OutOfDisk\", node.Name) <nl> + return fmt.Errorf(\"node %s is reporting OutOfDisk=%v\", node.Name, cond.Status) <nl> } <nl> if cond.Type == corev1.NodeNetworkUnavailable && cond.Status != corev1.ConditionFalse { <nl> - return fmt.Errorf(\"node %s is reporting NetworkUnavailable\", node.Name) <nl> + return fmt.Errorf(\"node %s is reporting NetworkUnavailable=%v\", node.Name, cond.Status) <nl> } <nl> } <nl> // Ignore nodes that are marked unschedulable <nl> ", "msg": "controller: Log the exact status in conditions\nWe're seeing `OutOfDisk` in some recent runs and I'm wondering\nwhether it's `ConditionUnknown` appearing recently.  This should\nhelp us find out."}
{"diff_id": 2629, "repo": "openshift/machine-config-operator", "sha": "c48694ba5d73b2a92351c05d3fdb7168adb244e9", "time": "24.01.2020 16:11:15", "diff": "mmm a / test/e2e/mcd_test.go <nl> ppp b / test/e2e/mcd_test.go <nl>@@ -283,6 +283,47 @@ func TestKernelArguments(t *testing.T) { <nl> } <nl> } <nl> +func TestKernelType(t *testing.T) { <nl> + cs := framework.NewClientSet(\"\") <nl> + kernelType := &mcfgv1.MachineConfig{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: fmt.Sprintf(\"kerneltype-%s\", uuid.NewUUID()), <nl> + Labels: mcLabelForWorkers(), <nl> + }, <nl> + Spec: mcfgv1.MachineConfigSpec{ <nl> + Config: ctrlcommon.NewIgnConfig(), <nl> + KernelType: \"realtime\", <nl> + }, <nl> + } <nl> + <nl> + _, err := cs.MachineConfigs().Create(kernelType) <nl> + require.Nil(t, err) <nl> + t.Logf(\"Created %s\", kernelType.Name) <nl> + renderedConfig, err := waitForRenderedConfig(t, cs, \"worker\", kernelType.Name) <nl> + require.Nil(t, err) <nl> + if err := waitForPoolComplete(t, cs, \"worker\", renderedConfig); err != nil { <nl> + t.Fatal(err) <nl> + } <nl> + nodes, err := getNodesByRole(cs, \"worker\") <nl> + require.Nil(t, err) <nl> + for _, node := range nodes { <nl> + assert.Equal(t, node.Annotations[constants.CurrentMachineConfigAnnotationKey], renderedConfig) <nl> + assert.Equal(t, node.Annotations[constants.MachineConfigDaemonStateAnnotationKey], constants.MachineConfigDaemonStateDone) <nl> + mcd, err := mcdForNode(cs, &node) <nl> + require.Nil(t, err) <nl> + mcdName := mcd.ObjectMeta.Name <nl> + // Worker node should have RT kernel <nl> + cmd, err := exec.Command(\"oc\", \"rsh\", \"-n\", \"openshift-machine-config-operator\", mcdName, <nl> + \"uname\", \"-a\").CombinedOutput() <nl> + require.Nil(t, err) <nl> + kernelInfo := string(cmd) <nl> + if !strings.Contains(kernelInfo, \"PREEMPT RT\") { <nl> + t.Fatalf(\"Node %s doesn't have expected kernel\", node.Name) <nl> + } <nl> + t.Logf(\"Node %s has expected kernel\", node.Name) <nl> + } <nl> +} <nl> + <nl> func getNodesByRole(cs *framework.ClientSet, role string) ([]corev1.Node, error) { <nl> listOptions := metav1.ListOptions{ <nl> LabelSelector: labels.SelectorFromSet(labels.Set{fmt.Sprintf(\"node-role.kubernetes.io/%s\", role): \"\"}).String(), <nl> ", "msg": "test/e2e - Add e2e test for Realtime Kernel"}
{"diff_id": 2639, "repo": "openshift/machine-config-operator", "sha": "a5cdb5d3e285df34209e41de7651c7b45f241cd6", "time": "01.04.2020 00:35:47", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -161,9 +161,11 @@ func (dn *Daemon) drain() error { <nl> if err == wait.ErrWaitTimeout { <nl> failMsg := fmt.Sprintf(\"%d tries: %v\", backoff.Steps, lastErr) <nl> MCDDrainErr.WithLabelValues(failTime, failMsg).SetToCurrentTime() <nl> + dn.recorder.Eventf(getNodeRef(dn.node), corev1.EventTypeWarning, \"FailedToDrain\", failMsg) <nl> return errors.Wrapf(lastErr, \"failed to drain node (%d tries): %v\", backoff.Steps, err) <nl> } <nl> MCDDrainErr.WithLabelValues(failTime, err.Error()).SetToCurrentTime() <nl> + dn.recorder.Eventf(getNodeRef(dn.node), corev1.EventTypeWarning, \"FailedToDrain\", err.Error()) <nl> return errors.Wrap(err, \"failed to drain node\") <nl> } <nl> ", "msg": "pkg/daemon: Add event for drain failures"}
{"diff_id": 2655, "repo": "openshift/machine-config-operator", "sha": "cd0e7631e3142b2971bdc08f73a79ab228159eb3", "time": "27.05.2020 12:49:32", "diff": "mmm a / pkg/controller/common/helpers.go <nl> ppp b / pkg/controller/common/helpers.go <nl>@@ -33,7 +33,7 @@ func MergeMachineConfigs(configs []*mcfgv1.MachineConfig, osImageURL string) (*m <nl> if len(configs) == 0 { <nl> return nil, nil <nl> } <nl> - sort.Slice(configs, func(i, j int) bool { return configs[i].Name < configs[j].Name }) <nl> + sort.SliceStable(configs, func(i, j int) bool { return configs[i].Name < configs[j].Name }) <nl> var fips, ok bool <nl> var kernelType string <nl> ", "msg": "pkg/controller/common: use sort.SliceStable to keep same elements original order"}
{"diff_id": 2685, "repo": "openshift/machine-config-operator", "sha": "197db252c04a34fd1c2789e6561f632895fe2da2", "time": "01.09.2020 15:10:38", "diff": "mmm a / lib/resourcemerge/core.go <nl> ppp b / lib/resourcemerge/core.go <nl>@@ -99,6 +99,23 @@ func ensureContainer(modified *bool, existing *corev1.Container, required corev1 <nl> setStringIfSet(modified, &existing.WorkingDir, required.WorkingDir) <nl> + // also sync the env vars here, added to handle proxy <nl> + for _, required := range required.Env { <nl> + var existingCurr *corev1.EnvVar <nl> + for j, curr := range existing.Env { <nl> + if curr.Name == required.Name { <nl> + existingCurr = &existing.Env[j] <nl> + break <nl> + } <nl> + } <nl> + if existingCurr == nil { <nl> + *modified = true <nl> + existing.Env = append(existing.Env, corev1.EnvVar{}) <nl> + existingCurr = &existing.Env[len(existing.Env)-1] <nl> + } <nl> + ensureEnvVar(modified, existingCurr, required) <nl> + } <nl> + <nl> // any port we specify, we require <nl> for _, required := range required.Ports { <nl> var existingCurr *corev1.ContainerPort <nl> @@ -177,6 +194,13 @@ func ensureContainerPort(modified *bool, existing *corev1.ContainerPort, require <nl> } <nl> } <nl> +func ensureEnvVar(modified *bool, existing *corev1.EnvVar, required corev1.EnvVar) { <nl> + if !equality.Semantic.DeepEqual(required, *existing) { <nl> + *modified = true <nl> + *existing = required <nl> + } <nl> +} <nl> + <nl> func ensureVolumeMount(modified *bool, existing *corev1.VolumeMount, required corev1.VolumeMount) { <nl> if !equality.Semantic.DeepEqual(required, *existing) { <nl> *modified = true <nl> ", "msg": "lib/resourcemerge: sync daemonset container env vars\nSync env var changes in the daemonset definition, so injected proxy\nenv var updates will get applied upon an upgrade."}
{"diff_id": 2692, "repo": "openshift/machine-config-operator", "sha": "4e5f1ee1bfddf1ddd7c4d3a5b89e50b8dec774db", "time": "29.10.2020 10:53:58", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -347,6 +347,7 @@ func (dn *Daemon) applyOSChanges(oldConfig, newConfig *mcfgv1.MachineConfig) (re <nl> // Update OS <nl> if err := dn.updateOS(newConfig, osImageContentDir); err != nil { <nl> + MCDPivotErr.WithLabelValues(dn.node.Name, newConfig.Spec.OSImageURL, err.Error()).SetToCurrentTime() <nl> return err <nl> } <nl> ", "msg": "daemon: add back metrics for pivot error"}
{"diff_id": 2696, "repo": "openshift/machine-config-operator", "sha": "b8e92305895f164d985c001d30fc65da8f0436b5", "time": "16.11.2020 16:09:57", "diff": "mmm a / pkg/operator/sync.go <nl> ppp b / pkg/operator/sync.go <nl>@@ -560,14 +560,6 @@ func (optr *Operator) syncRequiredMachineConfigPools(_ *renderConfig) error { <nl> if err != nil { <nl> return err <nl> } <nl> - isPoolStatusConditionTrue := func(pool *mcfgv1.MachineConfigPool, conditionType mcfgv1.MachineConfigPoolConditionType) bool { <nl> - for _, condition := range pool.Status.Conditions { <nl> - if condition.Type == conditionType { <nl> - return condition.Status == corev1.ConditionTrue <nl> - } <nl> - } <nl> - return false <nl> - } <nl> var lastErr error <nl> if err := wait.Poll(time.Second, 10*time.Minute, func() (bool, error) { <nl> @@ -843,3 +835,12 @@ func mergeCertWithCABundle(initialBundle, newBundle []byte, subject string) []by <nl> } <nl> return mergedBytes <nl> } <nl> + <nl> +func isPoolStatusConditionTrue(pool *mcfgv1.MachineConfigPool, conditionType mcfgv1.MachineConfigPoolConditionType) bool { <nl> + for _, condition := range pool.Status.Conditions { <nl> + if condition.Type == conditionType { <nl> + return condition.Status == corev1.ConditionTrue <nl> + } <nl> + } <nl> + return false <nl> +} <nl> ", "msg": "move isPoolStatusConditionTrue to standalone func"}
{"diff_id": 2712, "repo": "openshift/machine-config-operator", "sha": "44d95f3dd112a2a56935edb93a61953b83ab5f79", "time": "28.01.2021 13:59:22", "diff": "mmm a / pkg/daemon/daemon.go <nl> ppp b / pkg/daemon/daemon.go <nl>@@ -1414,14 +1414,19 @@ func checkV3Units(units []ign3types.Unit) error { <nl> content = *u.Dropins[j].Contents <nl> } <nl> - // Backwards compatibility check: the new behavior is to noop when a drop-in for zero length. <nl> - // However, to maintain backwards compatibility, we allow existing zero length files to exist. <nl> - if err := checkFileContentsAndMode(path, []byte(content), defaultFilePermissions); err != nil { <nl> - if content == \"\" && os.IsNotExist(err) { <nl> + // As of 4.7 we now remove any empty defined dropins, check for that first <nl> + if _, err := os.Stat(path); content == \"\" && err != nil { <nl> + if os.IsNotExist(err) { <nl> continue <nl> } <nl> return err <nl> } <nl> + <nl> + // To maintain backwards compatibility, we allow existing zero length files to exist. <nl> + // Thus we are also ok if the dropin exists but has no content <nl> + if err := checkFileContentsAndMode(path, []byte(content), defaultFilePermissions); err != nil { <nl> + return err <nl> + } <nl> } <nl> if u.Contents == nil || *u.Contents == \"\" { <nl> ", "msg": "Bug daemon: check empty dropins separately\nThe previous commit checks for IsNotExist as a return of\ncheckFileContentsAndMode, which since we wrap the error, will\nnever be the case, thus always erroring there. Instead check\nfor the error in a separate case."}
{"diff_id": 2718, "repo": "openshift/machine-config-operator", "sha": "39a93956b3e452c29ba4d51cc07778f606125adb", "time": "11.03.2021 16:41:51", "diff": "mmm a / pkg/operator/status.go <nl> ppp b / pkg/operator/status.go <nl>@@ -72,6 +72,11 @@ func (optr *Operator) syncRelatedObjects() error { <nl> {Group: \"machineconfiguration.openshift.io\", Resource: \"machineconfigs\"}, <nl> // gathered because the machineconfigs created container bootstrap credentials and node configuration that gets reflected via the API and is needed for debugging <nl> {Group: \"\", Resource: \"nodes\"}, <nl> + // Gathered for the on-prem services running in static pods. <nl> + {Resource: \"namespaces\", Name: \"openshift-kni-infra\"}, <nl> + {Resource: \"namespaces\", Name: \"openshift-openstack-infra\"}, <nl> + {Resource: \"namespaces\", Name: \"openshift-ovirt-infra\"}, <nl> + {Resource: \"namespaces\", Name: \"openshift-vsphere-infra\"}, <nl> } <nl> if !equality.Semantic.DeepEqual(coCopy.Status.RelatedObjects, co.Status.RelatedObjects) { <nl> ", "msg": "Add on-prem namespaces to relatedObjects\nPreviously the kni namespace was explicitly listed in must-gather,\nbut when I went to add the other on-prem ones it was pointed out\nthat this should probably be done with relatedObjects. This adds\nthe namespaces here so we don't need anything in must-gather itself."}
{"diff_id": 2747, "repo": "openshift/machine-config-operator", "sha": "fb33963cabe4940fbb6e5984fa76d9500a109753", "time": "04.10.2021 10:51:59", "diff": "mmm a / pkg/controller/template/render_test.go <nl> ppp b / pkg/controller/template/render_test.go <nl>@@ -31,9 +31,6 @@ func TestCloudProvider(t *testing.T) { <nl> featureGate *configv1.FeatureGate <nl> res string <nl> }{{ <nl> - platform: configv1.AWSPlatformType, <nl> - res: \"aws\", <nl> - }, { <nl> platform: configv1.AWSPlatformType, <nl> featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> res: \"external\", <nl> @@ -48,11 +45,22 @@ func TestCloudProvider(t *testing.T) { <nl> }, { <nl> platform: configv1.GCPPlatformType, <nl> featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> - res: \"gce\", <nl> + res: \"external\", <nl> + }, { <nl> + platform: configv1.VSpherePlatformType, <nl> + featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> + res: \"external\", <nl> + }, { <nl> + platform: configv1.OpenStackPlatformType, <nl> + featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> + res: \"external\", <nl> }, { <nl> platform: configv1.OpenStackPlatformType, <nl> featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", nil, []string{cloudprovider.ExternalCloudProviderFeature}), <nl> res: \"openstack\", <nl> + }, { <nl> + platform: configv1.AWSPlatformType, <nl> + res: \"aws\", <nl> }, { <nl> platform: configv1.OpenStackPlatformType, <nl> res: \"openstack\", <nl> @@ -157,7 +165,15 @@ func TestCloudConfigFlag(t *testing.T) { <nl> option = a <nl> `, <nl> featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> - res: \"--cloud-config=/etc/kubernetes/cloud.conf\", <nl> + res: \"\", <nl> + }, { <nl> + platform: configv1.VSpherePlatformType, <nl> + content: ` <nl> +[dummy-config] <nl> + option = a <nl> +`, <nl> + featureGate: newFeatures(\"cluster\", \"CustomNoUpgrade\", []string{cloudprovider.ExternalCloudProviderFeature}, nil), <nl> + res: \"\", <nl> }, { <nl> platform: configv1.AzurePlatformType, <nl> content: ` <nl> ", "msg": "Update external cloud provider FG unittests\nFix external cloud provider GCP unittests, add vSphere related ones.\nReorder tests a bit."}
{"diff_id": 2754, "repo": "openshift/machine-config-operator", "sha": "f72e677d854a6c9a73959117f229ddd14df1ecec", "time": "04.11.2021 09:30:17", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -46,8 +46,8 @@ const ( <nl> osImageContentBaseDir = \"/run/mco-machine-os-content/\" <nl> // These are the actions for a node to take after applying config changes. (e.g. a new machineconfig is applied) <nl> - // \"None\" means no special action needs to be taken. A drain will still happen. <nl> - // This currently happens when ssh keys or pull secret (/var/lib/kubelet/config.json) is changed <nl> + // \"None\" means no special action needs to be taken <nl> + // This happens for example when ssh keys or the pull secret (/var/lib/kubelet/config.json) is changed <nl> postConfigChangeActionNone = \"none\" <nl> // Rebooting is still the default scenario for any other change <nl> postConfigChangeActionReboot = \"reboot\" <nl> ", "msg": "Update postConfigChangeActionNone comment\nA drain no longer occurs, and another case, kubelet-ca.crt, has been\nadded"}
{"diff_id": 2785, "repo": "openshift/machine-config-operator", "sha": "f0acfe36347e32f0a50b42e60c0e7e2eddce0b54", "time": "12.04.2022 15:39:18", "diff": "mmm a / pkg/daemon/update_test.go <nl> ppp b / pkg/daemon/update_test.go <nl>@@ -129,7 +129,7 @@ func TestReconcilable(t *testing.T) { <nl> // Verify Disk changes react as expected <nl> oldIgnCfg.Storage.Disks = []ign3types.Disk{ <nl> - ign3types.Disk{ <nl> + { <nl> Device: \"/one\", <nl> }, <nl> } <nl> @@ -145,7 +145,7 @@ func TestReconcilable(t *testing.T) { <nl> // Verify Filesystems changes react as expected <nl> oldIgnCfg.Storage.Filesystems = []ign3types.Filesystem{ <nl> - ign3types.Filesystem{ <nl> + { <nl> Device: \"/dev/sda1\", <nl> Format: helpers.StrToPtr(\"ext4\"), <nl> Path: helpers.StrToPtr(\"/foo/bar\"), <nl> @@ -548,20 +548,20 @@ func TestDropinCheck(t *testing.T) { <nl> t.Run(fmt.Sprintf(\"case#%d\", idx), func(t *testing.T) { <nl> ignCfg := ctrlcommon.NewIgnConfig() <nl> ignCfg.Systemd.Units = []ign3types.Unit{ <nl> - ign3types.Unit{ <nl> + { <nl> Name: test.service, <nl> Dropins: []ign3types.Dropin{ <nl> - ign3types.Dropin{ <nl> + { <nl> Name: test.dropin, <nl> Contents: helpers.StrToPtr(\"[Unit]\"), <nl> }, <nl> - ign3types.Dropin{ <nl> + { <nl> Name: \"99-other.conf\", <nl> Contents: helpers.StrToPtr(\"[Unit]\"), <nl> }, <nl> }, <nl> }, <nl> - ign3types.Unit{ <nl> + { <nl> Name: \"other.service\", <nl> }, <nl> } <nl> ", "msg": "update_test: remove redundant types\nThese types are already declared by the containing arrays"}
{"diff_id": 2797, "repo": "openshift/machine-config-operator", "sha": "c7166c479ba0a7467ae84d6e94467735267fd0de", "time": "30.05.2022 18:46:21", "diff": "mmm a / pkg/controller/drain/drain_controller.go <nl> ppp b / pkg/controller/drain/drain_controller.go <nl>@@ -292,7 +292,6 @@ func (ctrl *Controller) syncNode(key string) error { <nl> // This is a bit problematic in practice since we don't really have a previous state. <nl> // TODO (jerzhang) consider using a new CRD for coordination <nl> - ctrl.logNode(node, \"initiating drain\") <nl> ongoingDrain := false <nl> for k, v := range ctrl.ongoingDrains { <nl> if k != node.Name { <nl> @@ -318,6 +317,7 @@ func (ctrl *Controller) syncNode(key string) error { <nl> } <nl> // Attempt drain <nl> + ctrl.logNode(node, \"initiating drain\") <nl> if err := drain.RunNodeDrain(drainer, node.Name); err != nil { <nl> ctrl.logNode(node, \"Drain failed, but overall timeout has not been reached. Waiting 1 minute then retrying. Error message from drain: %v\", err) <nl> ctrl.enqueueAfter(node, drainRequeueDelay) <nl> ", "msg": "Move drain log message to when drain starts"}
{"diff_id": 2803, "repo": "openshift/machine-config-operator", "sha": "a8d445c16a3ecc294a10745697eeee78cd574ced", "time": "06.07.2022 19:19:52", "diff": "mmm a / pkg/daemon/rpm-ostree_test.go <nl> ppp b / pkg/daemon/rpm-ostree_test.go <nl>@@ -46,3 +46,15 @@ func (r RpmOstreeClientMock) GetStatus() (string, error) { <nl> func (r RpmOstreeClientMock) GetBootedDeployment() (*RpmOstreeDeployment, error) { <nl> return &RpmOstreeDeployment{}, nil <nl> } <nl> + <nl> +func (r RpmOstreeClientMock) GetBootedAndStagedDeployment() (booted, staged *RpmOstreeDeployment, err error) { <nl> + return nil, nil, nil <nl> +} <nl> + <nl> +func (r RpmOstreeClientMock) IsBootableImage(string) (bool, error) { <nl> + return false, nil <nl> +} <nl> + <nl> +func (r RpmOstreeClientMock) RebaseLayered(string) error { <nl> + return nil <nl> +} <nl> ", "msg": "Update rpm-ostree tests with new signatures\nWe changed the interface for RpmOstreeClient, so we need to make sure\nthe mocks are also up to date with the new signatures"}
{"diff_id": 2804, "repo": "openshift/machine-config-operator", "sha": "49b591cd09bcbed53efdc756454c11e27a70a3b7", "time": "22.08.2022 15:06:16", "diff": "mmm a / pkg/controller/common/helpers.go <nl> ppp b / pkg/controller/common/helpers.go <nl>@@ -585,8 +585,6 @@ func decompressPayload(r io.Reader) ([]byte, error) { <nl> return nil, errConfigNotGzipped <nl> } <nl> - out := bytes.NewBuffer([]byte{}) <nl> - <nl> gz, err := gzip.NewReader(in) <nl> if err != nil { <nl> return nil, fmt.Errorf(\"initialize gzip reader failed: %w\", err) <nl> @@ -594,12 +592,12 @@ func decompressPayload(r io.Reader) ([]byte, error) { <nl> defer gz.Close() <nl> - // Decompress our payload. <nl> - if _, err := io.Copy(out, gz); err != nil { <nl> + data, err := ioutil.ReadAll(gz) <nl> + if err != nil { <nl> return nil, fmt.Errorf(\"decompression failed: %w\", err) <nl> } <nl> - return out.Bytes(), nil <nl> + return data, nil <nl> } <nl> // Function to remove duplicated files/units/users from a V2 MC, since the translator <nl> ", "msg": "fixes config decompression lint issue\nThis was caught by the gosec linter in the Hypershift project; gosec\nG110."}
{"diff_id": 2808, "repo": "openshift/machine-config-operator", "sha": "16ac486a3f5ad9294a92a0e6f22d4d83211763cc", "time": "29.08.2022 11:04:17", "diff": "mmm a / pkg/daemon/update.go <nl> ppp b / pkg/daemon/update.go <nl>@@ -2116,15 +2116,14 @@ func (dn *CoreOSDaemon) applyLayeredOSChanges(mcDiff machineConfigDiff, oldConfi <nl> } <nl> } <nl> - // TODO(jkyros): We can't currently switch kernels on layered images, so only allow it if they're both default. We'll come back for this when it's supported. <nl> - // If you did try to switch kernels when using layered image, you would get a \"No enabled repositories\" error. <nl> - if !(canonicalizeKernelType(oldConfig.Spec.KernelType) == ctrlcommon.KernelTypeDefault && canonicalizeKernelType(newConfig.Spec.KernelType) == ctrlcommon.KernelTypeDefault) { <nl> - return fmt.Errorf(\"Non-default kernels are not currently supported for layered images. (old: %s new %s)\", oldConfig.Spec.KernelType, newConfig.Spec.KernelType) <nl> + // Switch to real time kernel <nl> + if err := dn.switchKernel(oldConfig, newConfig); err != nil { <nl> + return err <nl> } <nl> - // TODO(jkyros): This is where we will handle Joseph's extensions container <nl> - if len(newConfig.Spec.Extensions) > 0 { <nl> - return fmt.Errorf(\"Extensions are not currently supported with layered images, but extensions were supplied: %s\", strings.Join(newConfig.Spec.Extensions, \" \")) <nl> + // Apply extensions <nl> + if err := dn.applyExtensions(oldConfig, newConfig); err != nil { <nl> + return err <nl> } <nl> return nil <nl> ", "msg": "Kernel/Exetensions support for new image format\nNow that we have the extensions container, we can apply extensions and\nswitch kernels again.\nThis just re-enables that functionality for the 'new image' path during\ndaemon updates, and removes the \"not supported\" messages."}
{"diff_id": 2887, "repo": "topolvm/topolvm", "sha": "485c525f8b2bea4dee41b4b51cb92091bfa48bed", "time": "27.02.2021 17:30:45", "diff": "mmm a / driver/node.go <nl> ppp b / driver/node.go <nl>@@ -366,6 +366,13 @@ func (s *nodeService) isEphemeralVolume(volume *proto.LogicalVolume) bool { <nl> func (s *nodeService) nodeUnpublishFilesystemVolume(req *csi.NodeUnpublishVolumeRequest, device string) (*csi.NodeUnpublishVolumeResponse, error) { <nl> target := req.GetTargetPath() <nl> + <nl> + mounted, err := filesystem.IsMounted(device, target) <nl> + if err != nil { <nl> + return nil, status.Errorf(codes.Internal, \"mount check failed: target=%s, error=%v\", target, err) <nl> + } <nl> + <nl> + if mounted { <nl> if err := s.mounter.Unmount(target); err != nil { <nl> return nil, status.Errorf(codes.Internal, \"unmount failed for %s: error=%v\", target, err) <nl> } <nl> @@ -375,6 +382,7 @@ func (s *nodeService) nodeUnpublishFilesystemVolume(req *csi.NodeUnpublishVolume <nl> if err := os.Remove(device); err != nil { <nl> return nil, status.Errorf(codes.Internal, \"remove device failed for %s: error=%v\", device, err) <nl> } <nl> + } <nl> nodeLogger.Info(\"NodeUnpublishVolume(fs) is succeeded\", <nl> \"volume_id\", req.GetVolumeId(), <nl> ", "msg": "Add mounting check for the target device"}
{"diff_id": 2890, "repo": "topolvm/topolvm", "sha": "1305ba5c50b2c8bab90507ebf897b734d80ffeab", "time": "16.03.2021 05:32:53", "diff": "mmm a / filesystem/util.go <nl> ppp b / filesystem/util.go <nl>@@ -26,9 +26,18 @@ func isSameDevice(dev1, dev2 string) (bool, error) { <nl> var st1, st2 unix.Stat_t <nl> if err := Stat(dev1, &st1); err != nil { <nl> + // Some filesystem such as tmpfs, nfs or etc. does not use block device. <nl> + // In such case, given device path does not exist, <nl> + // we regard it is not an error but devices are not same always. <nl> + if os.IsNotExist(err) { <nl> + return false, nil <nl> + } <nl> return false, fmt.Errorf(\"stat failed for %s: %v\", dev1, err) <nl> } <nl> if err := Stat(dev2, &st2); err != nil { <nl> + if os.IsNotExist(err) { <nl> + return false, nil <nl> + } <nl> return false, fmt.Errorf(\"stat failed for %s: %v\", dev2, err) <nl> } <nl> @@ -58,12 +67,21 @@ func IsMounted(device, target string) (bool, error) { <nl> continue <nl> } <nl> + // If the filesystem is nfs and its connection is broken, EvalSymlinks will be stuck. <nl> + // So it should be in before calling EvalSymlinks. <nl> + ok, err := isSameDevice(device, fields[0]) <nl> + if err != nil { <nl> + return false, err <nl> + } <nl> + if !ok { <nl> + continue <nl> + } <nl> d, err := filepath.EvalSymlinks(fields[1]) <nl> if err != nil { <nl> return false, err <nl> } <nl> if d == target { <nl> - return isSameDevice(device, fields[0]) <nl> + return true, nil <nl> } <nl> } <nl> ", "msg": "Avoid unnecessary readlink.\nCalling readlink would be stuck if nfs is used and its connection is\nbroken.\nThis commit defers readlink after the device check to resolve such\nissue.\nFixes"}
{"diff_id": 2913, "repo": "topolvm/topolvm", "sha": "a53d2ea2b26c0d8d95064203cd62f438c890a483", "time": "26.07.2022 21:27:18", "diff": "mmm a / lvmd/lvservice.go <nl> ppp b / lvmd/lvservice.go <nl>@@ -306,14 +306,39 @@ func (s *lvService) ResizeLV(_ context.Context, req *proto.ResizeLVRequest) (*pr <nl> return nil, status.Error(codes.OutOfRange, \"shrinking volume size is not allowed\") <nl> } <nl> - free, err := vg.Free() <nl> + free := uint64(0) <nl> + var pool *command.ThinPool <nl> + switch dc.Type { <nl> + case TypeThick: <nl> + free, err = vg.Free() <nl> if err != nil { <nl> - log.Error(\"failed to free VG\", map[string]interface{}{ <nl> + log.Error(\"failed to get free bytes\", map[string]interface{}{ <nl> + log.FnError: err, <nl> + }) <nl> + return nil, status.Error(codes.Internal, err.Error()) <nl> + } <nl> + case TypeThin: <nl> + pool, err = vg.FindPool(dc.ThinPoolConfig.Name) <nl> + if err != nil { <nl> + log.Error(\"failed to get thinpool\", map[string]interface{}{ <nl> log.FnError: err, <nl> - \"name\": req.GetName(), <nl> }) <nl> return nil, status.Error(codes.Internal, err.Error()) <nl> } <nl> + tpu, err := pool.Free() <nl> + if err != nil { <nl> + log.Error(\"failed to get free bytes\", map[string]interface{}{ <nl> + log.FnError: err, <nl> + }) <nl> + return nil, status.Error(codes.Internal, err.Error()) <nl> + } <nl> + free = uint64(math.Floor(dc.ThinPoolConfig.OverprovisionRatio*float64(tpu.SizeBytes))) - tpu.VirtualBytes <nl> + default: <nl> + // technically this block will not be hit however make sure we return error <nl> + // in such cases where deviceclass target is neither thick or thinpool <nl> + return nil, status.Error(codes.Internal, fmt.Sprintf(\"unsupported device class target: %s\", dc.Type)) <nl> + } <nl> + <nl> if free < (requested - current) { <nl> log.Error(\"no enough space left on VG\", map[string]interface{}{ <nl> log.FnError: err, <nl> ", "msg": "fix: ResizeLV does not check thinpool size\nThe ResizeLV function did not use the available overprovisioned\ncapacity in the size checks for thin-provisioned devices. This caused\nresizing a thin LV to fail when the VG did not have sufficient\nspace but the overprovisioned capacity did."}
{"diff_id": 2957, "repo": "openfaas/faas-netes", "sha": "b4b112e1342979590eadd2d4471530f45734a46e", "time": "19.10.2019 10:48:45", "diff": "mmm a / handlers/secrets_api_test.go <nl> ppp b / handlers/secrets_api_test.go <nl>@@ -12,7 +12,7 @@ import ( <nl> \"strings\" <nl> \"testing\" <nl> - \"github.com/openfaas/faas/gateway/requests\" <nl> + types \"github.com/openfaas/faas-provider/types\" <nl> v1 \"k8s.io/api/core/v1\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> testclient \"k8s.io/client-go/kubernetes/fake\" <nl> @@ -127,7 +127,7 @@ func Test_SecretsHandler(t *testing.T) { <nl> decoder := json.NewDecoder(resp.Body) <nl> - secretList := []requests.Secret{} <nl> + secretList := []types.Secret{} <nl> err := decoder.Decode(&secretList) <nl> if err != nil { <nl> t.Error(err) <nl> ", "msg": "Move to faas-provider for the secrets struct"}
{"diff_id": 2963, "repo": "openfaas/faas-netes", "sha": "d39a153855c6fcdb17e5ecf4317964f2ccacb49d", "time": "06.02.2020 11:11:28", "diff": "mmm a / k8s/proxy.go <nl> ppp b / k8s/proxy.go <nl>@@ -73,7 +73,15 @@ func (l *FunctionLookup) Resolve(name string) (url.URL, error) { <nl> return url.URL{}, fmt.Errorf(\"error listing %s.%s %s\", name, namespace, err.Error()) <nl> } <nl> + if len(svc.Subsets) == 0 { <nl> + return url.URL{}, fmt.Errorf(\"no subsets available for %s.%s\", name, namespace) <nl> + } <nl> + <nl> all := len(svc.Subsets[0].Addresses) <nl> + if len(svc.Subsets[0].Addresses) == 0 { <nl> + return url.URL{}, fmt.Errorf(\"no addresses in subset for %s.%s\", name, namespace) <nl> + } <nl> + <nl> target := rand.Intn(all) <nl> serviceIP := svc.Subsets[0].Addresses[target].IP <nl> ", "msg": "Add range checking for endpoints\nThis could fail when endpoints are removed during a scale down\noperation."}
{"diff_id": 2971, "repo": "openfaas/faas-netes", "sha": "39236d9d2474db0e77077ff774079b2b4ee483df", "time": "15.07.2020 16:48:30", "diff": "mmm a / main.go <nl> ppp b / main.go <nl>@@ -145,10 +145,10 @@ func runController(setup serverSetup) { <nl> endpointsInformer := kubeInformerFactory.Core().V1().Endpoints() <nl> - log.Println(\"waiting for openfaas CRD cache sync\") <nl> + log.Println(\"Waiting for openfaas CRD cache sync\") <nl> faasInformerFactory.WaitForCacheSync(stopCh) <nl> setup.profileInformerFactory.WaitForCacheSync(stopCh) <nl> - log.Println(\"cache sync complete\") <nl> + log.Println(\"Cache sync complete\") <nl> go faasInformerFactory.Start(stopCh) <nl> go kubeInformerFactory.Start(stopCh) <nl> go setup.profileInformerFactory.Start(stopCh) <nl> ", "msg": "Startup log lines start with captial letters\n**What**\nuse capital letters during the server startup log lines. This will\nkeep the logs consistent"}
{"diff_id": 2974, "repo": "openfaas/faas-netes", "sha": "811e55497a0431b438e0d628265a262f2267dbb0", "time": "17.09.2020 08:56:34", "diff": "mmm a / pkg/handlers/deploy.go <nl> ppp b / pkg/handlers/deploy.go <nl>@@ -172,7 +172,7 @@ func makeDeploymentSpec(request types.FunctionDeployment, existingSecrets map[st <nl> return nil, err <nl> } <nl> - isEnableServiceLinks := false <nl> + enableServiceLinks := false <nl> deploymentSpec := &appsv1.Deployment{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> @@ -216,7 +216,11 @@ func makeDeploymentSpec(request types.FunctionDeployment, existingSecrets map[st <nl> Name: request.Service, <nl> Image: request.Image, <nl> Ports: []apiv1.ContainerPort{ <nl> - {ContainerPort: factory.Config.RuntimeHTTPPort, Protocol: corev1.ProtocolTCP}, <nl> + { <nl> + Name: \"http\", <nl> + ContainerPort: factory.Config.RuntimeHTTPPort, <nl> + Protocol: corev1.ProtocolTCP, <nl> + }, <nl> }, <nl> Env: envVars, <nl> Resources: *resources, <nl> @@ -231,7 +235,9 @@ func makeDeploymentSpec(request types.FunctionDeployment, existingSecrets map[st <nl> ServiceAccountName: serviceAccount, <nl> RestartPolicy: corev1.RestartPolicyAlways, <nl> DNSPolicy: corev1.DNSClusterFirst, <nl> - EnableServiceLinks: &isEnableServiceLinks, <nl> + // EnableServiceLinks injects ENV vars about every other service within <nl> + // the namespace. <nl> + EnableServiceLinks: &enableServiceLinks, <nl> }, <nl> }, <nl> }, <nl> ", "msg": "Name the watchdog's port \"http\"\nHTTP is used by some services meshes as metadata when\nrouting. No harm change."}
{"diff_id": 2993, "repo": "coredns/coredns", "sha": "0d72efbbf9e45c8eeafdfa54154f5006a608e5e6", "time": "09.06.2017 14:09:16", "diff": "mmm a / coremain/run.go <nl> ppp b / coremain/run.go <nl>@@ -43,10 +43,10 @@ func init() { <nl> flag.StringVar(&conf, \"conf\", \"\", \"Corefile to load (default \\\"\"+caddy.DefaultConfigFile+\"\\\")\") <nl> flag.StringVar(&cpu, \"cpu\", \"100%\", \"CPU cap\") <nl> flag.BoolVar(&plugins, \"plugins\", false, \"List installed plugins\") <nl> - flag.StringVar(&logfile, \"log\", \"\", \"Process log file\") <nl> flag.StringVar(&caddy.PidFile, \"pidfile\", \"\", \"Path to write pid file\") <nl> flag.BoolVar(&version, \"version\", false, \"Show version\") <nl> flag.BoolVar(&dnsserver.Quiet, \"quiet\", false, \"Quiet mode (no initialization output)\") <nl> + flag.BoolVar(&logfile, \"log\", false, \"Log to standard output\") <nl> caddy.RegisterCaddyfileLoader(\"flag\", caddy.LoaderFunc(confLoader)) <nl> caddy.SetDefaultCaddyfileLoader(\"default\", caddy.LoaderFunc(defaultLoader)) <nl> @@ -61,12 +61,7 @@ func Run() { <nl> flag.Parse() <nl> // Set up process log before anything bad happens <nl> - switch logfile { <nl> - case \"stdout\": <nl> - log.SetOutput(os.Stdout) <nl> - case \"stderr\": <nl> - log.SetOutput(os.Stderr) <nl> - default: <nl> + if logfile { <nl> log.SetOutput(os.Stdout) <nl> } <nl> log.SetFlags(log.LstdFlags) <nl> @@ -228,7 +223,7 @@ func setCPU(cpu string) error { <nl> var ( <nl> conf string <nl> cpu string <nl> - logfile string <nl> + logfile bool <nl> version bool <nl> plugins bool <nl> ) <nl> ", "msg": "core: -log bolean flag to enable logging\n* core: -log bolean flag to enable logging\nChange to -log flag to a boolean that defaults false and when true\nlogs to stdout.\n* And bool here"}
{"diff_id": 3008, "repo": "coredns/coredns", "sha": "be037a32a5b05bf3e28ac04d369818ce524d3dca", "time": "09.08.2017 04:09:37", "diff": "mmm a / middleware/kubernetes/handler.go <nl> ppp b / middleware/kubernetes/handler.go <nl>@@ -38,14 +38,44 @@ func (k Kubernetes) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.M <nl> zone = state.Name() <nl> } <nl> - // TODO(miek): place contents of route-request back here. <nl> - records, extra, _, err := k.routeRequest(zone, state) <nl> + var ( <nl> + records []dns.RR <nl> + extra []dns.RR <nl> + err error <nl> + ) <nl> + <nl> + switch state.Type() { <nl> + case \"A\": <nl> + records, _, err = middleware.A(&k, zone, state, nil, middleware.Options{}) <nl> + case \"AAAA\": <nl> + records, _, err = middleware.AAAA(&k, zone, state, nil, middleware.Options{}) <nl> + case \"TXT\": <nl> + records, _, err = middleware.TXT(&k, zone, state, middleware.Options{}) <nl> + case \"CNAME\": <nl> + records, _, err = middleware.CNAME(&k, zone, state, middleware.Options{}) <nl> + case \"PTR\": <nl> + records, _, err = middleware.PTR(&k, zone, state, middleware.Options{}) <nl> + case \"MX\": <nl> + records, extra, _, err = middleware.MX(&k, zone, state, middleware.Options{}) <nl> + case \"SRV\": <nl> + records, extra, _, err = middleware.SRV(&k, zone, state, middleware.Options{}) <nl> + case \"SOA\": <nl> + records, _, err = middleware.SOA(&k, zone, state, middleware.Options{}) <nl> + case \"NS\": <nl> + if state.Name() == zone { <nl> + records, extra, _, err = middleware.NS(&k, zone, state, middleware.Options{}) <nl> + break <nl> + } <nl> + fallthrough <nl> + default: <nl> + // Do a fake A lookup, so we can distinguish between NODATA and NXDOMAIN <nl> + _, _, err = middleware.A(&k, zone, state, nil, middleware.Options{}) <nl> + } <nl> if k.IsNameError(err) { <nl> if k.Fallthrough { <nl> return middleware.NextOrFailure(k.Name(), k.Next, ctx, w, r) <nl> } <nl> - // Make err nil when returning here, so we don't log spam for NXDOMAIN. <nl> return middleware.BackendError(&k, zone, dns.RcodeNameError, state, nil /*debug*/, nil /* err */, middleware.Options{}) <nl> } <nl> if err != nil { <nl> @@ -66,36 +96,5 @@ func (k Kubernetes) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.M <nl> return dns.RcodeSuccess, nil <nl> } <nl> -func (k *Kubernetes) routeRequest(zone string, state request.Request) (records []dns.RR, extra []dns.RR, debug []dns.RR, err error) { <nl> - switch state.Type() { <nl> - case \"A\": <nl> - records, _, err = middleware.A(k, zone, state, nil, middleware.Options{}) <nl> - case \"AAAA\": <nl> - records, _, err = middleware.AAAA(k, zone, state, nil, middleware.Options{}) <nl> - case \"TXT\": <nl> - records, _, err = middleware.TXT(k, zone, state, middleware.Options{}) <nl> - case \"CNAME\": <nl> - records, _, err = middleware.CNAME(k, zone, state, middleware.Options{}) <nl> - case \"PTR\": <nl> - records, _, err = middleware.PTR(k, zone, state, middleware.Options{}) <nl> - case \"MX\": <nl> - records, extra, _, err = middleware.MX(k, zone, state, middleware.Options{}) <nl> - case \"SRV\": <nl> - records, extra, _, err = middleware.SRV(k, zone, state, middleware.Options{}) <nl> - case \"SOA\": <nl> - records, _, err = middleware.SOA(k, zone, state, middleware.Options{}) <nl> - case \"NS\": <nl> - if state.Name() == zone { <nl> - records, extra, _, err = middleware.NS(k, zone, state, middleware.Options{}) <nl> - break <nl> - } <nl> - fallthrough <nl> - default: <nl> - // Do a fake A lookup, so we can distinguish between NODATA and NXDOMAIN <nl> - _, _, err = middleware.A(k, zone, state, nil, middleware.Options{}) <nl> - } <nl> - return records, extra, nil, err <nl> -} <nl> - <nl> // Name implements the Handler interface. <nl> func (k Kubernetes) Name() string { return \"kubernetes\" } <nl> ", "msg": "mw/kubernetes: restore handler to pre-autopath state\nPull in the contents of routeRequest as it is only called once."}
{"diff_id": 3010, "repo": "coredns/coredns", "sha": "5a1875120cceeaa90f065a4fa7dccffb8c535a71", "time": "17.08.2017 18:42:14", "diff": "mmm a / middleware/kubernetes/reverse_test.go <nl> ppp b / middleware/kubernetes/reverse_test.go <nl>@@ -96,6 +96,13 @@ func TestReverse(t *testing.T) { <nl> test.SOA(\"0.10.in-addr.arpa. 300 IN SOA ns.dns.0.10.in-addr.arpa. hostmaster.0.10.in-addr.arpa. 1502782828 7200 1800 86400 60\"), <nl> }, <nl> }, <nl> + { <nl> + Qname: \"example.org.cluster.local.\", Qtype: dns.TypePTR, <nl> + Rcode: dns.RcodeSuccess, <nl> + Ns: []dns.RR{ <nl> + test.SOA(\"cluster.local. 300 IN SOA ns.dns.cluster.local. hostmaster.cluster.local. 1502989566 7200 1800 86400 60\"), <nl> + }, <nl> + }, <nl> } <nl> ctx := context.TODO() <nl> @@ -109,9 +116,6 @@ func TestReverse(t *testing.T) { <nl> t.Errorf(\"Test %d: expected no error, got %v\", i, err) <nl> return <nl> } <nl> - if tc.Error != nil { <nl> - continue <nl> - } <nl> resp := w.Msg <nl> if resp == nil { <nl> ", "msg": "mw/kubernetes: add reverse test case\nAdd a non-arpa testcase to the reverse test."}
{"diff_id": 3018, "repo": "coredns/coredns", "sha": "c72084187c42a526e4b754741c392a38733ceec2", "time": "31.08.2017 08:20:13", "diff": "mmm a / core/dnsserver/server.go <nl> ppp b / core/dnsserver/server.go <nl>@@ -289,6 +289,12 @@ func DefaultErrorFunc(w dns.ResponseWriter, r *dns.Msg, rc int) { <nl> answer := new(dns.Msg) <nl> answer.SetRcode(r, rc) <nl> + if r == nil { <nl> + log.Printf(\"[WARNING] DefaultErrorFunc called with nil *dns.Msg (Remote: %s)\", w.RemoteAddr().String()) <nl> + w.WriteMsg(answer) <nl> + return <nl> + } <nl> + <nl> state.SizeAndDo(answer) <nl> vars.Report(state, vars.Dropped, rcode.ToString(rc), answer.Len(), time.Now()) <nl> ", "msg": "core: add nil check\nCheck if msg is nil in DefaultErrorFunc. If this is the case log this\nand short cut the function.\nHoping to get more insight in"}
{"diff_id": 3024, "repo": "coredns/coredns", "sha": "74f9dc6c0046cf847e202f171663960f0f8a5d5a", "time": "14.09.2017 21:56:54", "diff": "mmm a / core/dnsserver/register.go <nl> ppp b / core/dnsserver/register.go <nl>@@ -124,12 +124,6 @@ func (c *Config) AddPlugin(m plugin.Plugin) { <nl> c.Plugin = append(c.Plugin, m) <nl> } <nl> -// AddMiddleware adds a plugin to a site's plugin stack. This method is deprecated, use AddPlugin. <nl> -func (c *Config) AddMiddleware(m plugin.Plugin) { <nl> - println(\"deprecated: use AddPlugin\") <nl> - c.AddPlugin(m) <nl> -} <nl> - <nl> // registerHandler adds a handler to a site's handler registration. Handlers <nl> // use this to announce that they exist to other plugin. <nl> func (c *Config) registerHandler(h plugin.Handler) { <nl> ", "msg": "core: Remove AddMiddleware\nThis does not help to make it backwards compatible. The middleware ->\nplugin rename invalidates all this. External middleware won't compile\neither way."}
{"diff_id": 3028, "repo": "coredns/coredns", "sha": "23526aec1d933b6362c80eaa084749a794d43aca", "time": "29.09.2017 22:27:40", "diff": "mmm a / core/dnsserver/server.go <nl> ppp b / core/dnsserver/server.go <nl>@@ -179,6 +179,13 @@ func (s *Server) Address() string { return s.Addr } <nl> // defined in the request so that the correct zone <nl> // (configuration and plugin stack) will handle the request. <nl> func (s *Server) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) { <nl> + // our dns library protects us against really invalid packets, we can still <nl> + // get semi valid packets. Drop them here. <nl> + if r == nil || len(r.Question) == 0 { <nl> + DefaultErrorFunc(w, r, dns.RcodeServerFailure) <nl> + return <nl> + } <nl> + <nl> if !s.debug { <nl> defer func() { <nl> // In case the user doesn't enable error plugin, we still <nl> ", "msg": "core: drop invalid packets\nWe can still be on the receiving end of invalid packet. Drop them\nhere."}
{"diff_id": 3042, "repo": "coredns/coredns", "sha": "6a02c349eaa87ef6b53826e570f0793caebf7f2f", "time": "28.11.2017 00:38:06", "diff": "mmm a / core/dnsserver/server-grpc.go <nl> ppp b / core/dnsserver/server-grpc.go <nl>@@ -133,8 +133,7 @@ func (s *ServergRPC) Query(ctx context.Context, in *pb.DnsPacket) (*pb.DnsPacket <nl> return nil, fmt.Errorf(\"no TCP peer in gRPC context: %v\", p.Addr) <nl> } <nl> - r := &net.IPAddr{IP: a.IP} <nl> - w := &gRPCresponse{localAddr: s.listenAddr, remoteAddr: r, Msg: msg} <nl> + w := &gRPCresponse{localAddr: s.listenAddr, remoteAddr: a, Msg: msg} <nl> s.ServeDNS(ctx, w, msg) <nl> ", "msg": "Pass net.TCPAddr type as response address to gRPCresponse writer\nDnstap require protocol, address and port info about peer. So, I\nremoved conversion of TCPAddr to IPAddr"}
{"diff_id": 3050, "repo": "coredns/coredns", "sha": "aeacbf6e24cae5c6bad9847628204bd39b6a0ad8", "time": "25.01.2018 21:40:40", "diff": "mmm a / coremain/run.go <nl> ppp b / coremain/run.go <nl>@@ -18,20 +18,6 @@ import ( <nl> ) <nl> func init() { <nl> - // Reset flag.CommandLine to get rid of unwanted flags for instance from glog (used in kubernetes). <nl> - // And readd the once we want to keep. <nl> - flag.VisitAll(func(f *flag.Flag) { <nl> - if _, ok := flagsBlacklist[f.Name]; ok { <nl> - return <nl> - } <nl> - flagsToKeep = append(flagsToKeep, f) <nl> - }) <nl> - <nl> - flag.CommandLine = flag.NewFlagSet(os.Args[0], flag.ExitOnError) <nl> - for _, f := range flagsToKeep { <nl> - flag.Var(f.Value, f.Name, f.Usage) <nl> - } <nl> - <nl> caddy.TrapSignals() <nl> caddy.DefaultConfigFile = \"Corefile\" <nl> caddy.Quiet = true // don't show init stuff from caddy <nl> @@ -54,6 +40,19 @@ func init() { <nl> // Run is CoreDNS's main() function. <nl> func Run() { <nl> + // Reset flag.CommandLine to get rid of unwanted flags for instance from glog (used in kubernetes). <nl> + // And readd the once we want to keep. <nl> + flag.VisitAll(func(f *flag.Flag) { <nl> + if _, ok := flagsBlacklist[f.Name]; ok { <nl> + return <nl> + } <nl> + flagsToKeep = append(flagsToKeep, f) <nl> + }) <nl> + <nl> + flag.CommandLine = flag.NewFlagSet(os.Args[0], flag.ExitOnError) <nl> + for _, f := range flagsToKeep { <nl> + flag.Var(f.Value, f.Name, f.Usage) <nl> + } <nl> flag.Parse() <nl> ", "msg": "move flag blacklisting to main\n* move flag blacklisting to main\nDoing it in init() is the wrong place or something else changed. Doing\nit in main() makes it easy to see *when* this happens.\n* keep trapsignals"}
{"diff_id": 3054, "repo": "coredns/coredns", "sha": "0af9b9b16fe069f3201584d7e698efb4ebaee7fb", "time": "30.01.2018 22:18:44", "diff": "mmm a / plugin/proxy/dns.go <nl> ppp b / plugin/proxy/dns.go <nl>@@ -63,10 +63,12 @@ func (d *dnsEx) Exchange(ctx context.Context, addr string, state request.Request <nl> if err != nil { <nl> return nil, err <nl> } <nl> - // Make sure it fits in the DNS response. <nl> - reply, _ = state.Scrub(reply) <nl> reply.Compress = true <nl> reply.Id = state.Req.Id <nl> + // When using force_tcp the upstream can send a message that is too big for <nl> + // the udp buffer, hence we need to truncate the message to at least make it <nl> + // fit the udp buffer. <nl> + reply, _ = state.Scrub(reply) <nl> return reply, nil <nl> } <nl> ", "msg": "plugin/proxy: Fix unnecessary message truncation\nAs plugin/proxy always returns compressed messages, it's important to\nset this before calling Scrub(), as some messages will be unnecessarily\ntruncated otherwise."}
{"diff_id": 3065, "repo": "coredns/coredns", "sha": "95342dfaadd6e90cc8924a8c695f012c48d95b8b", "time": "09.03.2018 19:55:43", "diff": "mmm a / plugin/kubernetes/setup.go <nl> ppp b / plugin/kubernetes/setup.go <nl>@@ -2,7 +2,9 @@ package kubernetes <nl> import ( <nl> \"errors\" <nl> + \"flag\" <nl> \"fmt\" <nl> + \"os\" <nl> \"strconv\" <nl> \"strings\" <nl> \"time\" <nl> @@ -19,6 +21,13 @@ import ( <nl> ) <nl> func init() { <nl> + // Kubernetes plugin uses the kubernetes library, which uses glog (ugh), we must set this *flag*, <nl> + // so we don't log to the filesystem, which can fill up and crash CoreDNS indirectly by calling os.Exit(). <nl> + // We also set: os.Stderr = os.Stdout in the setup function below so we output to standard out; as we do for <nl> + // all CoreDNS logging. We can't do *that* in the init function, because we, when starting, also barf some <nl> + // things to stderr. <nl> + flag.Set(\"logtostderr\", \"true\") <nl> + <nl> caddy.RegisterPlugin(\"kubernetes\", caddy.Plugin{ <nl> ServerType: \"dns\", <nl> Action: setup, <nl> @@ -26,6 +35,9 @@ func init() { <nl> } <nl> func setup(c *caddy.Controller) error { <nl> + // See comment in the init function. <nl> + os.Stderr = os.Stdout <nl> + <nl> k, err := kubernetesParse(c) <nl> if err != nil { <nl> return plugin.Error(\"kubernetes\", err) <nl> ", "msg": "plugin/kubernetes: make glog log to standard output\nJump through all the hoops to make this work."}
{"diff_id": 3076, "repo": "coredns/coredns", "sha": "565e4164071778d076a48ece1698a0cf66e7548b", "time": "08.05.2018 18:36:08", "diff": "mmm a / plugin/cache/item.go <nl> ppp b / plugin/cache/item.go <nl>@@ -32,7 +32,7 @@ func newItem(m *dns.Msg, now time.Time, d time.Duration) *item { <nl> i.Answer = m.Answer <nl> i.Ns = m.Ns <nl> i.Extra = make([]dns.RR, len(m.Extra)) <nl> - // Don't copy OPT record as these are hop-by-hop. <nl> + // Don't copy OPT records as these are hop-by-hop. <nl> j := 0 <nl> for _, e := range m.Extra { <nl> if e.Header().Rrtype == dns.TypeOPT { <nl> @@ -75,12 +75,11 @@ func (i *item) toMsg(m *dns.Msg, now time.Time) *dns.Msg { <nl> m1.Ns[j] = dns.Copy(r) <nl> m1.Ns[j].Header().Ttl = ttl <nl> } <nl> + // newItem skips OPT records, so we can just use i.Extra as is. <nl> for j, r := range i.Extra { <nl> m1.Extra[j] = dns.Copy(r) <nl> - if m1.Extra[j].Header().Rrtype != dns.TypeOPT { <nl> m1.Extra[j].Header().Ttl = ttl <nl> } <nl> - } <nl> return m1 <nl> } <nl> ", "msg": "plugin/cache: don't recheck the OPT records\nThese are not stored with newItem so we don't have to check them later."}
{"diff_id": 3084, "repo": "coredns/coredns", "sha": "e3534205c7c3a89b09c9a9e2aca1a2159eb9ecde", "time": "20.06.2018 12:35:57", "diff": "mmm a / plugin/forward/metrics.go <nl> ppp b / plugin/forward/metrics.go <nl>@@ -44,7 +44,7 @@ var ( <nl> SocketGauge = prometheus.NewGaugeVec(prometheus.GaugeOpts{ <nl> Namespace: plugin.Namespace, <nl> Subsystem: \"forward\", <nl> - Name: \"socket_count_total\", <nl> + Name: \"sockets_open\", <nl> Help: \"Gauge of open sockets per upstream.\", <nl> }, []string{\"to\"}) <nl> ) <nl> ", "msg": "Rename forward metrics socket_count_total to sockets_open\nThe prometheus naming convention states only counters should have a\n`_total` suffix, so that gagues and counters can be easily\ndistinguished."}
{"diff_id": 3095, "repo": "coredns/coredns", "sha": "1697ab359d28e1af271ba8e41012c318b7846562", "time": "22.09.2018 13:25:31", "diff": "mmm a / plugin/forward/setup_test.go <nl> ppp b / plugin/forward/setup_test.go <nl>package forward <nl> import ( <nl> + \"io/ioutil\" <nl> + \"os\" <nl> \"reflect\" <nl> \"strings\" <nl> \"testing\" <nl> @@ -118,3 +120,55 @@ func TestSetupTLS(t *testing.T) { <nl> } <nl> } <nl> } <nl> + <nl> +func TestSetupResolvconf(t *testing.T) { <nl> + const resolv = \"resolv.conf\" <nl> + if err := ioutil.WriteFile(resolv, <nl> + []byte(`nameserver 10.10.255.252 <nl> +nameserver 10.10.255.253`), 0666); err != nil { <nl> + t.Fatalf(\"Failed to write resolv.conf file: %s\", err) <nl> + } <nl> + defer os.Remove(resolv) <nl> + <nl> + tests := []struct { <nl> + input string <nl> + shouldErr bool <nl> + expectedErr string <nl> + expectedNames []string <nl> + }{ <nl> + // pass <nl> + {`forward . ` + resolv, false, \"\", []string{\"10.10.255.252:53\", \"10.10.255.253:53\"}}, <nl> + } <nl> + <nl> + for i, test := range tests { <nl> + c := caddy.NewTestController(\"dns\", test.input) <nl> + f, err := parseForward(c) <nl> + <nl> + if test.shouldErr && err == nil { <nl> + t.Errorf(\"Test %d: expected error but found %s for input %s\", i, err, test.input) <nl> + continue <nl> + } <nl> + <nl> + if err != nil { <nl> + if !test.shouldErr { <nl> + t.Errorf(\"Test %d: expected no error but found one for input %s, got: %v\", i, test.input, err) <nl> + } <nl> + <nl> + if !strings.Contains(err.Error(), test.expectedErr) { <nl> + t.Errorf(\"Test %d: expected error to contain: %v, found error: %v, input: %s\", i, test.expectedErr, err, test.input) <nl> + } <nl> + } <nl> + <nl> + if !test.shouldErr { <nl> + for j, n := range test.expectedNames { <nl> + addr := f.proxies[j].addr <nl> + if n != addr { <nl> + t.Errorf(\"Test %d, expected %q, got %q\", j, n, addr) <nl> + } <nl> + } <nl> + } <nl> + for _, p := range f.proxies { <nl> + p.health.Check(p) // this should almost always err, we don't care it shoulnd't crash <nl> + } <nl> + } <nl> +} <nl> ", "msg": "Add test for\nThis adds a test for cleanup in"}
{"diff_id": 3100, "repo": "coredns/coredns", "sha": "1847ef6bd31ecd38fe5d19e54c47a812cb2ed303", "time": "14.10.2018 04:14:42", "diff": "mmm a / plugin/file/file.go <nl> ppp b / plugin/file/file.go <nl>@@ -121,6 +121,12 @@ func (s *serialErr) Error() string { <nl> // it returns an error indicating nothing was read. <nl> func Parse(f io.Reader, origin, fileName string, serial int64) (*Zone, error) { <nl> tokens := dns.ParseZone(f, dns.Fqdn(origin), fileName) <nl> + defer func() { <nl> + // Drain the tokens chan so that large zone files won't <nl> + // leak goroutines and memory. <nl> + for range tokens { <nl> + } <nl> + }() <nl> z := NewZone(origin, fileName) <nl> seenSOA := false <nl> for x := range tokens { <nl> ", "msg": "plugin/file: Fix memory leak in Parse\nFor zone files with more than 10,000 records, the goroutines and memory\npinned by dns.ParseZone won't be released unless the tokens chan is\ndrained. As Parse is called by (*Zone).Reload very frequently, this\ncauses memory leaks and OOM conditions.\nUpdates miekg/dns#786"}
{"diff_id": 3107, "repo": "coredns/coredns", "sha": "65be5617220df35718f7f2e3ce2fa61d4e22ea87", "time": "08.12.2018 13:19:22", "diff": "mmm a / plugin/backend_lookup.go <nl> ppp b / plugin/backend_lookup.go <nl>@@ -19,7 +19,7 @@ func A(b ServiceBackend, zone string, state request.Request, previousRecords []d <nl> return nil, err <nl> } <nl> - dup := make(map[string]bool) <nl> + dup := make(map[string]struct{}) <nl> for _, serv := range services { <nl> @@ -68,7 +68,7 @@ func A(b ServiceBackend, zone string, state request.Request, previousRecords []d <nl> case dns.TypeA: <nl> if _, ok := dup[serv.Host]; !ok { <nl> - dup[serv.Host] = true <nl> + dup[serv.Host] = struct{}{} <nl> records = append(records, serv.NewA(state.QName(), ip)) <nl> } <nl> @@ -86,7 +86,7 @@ func AAAA(b ServiceBackend, zone string, state request.Request, previousRecords <nl> return nil, err <nl> } <nl> - dup := make(map[string]bool) <nl> + dup := make(map[string]struct{}) <nl> for _, serv := range services { <nl> @@ -139,7 +139,7 @@ func AAAA(b ServiceBackend, zone string, state request.Request, previousRecords <nl> case dns.TypeAAAA: <nl> if _, ok := dup[serv.Host]; !ok { <nl> - dup[serv.Host] = true <nl> + dup[serv.Host] = struct{}{} <nl> records = append(records, serv.NewAAAA(state.QName(), ip)) <nl> } <nl> } <nl> @@ -155,8 +155,8 @@ func SRV(b ServiceBackend, zone string, state request.Request, opt Options) (rec <nl> return nil, nil, err <nl> } <nl> - dup := make(map[item]bool) <nl> - lookup := make(map[string]bool) <nl> + dup := make(map[item]struct{}) <nl> + lookup := make(map[string]struct{}) <nl> // Looping twice to get the right weight vs priority. This might break because we may drop duplicate SRV records latter on. <nl> w := make(map[int]int) <nl> @@ -196,7 +196,7 @@ func SRV(b ServiceBackend, zone string, state request.Request, opt Options) (rec <nl> break <nl> } <nl> - lookup[srv.Target] = true <nl> + lookup[srv.Target] = struct{}{} <nl> if !dns.IsSubDomain(zone, srv.Target) { <nl> m1, e1 := b.Lookup(state, srv.Target, dns.TypeA) <nl> @@ -248,8 +248,8 @@ func MX(b ServiceBackend, zone string, state request.Request, opt Options) (reco <nl> return nil, nil, err <nl> } <nl> - dup := make(map[item]bool) <nl> - lookup := make(map[string]bool) <nl> + dup := make(map[item]struct{}) <nl> + lookup := make(map[string]struct{}) <nl> for _, serv := range services { <nl> if !serv.Mail { <nl> continue <nl> @@ -263,7 +263,7 @@ func MX(b ServiceBackend, zone string, state request.Request, opt Options) (reco <nl> break <nl> } <nl> - lookup[mx.Mx] = true <nl> + lookup[mx.Mx] = struct{}{} <nl> if !dns.IsSubDomain(zone, mx.Mx) { <nl> m1, e1 := b.Lookup(state, mx.Mx, dns.TypeA) <nl> @@ -346,12 +346,12 @@ func PTR(b ServiceBackend, zone string, state request.Request, opt Options) (rec <nl> return nil, err <nl> } <nl> - dup := make(map[string]bool) <nl> + dup := make(map[string]struct{}) <nl> for _, serv := range services { <nl> if ip := net.ParseIP(serv.Host); ip == nil { <nl> if _, ok := dup[serv.Host]; !ok { <nl> - dup[serv.Host] = true <nl> + dup[serv.Host] = struct{}{} <nl> records = append(records, serv.NewPTR(state.QName(), serv.Host)) <nl> } <nl> } <nl> @@ -472,17 +472,17 @@ type item struct { <nl> // isDuplicate uses m to see if the combo (name, addr, port) already exists. If it does <nl> // not exist already IsDuplicate will also add the record to the map. <nl> -func isDuplicate(m map[item]bool, name, addr string, port uint16) bool { <nl> +func isDuplicate(m map[item]struct{}, name, addr string, port uint16) bool { <nl> if addr != \"\" { <nl> _, ok := m[item{name, 0, addr}] <nl> if !ok { <nl> - m[item{name, 0, addr}] = true <nl> + m[item{name, 0, addr}] = struct{}{} <nl> } <nl> return ok <nl> } <nl> _, ok := m[item{name, port, \"\"}] <nl> if !ok { <nl> - m[item{name, port, \"\"}] = true <nl> + m[item{name, port, \"\"}] = struct{}{} <nl> } <nl> return ok <nl> } <nl> ", "msg": "Make backand.go maps smaller\nThese maps where all map[x]bool. Change this a map[x]struct{} as this\nis smaller and we only use these map to signal \"this element exists\".\nThis should preserve a (small) amount of memory."}
{"diff_id": 3116, "repo": "coredns/coredns", "sha": "67932239489d955a0af69b41d78282ad5ed29a34", "time": "09.02.2019 23:39:51", "diff": "mmm a / test/etcd_credentials_test.go <nl> ppp b / test/etcd_credentials_test.go <nl>@@ -37,9 +37,21 @@ func TestEtcdCredentials(t *testing.T) { <nl> if _, err := etc.Client.RoleAdd(ctx, \"root\"); err != nil { <nl> t.Errorf(\"Failed to create root role: %s\", err) <nl> } <nl> + defer func() { <nl> + if _, err := etc.Client.RoleDelete(ctx, \"root\"); err != nil { <nl> + t.Errorf(\"Failed to delete root role: %s\", err) <nl> + } <nl> + }() <nl> + <nl> if _, err := etc.Client.UserAdd(ctx, username, password); err != nil { <nl> t.Errorf(\"Failed to create user: %s\", err) <nl> } <nl> + defer func() { <nl> + if _, err := etc.Client.UserDelete(ctx, username); err != nil { <nl> + t.Errorf(\"Failed to delete user: %s\", err) <nl> + } <nl> + }() <nl> + <nl> if _, err := etc.Client.UserGrantRole(ctx, username, \"root\"); err != nil { <nl> t.Errorf(\"Failed to assign role to root user: %v\", err) <nl> } <nl> ", "msg": "Fix etcd_cache_test to runnable multiple times.\nCurrently, when you run `TestEtcdCredentials` at etcd_credentials_test.go multiple times without clearing data of etcd, you will get following errors.\n```\netcd_credentials_test.go:38: Failed to create root role: etcdserver: role name already exists\netcd_credentials_test.go:41: Failed to create user: etcdserver: user name already exists\n```"}
{"diff_id": 3125, "repo": "coredns/coredns", "sha": "d652f72fa794b6417d46b305f7654f474387b510", "time": "13.03.2019 07:26:29", "diff": "mmm a / plugin/metadata/provider.go <nl> ppp b / plugin/metadata/provider.go <nl>// <nl> // Basic example: <nl> // <nl> -// Implement the Provider interface for a plugin: <nl> +// Implement the Provider interface for a plugin p: <nl> +// <nl> +// func (p P) Metadata(ctx context.Context, state request.Request) context.Context { <nl> +// metadata.SetValueFunc(ctx, \"test/something\", func() string { return \"myvalue\" }) <nl> +// return ctx <nl> +// } <nl> +// <nl> +// Basic example with caching: <nl> // <nl> // func (p P) Metadata(ctx context.Context, state request.Request) context.Context { <nl> // cached := \"\" <nl> // return ctx <nl> // } <nl> // <nl> -// Check the metadata from another plugin: <nl> +// If you need access to this metadata from another plugin: <nl> // <nl> // // ... <nl> // valueFunc := metadata.ValueFunc(ctx, \"test/something\") <nl> ", "msg": "plugin/metadata: tweak the docs a little\nAdd a simpler example that returns a static string the for metadata."}
{"diff_id": 3131, "repo": "coredns/coredns", "sha": "d41e9ff7b7196374856d8db4bf33b31df8e20abc", "time": "18.05.2019 09:08:34", "diff": "mmm a / plugin/pkg/log/log_test.go <nl> ppp b / plugin/pkg/log/log_test.go <nl>@@ -16,6 +16,7 @@ func TestDebug(t *testing.T) { <nl> if x := f.String(); x != \"\" { <nl> t.Errorf(\"Expected no debug logs, got %s\", x) <nl> } <nl> + f.Reset() <nl> D = true <nl> Debug(\"debug\") <nl> @@ -34,6 +35,7 @@ func TestDebugx(t *testing.T) { <nl> if x := f.String(); !strings.Contains(x, debug+\"debug\") { <nl> t.Errorf(\"Expected debug log to be %s, got %s\", debug+\"debug\", x) <nl> } <nl> + f.Reset() <nl> Debug(\"debug\") <nl> if x := f.String(); !strings.Contains(x, debug+\"debug\") { <nl> @@ -50,10 +52,12 @@ func TestLevels(t *testing.T) { <nl> if x := f.String(); !strings.Contains(x, info+ts) { <nl> t.Errorf(\"Expected log to be %s, got %s\", info+ts, x) <nl> } <nl> + f.Reset() <nl> Warning(ts) <nl> if x := f.String(); !strings.Contains(x, warning+ts) { <nl> t.Errorf(\"Expected log to be %s, got %s\", warning+ts, x) <nl> } <nl> + f.Reset() <nl> Error(ts) <nl> if x := f.String(); !strings.Contains(x, err+ts) { <nl> t.Errorf(\"Expected log to be %s, got %s\", err+ts, x) <nl> ", "msg": "pkg/log: reset the buffer in the tests\nReset the buf otherwise we're not checking the new value."}
{"diff_id": 3141, "repo": "coredns/coredns", "sha": "4fda9535d2f446de9dbaf8b397b0d871bdc882e5", "time": "30.07.2019 16:35:07", "diff": "mmm a / plugin/kubernetes/setup.go <nl> ppp b / plugin/kubernetes/setup.go <nl>@@ -2,7 +2,6 @@ package kubernetes <nl> import ( <nl> \"errors\" <nl> - \"flag\" <nl> \"fmt\" <nl> \"os\" <nl> \"strconv\" <nl> @@ -36,15 +35,7 @@ import ( <nl> var log = clog.NewWithPlugin(\"kubernetes\") <nl> func init() { <nl> - // Kubernetes plugin uses the kubernetes library, which now uses klog, we must set and parse this flag <nl> - // so we don't log to the filesystem, which can fill up and crash CoreDNS indirectly by calling os.Exit(). <nl> - // We also set: os.Stderr = os.Stdout in the setup function below so we output to standard out; as we do for <nl> - // all CoreDNS logging. We can't do *that* in the init function, because we, when starting, also barf some <nl> - // things to stderr. <nl> - klogFlags := flag.NewFlagSet(\"klog\", flag.ExitOnError) <nl> - klog.InitFlags(klogFlags) <nl> - logtostderr := klogFlags.Lookup(\"logtostderr\") <nl> - logtostderr.Value.Set(\"true\") <nl> + klog.SetOutput(os.Stdout) <nl> caddy.RegisterPlugin(\"kubernetes\", caddy.Plugin{ <nl> ServerType: \"dns\", <nl> @@ -53,9 +44,6 @@ func init() { <nl> } <nl> func setup(c *caddy.Controller) error { <nl> - // See comment in the init function. <nl> - os.Stderr = os.Stdout <nl> - <nl> k, err := kubernetesParse(c) <nl> if err != nil { <nl> return plugin.Error(\"kubernetes\", err) <nl> ", "msg": "plugin/kubernetes: remove some of the klog setup\nI don't believe this is actually needed (anymore). The:\nos.Stderr = os.Stdout\nis a crazy hack that def. needs to go."}
{"diff_id": 3170, "repo": "coredns/coredns", "sha": "1e3330c12b285de45888942d8dd69c6e4253098e", "time": "15.11.2019 14:14:29", "diff": "mmm a / plugin/pkg/fall/fall.go <nl> ppp b / plugin/pkg/fall/fall.go <nl>-// Package fall handles the fallthrough logic used in plugins that support it. <nl> +// Package fall handles the fallthrough logic used in plugins that support it. Be careful when including this <nl> +// functionality in your plugin. Why? In the DNS only 1 source is authoritative for a set of names. Fallthrough <nl> +// breaks this convention by allowing a plugin to query multiple sources, depending on the replies it got sofar. <nl> +// <nl> +// This may cause issues in downstream caches, where different answers for the same query can potentially confuse clients. <nl> +// On the other hand this is a powerful feature that can aid in migration or other edge cases. <nl> +// <nl> +// The take away: be mindful of this and don't blindly assume it's a good feature to have in your plugin. <nl> +// <nl> +// See http://github.com/coredns/coredns/issues/2723 for some discussion on this, which includes this quote: <nl> +// <nl> +// TL;DR: `fallthrough` is indeed risky and hackish, but still a good feature of CoreDNS as it allows to quickly answer boring edge cases. <nl> +// <nl> package fall <nl> import ( <nl> ", "msg": "pkg/fall: add (a lot of) guidance\nupdate the pkg doc to talk about various trade off\nFixes"}
{"diff_id": 3204, "repo": "coredns/coredns", "sha": "296222d613ee2dd52b0916ccb515cc44efc4b37e", "time": "10.01.2021 08:30:00", "diff": "mmm a / plugin/dnssec/cache.go <nl> ppp b / plugin/dnssec/cache.go <nl>@@ -2,6 +2,9 @@ package dnssec <nl> import ( <nl> \"hash/fnv\" <nl> + \"io\" <nl> + \"strconv\" <nl> + \"strings\" <nl> \"github.com/miekg/dns\" <nl> ) <nl> @@ -9,14 +12,16 @@ import ( <nl> // hash serializes the RRset and returns a signature cache key. <nl> func hash(rrs []dns.RR) uint64 { <nl> h := fnv.New64() <nl> - buf := make([]byte, 256) <nl> - for _, r := range rrs { <nl> - off, err := dns.PackRR(r, buf, 0, nil, false) <nl> - if err == nil { <nl> - h.Write(buf[:off]) <nl> + // Only need this to be unique for ownername + qtype (+class), but we <nl> + // only care about IN. Its already an RRSet, so the ownername is the <nl> + // same as is the qtype. Take the first one and construct the hash <nl> + // string that creates the key <nl> + io.WriteString(h, strings.ToLower(rrs[0].Header().Name)) <nl> + typ, ok := dns.TypeToString[rrs[0].Header().Rrtype] <nl> + if !ok { <nl> + typ = \"TYPE\" + strconv.FormatUint(uint64(rrs[0].Header().Rrtype), 10) <nl> } <nl> - } <nl> - <nl> + io.WriteString(h, typ) <nl> i := h.Sum64() <nl> return i <nl> } <nl> ", "msg": "plugin/dnssec: Change hash key input\nMake this vastly simpler and more efficient. Adding all the bytes and\nthen letting loose fnv doesn't add anything and may actually do the\nwrong thing.\nSee:\nFixes:"}
{"diff_id": 3228, "repo": "coredns/coredns", "sha": "1029fea9068f6d7ef951256d6fe6d4e82acd3193", "time": "15.11.2021 20:29:52", "diff": "mmm a / plugin/health/overloaded.go <nl> ppp b / plugin/health/overloaded.go <nl>@@ -53,7 +53,7 @@ var ( <nl> Buckets: plugin.SlimTimeBuckets, <nl> Help: \"Histogram of the time (in seconds) each request took.\", <nl> }) <nl> - // HealthFailures is the metric used to count how many times the thealth request failed <nl> + // HealthFailures is the metric used to count how many times the health request failed <nl> HealthFailures = promauto.NewCounter(prometheus.CounterOpts{ <nl> Namespace: plugin.Namespace, <nl> Subsystem: \"health\", <nl> ", "msg": "Fix a typo in plugin/health"}
{"diff_id": 3251, "repo": "coredns/coredns", "sha": "1f0a41a66597cb8ab4aace8ea5b5bad880bcd23b", "time": "08.09.2022 10:35:03", "diff": "mmm a / coremain/version.go <nl> ppp b / coremain/version.go <nl>@@ -2,7 +2,7 @@ package coremain <nl> // Various CoreDNS constants. <nl> const ( <nl> - CoreVersion = \"1.9.3\" <nl> + CoreVersion = \"1.9.4\" <nl> coreName = \"CoreDNS\" <nl> serverType = \"dns\" <nl> ) <nl> ", "msg": "Update Release note and prepare 1.9.4 release\n* Update Release note and prepare 1.9.4 release\nThis PR updates release notes and prepare 1.9.4 release\n* Remove spaces\n* Add additional note for header plugin change."}
{"diff_id": 3258, "repo": "filebrowser/filebrowser", "sha": "e3416a6181ae795823dfaed77b8e3cd365534a6a", "time": "21.01.2017 20:11:48", "diff": "mmm a / setup.go <nl> ppp b / setup.go <nl>@@ -203,7 +203,14 @@ func getFrontMatter(conf *Config) string { <nl> log.Println(err) <nl> fmt.Printf(\"Can't get the default frontmatter from the configuration. %s will be used.\\n\", format) <nl> } else { <nl> - bytes = frontmatter.AppendRune(bytes, frontmatter.StringFormatToRune(format)) <nl> + r, err := frontmatter.StringFormatToRune(format) <nl> + if err != nil { <nl> + log.Println(err) <nl> + fmt.Printf(\"Can't get the default frontmatter from the configuration. %s will be used.\\n\", format) <nl> + return format <nl> + } <nl> + <nl> + bytes = frontmatter.AppendRune(bytes, r) <nl> f, err := frontmatter.Unmarshal(bytes) <nl> if err != nil { <nl> ", "msg": "fix critical bug"}
{"diff_id": 3296, "repo": "filebrowser/filebrowser", "sha": "610d55c26f0172515900878f062b4dd703f45775", "time": "24.08.2017 12:33:54", "diff": "mmm a / http/resource.go <nl> ppp b / http/resource.go <nl>@@ -160,7 +160,7 @@ func resourcePostPutHandler(c *fm.Context, w http.ResponseWriter, r *http.Reques <nl> } <nl> // Discard any invalid upload before returning to avoid connection <nl> - // reset fm.Error. <nl> + // reset error. <nl> defer func() { <nl> io.Copy(ioutil.Discard, r.Body) <nl> }() <nl> @@ -179,9 +179,9 @@ func resourcePostPutHandler(c *fm.Context, w http.ResponseWriter, r *http.Reques <nl> } <nl> // If using POST method, we are trying to create a new file so it is not <nl> - // desirable to ovfm.Erride an already existent file. Thus, we check <nl> + // desirable to override an already existent file. Thus, we check <nl> // if the file already exists. If so, we just return a 409 Conflict. <nl> - if r.Method == http.MethodPost && r.Header.Get(\"Action\") != \"ovfm.Erride\" { <nl> + if r.Method == http.MethodPost && r.Header.Get(\"Action\") != \"override\" { <nl> if _, err := c.User.FileSystem.Stat(r.URL.Path); err == nil { <nl> return http.StatusConflict, errors.New(\"There is already a file on that path\") <nl> } <nl> ", "msg": "Fix override."}
{"diff_id": 3325, "repo": "filebrowser/filebrowser", "sha": "3890a9a416c8bd1a5d329ddbb5284cfe1f6f5910", "time": "06.01.2019 09:24:09", "diff": "mmm a / files/file.go <nl> ppp b / files/file.go <nl>@@ -76,7 +76,7 @@ func NewFileInfo(opts FileOptions) (*FileInfo, error) { <nl> return file, file.readListing(opts.Checker) <nl> } <nl> - err = file.detectType(opts.Modify) <nl> + err = file.detectType(opts.Modify, true) <nl> if err != nil { <nl> return nil, err <nl> } <nl> @@ -126,7 +126,7 @@ func (i *FileInfo) Checksum(algo string) error { <nl> return nil <nl> } <nl> -func (i *FileInfo) detectType(modify bool) error { <nl> +func (i *FileInfo) detectType(modify, saveContent bool) error { <nl> reader, err := i.Fs.Open(i.Path) <nl> if err != nil { <nl> return err <nl> @@ -160,18 +160,21 @@ func (i *FileInfo) detectType(modify bool) error { <nl> return nil <nl> default: <nl> i.Type = \"text\" <nl> + <nl> + if !modify { <nl> + i.Type = \"textImmutable\" <nl> + } <nl> + <nl> + if saveContent { <nl> afs := &afero.Afero{Fs: i.Fs} <nl> content, err := afs.ReadFile(i.Path) <nl> if err != nil { <nl> return err <nl> } <nl> - if !modify { <nl> - i.Type = \"textImmutable\" <nl> - } <nl> - <nl> i.Content = string(content) <nl> } <nl> + } <nl> return nil <nl> } <nl> @@ -238,7 +241,7 @@ func (i *FileInfo) readListing(checker rules.Checker) error { <nl> } else { <nl> listing.NumFiles++ <nl> - err := file.detectType(true) <nl> + err := file.detectType(true, false) <nl> if err != nil { <nl> return err <nl> } <nl> ", "msg": "fix: do not read whole file on listings\nLicense: MIT"}
{"diff_id": 3355, "repo": "filebrowser/filebrowser", "sha": "65ac73414fadc4686c94803a93ff319e8f7ce9d1", "time": "11.09.2020 15:59:06", "diff": "mmm a / cmd/root.go <nl> ppp b / cmd/root.go <nl>@@ -58,6 +58,7 @@ func addServerFlags(flags *pflag.FlagSet) { <nl> flags.StringP(\"key\", \"k\", \"\", \"tls key\") <nl> flags.StringP(\"root\", \"r\", \".\", \"root to prepend to relative paths\") <nl> flags.String(\"socket\", \"\", \"socket to listen to (cannot be used with address, port, cert nor key flags)\") <nl> + flags.Uint32(\"socket-perm\", 0666, \"unix socket file permissions\") <nl> flags.StringP(\"baseurl\", \"b\", \"\", \"base url\") <nl> flags.String(\"cache-dir\", \"\", \"file cache directory (disabled if empty)\") <nl> flags.Int(\"img-processors\", 4, \"image processors count\") <nl> @@ -143,6 +144,10 @@ user created with the credentials from options \"username\" and \"password\".`, <nl> case server.Socket != \"\": <nl> listener, err = net.Listen(\"unix\", server.Socket) <nl> checkErr(err) <nl> + socketPerm, err := cmd.Flags().GetUint32(\"socket-perm\") <nl> + checkErr(err) <nl> + err = os.Chmod(server.Socket, os.FileMode(socketPerm)) <nl> + checkErr(err) <nl> case server.TLSKey != \"\" && server.TLSCert != \"\": <nl> cer, err := tls.LoadX509KeyPair(server.TLSCert, server.TLSKey) //nolint:shadow <nl> checkErr(err) <nl> ", "msg": "feat: add --socket-perm flag to control unix socket file permissions (closes"}
{"diff_id": 3370, "repo": "filebrowser/filebrowser", "sha": "e1a6f593e1824e7fa4345a61dff5b1bb8cd22d05", "time": "23.03.2021 13:13:46", "diff": "mmm a / http/resource.go <nl> ppp b / http/resource.go <nl>@@ -125,7 +125,10 @@ func resourcePostHandler(fileCache FileCache) handleFunc { <nl> } <nl> err = d.RunHook(func() error { <nl> - info, _ := writeFile(d.user.Fs, r.URL.Path, r.Body) <nl> + info, writeErr := writeFile(d.user.Fs, r.URL.Path, r.Body) <nl> + if writeErr != nil { <nl> + return writeErr <nl> + } <nl> etag := fmt.Sprintf(`\"%x%x\"`, info.ModTime().UnixNano(), info.Size()) <nl> w.Header().Set(\"ETag\", etag) <nl> @@ -155,7 +158,10 @@ var resourcePutHandler = withUser(func(w http.ResponseWriter, r *http.Request, d <nl> } <nl> err := d.RunHook(func() error { <nl> - info, _ := writeFile(d.user.Fs, r.URL.Path, r.Body) <nl> + info, writeErr := writeFile(d.user.Fs, r.URL.Path, r.Body) <nl> + if writeErr != nil { <nl> + return writeErr <nl> + } <nl> etag := fmt.Sprintf(`\"%x%x\"`, info.ModTime().UnixNano(), info.Size()) <nl> w.Header().Set(\"ETag\", etag) <nl> ", "msg": "fix: error causes panic on upload"}
{"diff_id": 3408, "repo": "cloudfoundry/loggregator-release", "sha": "6c48c23705ff1fbf7b5e8e3a73a79e04083f0c0f", "time": "23.01.2017 10:58:07", "diff": "mmm a / src/integration_tests/doppler/doppler_suite_test.go <nl> ppp b / src/integration_tests/doppler/doppler_suite_test.go <nl>@@ -93,7 +93,7 @@ var _ = BeforeEach(func() { <nl> conn.Close() <nl> return true <nl> } <nl> - Eventually(dopplerStartedFn).Should(BeTrue()) <nl> + Eventually(dopplerStartedFn, 3).Should(BeTrue()) <nl> localIPAddress, _ = localip.LocalIP() <nl> Eventually(func() error { <nl> _, err := etcdAdapter.Get(\"healthstatus/doppler/z1/doppler_z1/0\") <nl> ", "msg": "Allow three seconds for doppler to come up"}
{"diff_id": 3423, "repo": "cloudfoundry/loggregator-release", "sha": "bad06177e16aad0f897ce7da45316184c8e9ca96", "time": "21.02.2017 09:45:27", "diff": "mmm a / src/plumbing/conversion/end_to_end_test.go <nl> ppp b / src/plumbing/conversion/end_to_end_test.go <nl>@@ -3,6 +3,8 @@ package conversion_test <nl> import ( <nl> . \"plumbing/conversion\" <nl> + v2 \"plumbing/v2\" <nl> + <nl> \"github.com/cloudfoundry/sonde-go/events\" <nl> \"github.com/gogo/protobuf/proto\" <nl> @@ -150,3 +152,56 @@ var _ = DescribeTable(\"v1->v2->v1\", <nl> }, <nl> }), <nl> ) <nl> + <nl> +var ValueText = func(s string) *v2.Value { <nl> + return &v2.Value{&v2.Value_Text{Text: s}} <nl> +} <nl> + <nl> +var ValueInteger = func(i int64) *v2.Value { <nl> + return &v2.Value{&v2.Value_Integer{Integer: i}} <nl> +} <nl> + <nl> +var _ = FDescribeTable(\"v2->v1->v2\", <nl> + <nl> + func(v2e *v2.Envelope) { <nl> + _, err := proto.Marshal(v2e) <nl> + Expect(err).ToNot(HaveOccurred()) <nl> + <nl> + v1e := ToV1(v2e) <nl> + <nl> + _, err = proto.Marshal(v1e) <nl> + Expect(err).ToNot(HaveOccurred()) <nl> + <nl> + newV2e := ToV2(v1e) <nl> + Expect(newV2e).To(Equal(v2e)) <nl> + }, <nl> + Entry(\"HttpStartStop\", &v2.Envelope{ <nl> + SourceId: \"b3015d69-09cd-476d-aace-ad2d824d5ab7\", <nl> + Message: &v2.Envelope_Timer{ <nl> + Timer: &v2.Timer{ <nl> + Name: \"http\", <nl> + Start: 99, <nl> + Stop: 100, <nl> + }, <nl> + }, <nl> + Tags: map[string]*v2.Value{ <nl> + \"request_id\": ValueText(\"954f61c4-ac84-44be-9217-cdfa3117fb41\"), <nl> + \"peer_type\": ValueText(\"Client\"), <nl> + \"method\": ValueText(\"GET\"), <nl> + \"uri\": ValueText(\"/hello-world\"), <nl> + \"remote_address\": ValueText(\"10.1.1.0\"), <nl> + \"user_agent\": ValueText(\"Mozilla/5.0\"), <nl> + \"status_code\": ValueInteger(200), <nl> + \"content_length\": ValueInteger(1000000), <nl> + \"instance_index\": ValueInteger(10), <nl> + \"instance_id\": ValueText(\"application-id\"), <nl> + \"forwarded\": ValueText(\"6.6.6.6\\n8.8.8.8\"), <nl> + \"deployment\": ValueText(\"some-deployment\"), <nl> + \"ip\": ValueText(\"some-ip\"), <nl> + \"job\": ValueText(\"some-job\"), <nl> + \"origin\": ValueText(\"some-origin\"), <nl> + \"index\": ValueText(\"some-index\"), <nl> + \"__v1_type\": ValueText(\"HttpStartStop\"), <nl> + }, <nl> + }), <nl> +) <nl> ", "msg": "Add additional e2e test for v2->v1->v2 conversion"}
{"diff_id": 3426, "repo": "cloudfoundry/loggregator-release", "sha": "19369fadc9e3ca9ccbddcb124a7102e7fdd2bd66", "time": "22.02.2017 11:26:48", "diff": "mmm a / src/testservers/certificates.go <nl> ppp b / src/testservers/certificates.go <nl>package testservers <nl> -//go:generate bash -c \"../../scripts/generate-loggregator-certs no-bbs-ca && go-bindata -nocompress -pkg testservers -prefix loggregator-certs/ loggregator-certs/\" <nl> +//go:generate ../../scripts/generate-loggregator-certs no-bbs-ca <nl> +//go:generate go-bindata -nocompress -pkg testservers -prefix loggregator-certs/ loggregator-certs/ <nl> import ( <nl> \"io/ioutil\" <nl> @@ -43,6 +44,24 @@ func CACertPath() string { <nl> return createTempFile(MustAsset(\"loggregator-ca.crt\")) <nl> } <nl> +func Cert(filename string) string { <nl> + contents := MustAsset(filename) <nl> + tmpfile, err := ioutil.TempFile(\"\", \"\") <nl> + <nl> + if err != nil { <nl> + log.Fatal(err) <nl> + } <nl> + <nl> + if _, err := tmpfile.Write(contents); err != nil { <nl> + log.Fatal(err) <nl> + } <nl> + if err := tmpfile.Close(); err != nil { <nl> + log.Fatal(err) <nl> + } <nl> + <nl> + return tmpfile.Name() <nl> +} <nl> + <nl> func createTempFile(contents []byte) string { <nl> tmpfile, err := ioutil.TempFile(\"\", \"\") <nl> ", "msg": "Use Cert to write files and split go generate"}
{"diff_id": 3436, "repo": "cloudfoundry/loggregator-release", "sha": "575d69fe2136d533ebace5ffaf0e7cb7e1e9afad", "time": "09.03.2017 14:25:54", "diff": "mmm a / src/doppler/grpcmanager/v1/router.go <nl> ppp b / src/doppler/grpcmanager/v1/router.go <nl>@@ -8,14 +8,16 @@ import ( <nl> \"github.com/cloudfoundry/sonde-go/events\" <nl> ) <nl> +type shardID string <nl> + <nl> type Router struct { <nl> lock sync.RWMutex <nl> - subscriptions map[plumbing.Filter]map[string][]DataSetter <nl> + subscriptions map[plumbing.Filter]map[shardID][]DataSetter <nl> } <nl> func NewRouter() *Router { <nl> return &Router{ <nl> - subscriptions: make(map[plumbing.Filter]map[string][]DataSetter), <nl> + subscriptions: make(map[plumbing.Filter]map[shardID][]DataSetter), <nl> } <nl> } <nl> @@ -42,18 +44,18 @@ func (r *Router) SendTo(appID string, envelope *events.Envelope) { <nl> AppID: appID, <nl> } <nl> - for shardID, setters := range r.subscriptions[filter] { <nl> - r.writeToShard(shardID, setters, data) <nl> + for id, setters := range r.subscriptions[filter] { <nl> + r.writeToShard(id, setters, data) <nl> } <nl> var noFilter plumbing.Filter <nl> - for shardID, setters := range r.subscriptions[noFilter] { <nl> - r.writeToShard(shardID, setters, data) <nl> + for id, setters := range r.subscriptions[noFilter] { <nl> + r.writeToShard(id, setters, data) <nl> } <nl> } <nl> -func (r *Router) writeToShard(shardID string, setters []DataSetter, data []byte) { <nl> - if shardID == \"\" { <nl> +func (r *Router) writeToShard(id shardID, setters []DataSetter, data []byte) { <nl> + if id == \"\" { <nl> for _, setter := range setters { <nl> setter.Set(data) <nl> } <nl> @@ -71,11 +73,11 @@ func (r *Router) registerSetter(req *plumbing.SubscriptionRequest, dataSetter Da <nl> m, ok := r.subscriptions[filter] <nl> if !ok { <nl> - m = make(map[string][]DataSetter) <nl> + m = make(map[shardID][]DataSetter) <nl> r.subscriptions[filter] = m <nl> } <nl> - m[req.ShardID] = append(m[req.ShardID], dataSetter) <nl> + m[shardID(req.ShardID)] = append(m[shardID(req.ShardID)], dataSetter) <nl> } <nl> func (r *Router) buildCleanup(req *plumbing.SubscriptionRequest, dataSetter DataSetter) func() { <nl> @@ -89,18 +91,18 @@ func (r *Router) buildCleanup(req *plumbing.SubscriptionRequest, dataSetter Data <nl> } <nl> var setters []DataSetter <nl> - for _, s := range r.subscriptions[filter][req.ShardID] { <nl> + for _, s := range r.subscriptions[filter][shardID(req.ShardID)] { <nl> if s != dataSetter { <nl> setters = append(setters, s) <nl> } <nl> } <nl> if len(setters) > 0 { <nl> - r.subscriptions[filter][req.ShardID] = setters <nl> + r.subscriptions[filter][shardID(req.ShardID)] = setters <nl> return <nl> } <nl> - delete(r.subscriptions[filter], req.ShardID) <nl> + delete(r.subscriptions[filter], shardID(req.ShardID)) <nl> if len(r.subscriptions[filter]) == 0 { <nl> delete(r.subscriptions, filter) <nl> ", "msg": "Use type alias for readability"}
{"diff_id": 3438, "repo": "cloudfoundry/loggregator-release", "sha": "272540f556e1c44055cc73c5651fffc3bb2d89f6", "time": "10.03.2017 09:39:54", "diff": "mmm a / src/doppler/grpcmanager/v1/router.go <nl> ppp b / src/doppler/grpcmanager/v1/router.go <nl>@@ -12,12 +12,16 @@ type shardID string <nl> type Router struct { <nl> lock sync.RWMutex <nl> - subscriptions map[plumbing.Filter]map[shardID][]DataSetter <nl> + subscriptions map[filter]map[shardID][]DataSetter <nl> +} <nl> + <nl> +type filter struct { <nl> + appID string <nl> } <nl> func NewRouter() *Router { <nl> return &Router{ <nl> - subscriptions: make(map[plumbing.Filter]map[shardID][]DataSetter), <nl> + subscriptions: make(map[filter]map[shardID][]DataSetter), <nl> } <nl> } <nl> @@ -40,15 +44,15 @@ func (r *Router) SendTo(appID string, envelope *events.Envelope) { <nl> return <nl> } <nl> - filter := plumbing.Filter{ <nl> - AppID: appID, <nl> + singleAppFilter := filter{ <nl> + appID: appID, <nl> } <nl> - for id, setters := range r.subscriptions[filter] { <nl> + for id, setters := range r.subscriptions[singleAppFilter] { <nl> r.writeToShard(id, setters, data) <nl> } <nl> - var noFilter plumbing.Filter <nl> + var noFilter filter <nl> for id, setters := range r.subscriptions[noFilter] { <nl> r.writeToShard(id, setters, data) <nl> } <nl> @@ -66,15 +70,12 @@ func (r *Router) writeToShard(id shardID, setters []DataSetter, data []byte) { <nl> } <nl> func (r *Router) registerSetter(req *plumbing.SubscriptionRequest, dataSetter DataSetter) { <nl> - var filter plumbing.Filter <nl> - if req.Filter != nil { <nl> - filter = *req.Filter <nl> - } <nl> + f := r.convertFilter(req) <nl> - m, ok := r.subscriptions[filter] <nl> + m, ok := r.subscriptions[f] <nl> if !ok { <nl> m = make(map[shardID][]DataSetter) <nl> - r.subscriptions[filter] = m <nl> + r.subscriptions[f] = m <nl> } <nl> m[shardID(req.ShardID)] = append(m[shardID(req.ShardID)], dataSetter) <nl> @@ -85,27 +86,23 @@ func (r *Router) buildCleanup(req *plumbing.SubscriptionRequest, dataSetter Data <nl> r.lock.Lock() <nl> defer r.lock.Unlock() <nl> - var filter plumbing.Filter <nl> - if req.Filter != nil { <nl> - filter = *req.Filter <nl> - } <nl> - <nl> + f := r.convertFilter(req) <nl> var setters []DataSetter <nl> - for _, s := range r.subscriptions[filter][shardID(req.ShardID)] { <nl> + for _, s := range r.subscriptions[f][shardID(req.ShardID)] { <nl> if s != dataSetter { <nl> setters = append(setters, s) <nl> } <nl> } <nl> if len(setters) > 0 { <nl> - r.subscriptions[filter][shardID(req.ShardID)] = setters <nl> + r.subscriptions[f][shardID(req.ShardID)] = setters <nl> return <nl> } <nl> - delete(r.subscriptions[filter], shardID(req.ShardID)) <nl> + delete(r.subscriptions[f], shardID(req.ShardID)) <nl> - if len(r.subscriptions[filter]) == 0 { <nl> - delete(r.subscriptions, filter) <nl> + if len(r.subscriptions[f]) == 0 { <nl> + delete(r.subscriptions, f) <nl> } <nl> } <nl> } <nl> @@ -118,3 +115,13 @@ func (r *Router) marshal(envelope *events.Envelope) []byte { <nl> return data <nl> } <nl> + <nl> +func (r *Router) convertFilter(req *plumbing.SubscriptionRequest) filter { <nl> + if req.GetFilter() == nil { <nl> + return filter{} <nl> + } <nl> + <nl> + return filter{ <nl> + appID: req.Filter.AppID, <nl> + } <nl> +} <nl> ", "msg": "Doppler uses private struct for filter instead of generated filter"}
{"diff_id": 3454, "repo": "cloudfoundry/loggregator-release", "sha": "ca8dc7d04669b3f9288a6ef7060df229862f143f", "time": "23.03.2017 11:02:08", "diff": "mmm a / src/metron/component_tests/metron_test.go <nl> ppp b / src/metron/component_tests/metron_test.go <nl>@@ -145,13 +145,14 @@ var _ = Describe(\"Metron\", func() { <nl> } <nl> client := metronClient(metronConfig) <nl> - ctx, _ := context.WithDeadline(context.Background(), time.Now().Add(10*time.Second)) <nl> + ctx, _ := context.WithDeadline(context.Background(), time.Now().Add(20*time.Second)) <nl> sender, err := client.Sender(ctx) <nl> Expect(err).ToNot(HaveOccurred()) <nl> go func() { <nl> for { <nl> sender.Send(emitEnvelope) <nl> + time.Sleep(10 * time.Millisecond) <nl> } <nl> }() <nl> @@ -159,14 +160,13 @@ var _ = Describe(\"Metron\", func() { <nl> Eventually(consumerServer.V2.SenderInput.Arg0).Should(Receive(&rx)) <nl> f := func() bool { <nl> - sender.Send(emitEnvelope) <nl> envelope, err := rx.Recv() <nl> Expect(err).ToNot(HaveOccurred()) <nl> return envelope.GetCounter() != nil && <nl> envelope.GetCounter().GetTotal() > 5 <nl> } <nl> - Eventually(f, 15, \"1ns\").Should(Equal(true)) <nl> + Eventually(f, 20, \"1ns\").Should(Equal(true)) <nl> }) <nl> }) <nl> ", "msg": "Tweak metron component test to prevent deadlock"}
{"diff_id": 3468, "repo": "cloudfoundry/loggregator-release", "sha": "016c1ec90cdcc8cda01142a7978433a0deedd36a", "time": "13.04.2017 10:01:07", "diff": "mmm a / src/doppler/internal/groupedsinks/grouped_sinks.go <nl> ppp b / src/doppler/internal/groupedsinks/grouped_sinks.go <nl>package groupedsinks <nl> import ( <nl> + \"metric\" <nl> \"sync\" <nl> \"doppler/internal/groupedsinks/firehose_group\" <nl> @@ -99,6 +100,10 @@ func (group *GroupedSinks) Broadcast(appId string, msg *events.Envelope) { <nl> // metric-documentation-v1: (sinks.dropped) Number of envelopes dropped <nl> // while inserting envelope into sink. <nl> group.batcher.BatchIncrementCounter(\"sinks.dropped\") <nl> + <nl> + // metric-documentation-v2: (loggregator.doppler.sinks.dropped) <nl> + // Number of envelopes dropped while inserting envelope into sink. <nl> + metric.IncCounter(\"sinks.dropped\", metric.WithVersion(2, 0)) <nl> } <nl> } <nl> @@ -117,6 +122,10 @@ func (group *GroupedSinks) BroadcastError(appId string, errorMsg *events.Envelop <nl> // metric-documentation-v1: (sinks.errors.dropped) Number of errors dropped <nl> // while inserting error into sink. <nl> group.batcher.BatchIncrementCounter(\"sinks.errors.dropped\") <nl> + <nl> + // metric-documentation-v2: (loggregator.doppler.sinks.errors.dropped) <nl> + // Number of errors dropped while inserting error into sink. <nl> + metric.IncCounter(\"sinks.errors.dropped\", metric.WithVersion(2, 0)) <nl> } <nl> } <nl> } <nl> ", "msg": "Emit v2 metric for drops at sink insertion"}
{"diff_id": 3489, "repo": "cloudfoundry/loggregator-release", "sha": "b7a069fd93660048f54e6a4989eb4d5acab4c6b9", "time": "04.05.2017 15:22:49", "diff": "mmm a / src/integration_tests/endtoend/endtoend_suite_test.go <nl> ppp b / src/integration_tests/endtoend/endtoend_suite_test.go <nl>@@ -10,7 +10,9 @@ import ( <nl> func TestIntegrationTest(t *testing.T) { <nl> RegisterFailHandler(Fail) <nl> - RunSpecs(t, \"End to end Integration Test Suite\") <nl> + <nl> + // This test is pending for being flaky... <nl> + // RunSpecs(t, \"End to end Integration Test Suite\") <nl> } <nl> var _ = SynchronizedBeforeSuite(func() []byte { <nl> ", "msg": "Fully pend end2end test\nThe spec is pended, however the setup code is still running"}
{"diff_id": 3491, "repo": "cloudfoundry/loggregator-release", "sha": "28d04350681ce44db419b2c9341b19f3bfbaafea", "time": "09.05.2017 10:42:04", "diff": "mmm a / src/metron/internal/ingress/v2/receiver.go <nl> ppp b / src/metron/internal/ingress/v2/receiver.go <nl>@@ -35,6 +35,8 @@ func (s *Receiver) Sender(sender v2.Ingress_SenderServer) error { <nl> } <nl> s.dataSetter.Set(e) <nl> + // metric-documentation-v2: (loggregator.metron.ingress) The number of <nl> + // received messages over Metrons V2 gRPC API. <nl> s.ingressMetric.Increment(1) <nl> } <nl> @@ -53,6 +55,8 @@ func (s *Receiver) BatchSender(sender v2.Ingress_BatchSenderServer) error { <nl> s.dataSetter.Set(e) <nl> } <nl> + // metric-documentation-v2: (loggregator.metron.ingress) The number of <nl> + // received messages over Metrons V2 gRPC API. <nl> s.ingressMetric.Increment(uint64(len(envelopes.Batch))) <nl> } <nl> ", "msg": "add back metric documentation for metron ingress"}
{"diff_id": 3493, "repo": "cloudfoundry/loggregator-release", "sha": "018ce2b885b723cfdebd19478808bc39ebcce62d", "time": "10.05.2017 14:35:11", "diff": "mmm a / src/metron/internal/clientpool/v1/pusher_fetcher_test.go <nl> ppp b / src/metron/internal/clientpool/v1/pusher_fetcher_test.go <nl>@@ -44,7 +44,7 @@ var _ = Describe(\"PusherFetcher\", func() { <nl> Expect(registry.GetValue(\"doppler_v1_streams\")).To(Equal(int64(1))) <nl> }) <nl> - It(\"decremtns a counter when a connection is closed\", func() { <nl> + It(\"decremetns a counter when a connection is closed\", func() { <nl> server := newSpyIngestorServer() <nl> Expect(server.Start()).To(Succeed()) <nl> defer server.Stop() <nl> @@ -108,7 +108,7 @@ func newSpyIngestorServer() *SpyIngestorServer { <nl> } <nl> func (s *SpyIngestorServer) Start() error { <nl> - lis, err := net.Listen(\"tcp\", \":0\") <nl> + lis, err := net.Listen(\"tcp\", \"localhost:0\") <nl> if err != nil { <nl> return err <nl> } <nl> ", "msg": "explicitly set host to localhost in clientpool test"}
{"diff_id": 3496, "repo": "cloudfoundry/loggregator-release", "sha": "5a2b5835ccacdeabce53db26c7b2e23ace0034c3", "time": "15.05.2017 12:03:37", "diff": "mmm a / src/tools/udpwriter/main.go <nl> ppp b / src/tools/udpwriter/main.go <nl>@@ -20,7 +20,7 @@ import ( <nl> ) <nl> var ( <nl> - target = flag.String(\"target\", \"\", \"the host:port of the target metron\") <nl> + target = flag.String(\"target\", \"localhost:3457\", \"the host:port of the target metron\") <nl> fast = flag.Duration(\"fast\", time.Second, \"the delay of the fast writer\") <nl> slow = flag.Duration(\"slow\", time.Second, \"the delay of the slow writer\") <nl> ) <nl> ", "msg": "Default udpwriter to talking to local metron"}
{"diff_id": 3509, "repo": "cloudfoundry/loggregator-release", "sha": "68d96c3323e332e9aa5593f634ccf6816a113cf9", "time": "25.05.2017 13:40:58", "diff": "mmm a / src/rlp/internal/egress/server.go <nl> ppp b / src/rlp/internal/egress/server.go <nl>@@ -72,15 +72,15 @@ func (s *Server) Receiver(r *v2.EgressRequest, srv v2.Egress_ReceiverServer) err <nl> return io.ErrUnexpectedEOF <nl> } <nl> - // metric-documentation-v2: (egress) Number of v2 envelopes sent to RLP <nl> - // consumers. <nl> + // metric-documentation-v2: (loggregator.rlp.egress) Number of v2 <nl> + // envelopes sent to RLP consumers. <nl> s.egressMetric.Increment(1) <nl> } <nl> } <nl> func (s *Server) Alert(missed int) { <nl> - // metric-documentation-v2: (dropped) Number of v2 envelopes dropped <nl> - // while egressing to a consumer. <nl> + // metric-documentation-v2: (loggregator.rlp.dropped) Number of v2 <nl> + // envelopes dropped while egressing to a consumer. <nl> s.droppedMetric.Increment(uint64(missed)) <nl> log.Printf(\"Dropped (egress) %d envelopes\", missed) <nl> } <nl> ", "msg": "Updates metric documentation\nTo be consistent with other metric docs."}
{"diff_id": 3516, "repo": "cloudfoundry/loggregator-release", "sha": "a8ce30e5a4c3d7b587963457f580dc320e3e1d7a", "time": "02.06.2017 11:27:22", "diff": "mmm a / src/integration_tests/doppler/container_metrics_test.go <nl> ppp b / src/integration_tests/doppler/container_metrics_test.go <nl>@@ -17,38 +17,42 @@ import ( <nl> var _ = Describe(\"Container Metrics\", func() { <nl> var ( <nl> - receivedChan chan []byte <nl> appID string <nl> - conn *grpc.ClientConn <nl> - client plumbing.DopplerIngestor_PusherClient <nl> + ingressConn, egressConn *grpc.ClientConn <nl> + ingressClient plumbing.DopplerIngestor_PusherClient <nl> + egressClient plumbing.DopplerClient <nl> ) <nl> Context(\"gRPC V1\", func() { <nl> JustBeforeEach(func() { <nl> - conn, client = dopplerIngressV1Client(\"localhost:5678\") <nl> + ingressConn, ingressClient = dopplerIngressV1Client(\"localhost:5678\") <nl> guid, _ := uuid.NewV4() <nl> appID = guid.String() <nl> + <nl> + conf := fetchDopplerConfig(\"fixtures/doppler.json\") <nl> + egressConn, egressClient = connectToGRPC(conf) <nl> }) <nl> AfterEach(func() { <nl> - conn.Close() <nl> + ingressConn.Close() <nl> + egressConn.Close() <nl> }) <nl> It(\"returns container metrics for an app\", func() { <nl> containerMetric := factories.NewContainerMetric(appID, 0, 1, 2, 3) <nl> - client.Send(marshalContainerMetric(containerMetric)) <nl> + ingressClient.Send(marshalContainerMetric(containerMetric)) <nl> time.Sleep(5 * time.Second) <nl> - receivedChan = make(chan []byte) <nl> - ws, _ := AddWSSink(receivedChan, \"4567\", \"/apps/\"+appID+\"/containermetrics\") <nl> - defer ws.Close() <nl> - <nl> - var receivedMessageBytes []byte <nl> - Eventually(receivedChan).Should(Receive(&receivedMessageBytes)) <nl> + ctx, _ := context.WithTimeout(context.TODO(), time.Second) <nl> + resp, err := egressClient.ContainerMetrics(ctx, &plumbing.ContainerMetricsRequest{ <nl> + AppID: appID, <nl> + }) <nl> + Expect(err).ToNot(HaveOccurred()) <nl> - receivedEnvelope := UnmarshalMessage(receivedMessageBytes) <nl> + Expect(resp.Payload).To(HaveLen(1)) <nl> + receivedEnvelope := UnmarshalMessage(resp.Payload[0]) <nl> Expect(receivedEnvelope.GetEventType()).To(Equal(events.Envelope_ContainerMetric)) <nl> receivedMetric := receivedEnvelope.GetContainerMetric() <nl> @@ -56,79 +60,50 @@ var _ = Describe(\"Container Metrics\", func() { <nl> }) <nl> It(\"does not receive metrics for different appIds\", func() { <nl> - client.Send(marshalContainerMetric( <nl> + ingressClient.Send(marshalContainerMetric( <nl> factories.NewContainerMetric(appID+\"other\", 0, 1, 2, 3), <nl> )) <nl> goodMetric := factories.NewContainerMetric(appID, 0, 100, 2, 3) <nl> - client.Send(marshalContainerMetric(goodMetric)) <nl> + ingressClient.Send(marshalContainerMetric(goodMetric)) <nl> - client.Send(marshalContainerMetric( <nl> + ingressClient.Send(marshalContainerMetric( <nl> factories.NewContainerMetric(appID+\"other\", 1, 1, 2, 3), <nl> )) <nl> time.Sleep(5 * time.Second) <nl> - receivedChan = make(chan []byte) <nl> - ws, _ := AddWSSink(receivedChan, \"4567\", \"/apps/\"+appID+\"/containermetrics\") <nl> - defer ws.Close() <nl> - <nl> - var receivedMessageBytes []byte <nl> - Eventually(receivedChan).Should(Receive(&receivedMessageBytes)) <nl> - Eventually(receivedChan).Should(BeClosed()) <nl> + ctx, _ := context.WithTimeout(context.TODO(), time.Second) <nl> + resp, err := egressClient.ContainerMetrics(ctx, &plumbing.ContainerMetricsRequest{ <nl> + AppID: appID, <nl> + }) <nl> + Expect(err).ToNot(HaveOccurred()) <nl> - receivedEnvelope := UnmarshalMessage(receivedMessageBytes) <nl> + Expect(resp.Payload).To(HaveLen(1)) <nl> + receivedEnvelope := UnmarshalMessage(resp.Payload[0]) <nl> Expect(receivedEnvelope.GetContainerMetric().GetApplicationId()).To(Equal(appID)) <nl> Expect(receivedEnvelope.GetContainerMetric()).To(Equal(goodMetric)) <nl> }) <nl> - XIt(\"returns metrics for all instances of the app\", func() { <nl> - client.Send(marshalContainerMetric( <nl> - factories.NewContainerMetric(appID, 0, 1, 2, 3), <nl> - )) <nl> - client.Send(marshalContainerMetric( <nl> - factories.NewContainerMetric(appID, 1, 1, 2, 3), <nl> - )) <nl> - <nl> - time.Sleep(5 * time.Second) <nl> - <nl> - receivedChan = make(chan []byte) <nl> - ws, _ := AddWSSink(receivedChan, \"4567\", \"/apps/\"+appID+\"/containermetrics\") <nl> - defer ws.Close() <nl> - <nl> - var firstReceivedMessageBytes []byte <nl> - var secondReceivedMessageBytes []byte <nl> - <nl> - Eventually(receivedChan).Should(Receive(&firstReceivedMessageBytes)) <nl> - Eventually(receivedChan).Should(Receive(&secondReceivedMessageBytes)) <nl> - <nl> - firstEnvelope := UnmarshalMessage(firstReceivedMessageBytes) <nl> - secondEnvelope := UnmarshalMessage(secondReceivedMessageBytes) <nl> - <nl> - Expect(firstEnvelope.GetContainerMetric().GetApplicationId()).To(Equal(appID)) <nl> - Expect(secondEnvelope.GetContainerMetric().GetApplicationId()).To(Equal(appID)) <nl> - Expect(firstEnvelope.GetContainerMetric()).NotTo(Equal(secondEnvelope.GetContainerMetric())) <nl> - }) <nl> - <nl> It(\"returns only the latest container metric\", func() { <nl> - client.Send(marshalContainerMetric( <nl> + ingressClient.Send(marshalContainerMetric( <nl> factories.NewContainerMetric(appID, 0, 10, 2, 3), <nl> )) <nl> laterMetric := factories.NewContainerMetric(appID, 0, 20, 2, 3) <nl> - client.Send(marshalContainerMetric(laterMetric)) <nl> + ingressClient.Send(marshalContainerMetric(laterMetric)) <nl> time.Sleep(5 * time.Second) <nl> - receivedChan = make(chan []byte) <nl> - ws, _ := AddWSSink(receivedChan, \"4567\", \"/apps/\"+appID+\"/containermetrics\") <nl> - defer ws.Close() <nl> - <nl> - var receivedMessageBytes []byte <nl> - Eventually(receivedChan).Should(Receive(&receivedMessageBytes)) <nl> + ctx, _ := context.WithTimeout(context.TODO(), time.Second) <nl> + resp, err := egressClient.ContainerMetrics(ctx, &plumbing.ContainerMetricsRequest{ <nl> + AppID: appID, <nl> + }) <nl> + Expect(err).ToNot(HaveOccurred()) <nl> - receivedEnvelope := UnmarshalMessage(receivedMessageBytes) <nl> + Expect(resp.Payload).To(HaveLen(1)) <nl> + receivedEnvelope := UnmarshalMessage(resp.Payload[0]) <nl> Expect(receivedEnvelope.GetContainerMetric()).To(Equal(laterMetric)) <nl> }) <nl> ", "msg": "Use gRPC consumer in doppler integration test\nUpdate doppler container metric integration tests to use gRPC consumer"}
{"diff_id": 3585, "repo": "cloudfoundry/loggregator-release", "sha": "9d8bca4af76a31b2cf90f65a56ae5d98fc80efb6", "time": "30.11.2017 14:46:19", "diff": "mmm a / src/tools/rlpreader/main.go <nl> ppp b / src/tools/rlpreader/main.go <nl>@@ -5,13 +5,9 @@ package main <nl> import ( <nl> \"context\" <nl> \"crypto/rand\" <nl> - \"encoding/json\" <nl> \"flag\" <nl> \"fmt\" <nl> \"log\" <nl> - \"net/http\" <nl> - \"strconv\" <nl> - \"sync/atomic\" <nl> \"time\" <nl> \"google.golang.org/grpc\" <nl> @@ -23,12 +19,11 @@ import ( <nl> var ( <nl> target = flag.String(\"target\", \"localhost:3457\", \"the host:port of the target rlp\") <nl> - httpAddr = flag.String(\"http-addr\", \"localhost:8081\", \"the host:port for HTTP to listen\") <nl> appID = flag.String(\"app-id\", \"\", \"app-id to stream data\") <nl> certFile = flag.String(\"cert\", \"\", \"cert to use to connect to rlp\") <nl> keyFile = flag.String(\"key\", \"\", \"key to use to connect to rlp\") <nl> caFile = flag.String(\"ca\", \"\", \"ca cert to use to connect to rlp\") <nl> - delay = flag.Duration(\"delay\", time.Second, \"delay inbetween reading messages\") <nl> + delay = flag.Duration(\"delay\", 0, \"delay inbetween reading messages\") <nl> ) <nl> func main() { <nl> @@ -50,8 +45,8 @@ func main() { <nl> log.Fatal(err) <nl> } <nl> client := v2.NewEgressClient(conn) <nl> - receiver, err := client.Receiver(context.TODO(), &v2.EgressRequest{ <nl> - ShardId: buildShardId(), <nl> + receiver, err := client.BatchedReceiver(context.TODO(), &v2.EgressBatchRequest{ <nl> + ShardId: buildShardID(), <nl> LegacySelector: &v2.Selector{ <nl> SourceId: *appID, <nl> Message: &v2.Selector_Log{ <nl> @@ -64,39 +59,20 @@ func main() { <nl> log.Fatal(err) <nl> } <nl> - reporter := &idxReporter{} <nl> - go func() { <nl> - var lastIdx int64 <nl> for { <nl> - env, err := receiver.Recv() <nl> + batch, err := receiver.Recv() <nl> if err != nil { <nl> - fmt.Printf(\"stopping reader, lastIdx: %d\\n\", lastIdx) <nl> + log.Printf(\"stopping reader, got err: %s\", err) <nl> return <nl> } <nl> - lastIdx, err = strconv.ParseInt(env.Tags[\"idx\"], 10, 0) <nl> - if err != nil { <nl> - log.Fatal(err) <nl> + for _, e := range batch.Batch { <nl> + fmt.Printf(\"%+v\\n\", e) <nl> } <nl> - reporter.set(lastIdx) <nl> time.Sleep(*delay) <nl> } <nl> - }() <nl> - log.Fatal(http.ListenAndServe(*httpAddr, reporter)) <nl> -} <nl> - <nl> -type idxReporter struct { <nl> - lastIdx int64 <nl> -} <nl> - <nl> -func (r *idxReporter) ServeHTTP(rw http.ResponseWriter, _ *http.Request) { <nl> - json.NewEncoder(rw).Encode(atomic.LoadInt64(&r.lastIdx)) <nl> -} <nl> - <nl> -func (r *idxReporter) set(v int64) { <nl> - atomic.StoreInt64(&r.lastIdx, v) <nl> } <nl> -func buildShardId() string { <nl> +func buildShardID() string { <nl> return \"rlp-reader-\" + randString() <nl> } <nl> ", "msg": "Simplify rlpreader tool"}
{"diff_id": 3588, "repo": "cloudfoundry/loggregator-release", "sha": "03f14001d6abcaa4695821e9e36a65a574c7dca6", "time": "20.02.2018 13:12:40", "diff": "mmm a / src/tools/reliability/server/internal/api/integration_test.go <nl> ppp b / src/tools/reliability/server/internal/api/integration_test.go <nl>@@ -43,7 +43,7 @@ func initiateTest(createTestHandler http.Handler) *httptest.ResponseRecorder { <nl> } <nl> func attachWorker(workerHandler http.Handler) func() { <nl> - d := wstest.NewDialer(workerHandler) <nl> + d := wstest.NewDialer(workerHandler, nil) <nl> c, _, err := d.Dial(\"ws://localhost:8080/ws\", nil) <nl> Expect(err).ToNot(HaveOccurred()) <nl> return func() { <nl> ", "msg": "Fix reliability server integration tests"}
{"diff_id": 3589, "repo": "cloudfoundry/loggregator-release", "sha": "6deb2b50d16708b5befdec52b59785f5af536d0b", "time": "21.02.2018 15:01:28", "diff": "mmm a / src/tools/rlpreader/main.go <nl> ppp b / src/tools/rlpreader/main.go <nl>@@ -19,8 +19,10 @@ import ( <nl> ) <nl> var ( <nl> - target = flag.String(\"target\", \"localhost:3457\", \"the host:port of the target rlp\") <nl> + target = flag.String(\"target\", \"localhost:8082\", \"the host:port of the target rlp\") <nl> appID = flag.String(\"app-id\", \"\", \"app-id to stream data\") <nl> + shardID = flag.String(\"shard-id\", \"\", \"sets the shard_id field\") <nl> + deterministicName = flag.String(\"deterministic-name\", \"\", \"sets the deterministic_name field\") <nl> certFile = flag.String(\"cert\", \"\", \"cert to use to connect to rlp\") <nl> keyFile = flag.String(\"key\", \"\", \"key to use to connect to rlp\") <nl> caFile = flag.String(\"ca\", \"\", \"ca cert to use to connect to rlp\") <nl> @@ -28,6 +30,8 @@ var ( <nl> preferredTags = flag.Bool(\"preferred-tags\", false, \"use preferred tags\") <nl> counterName = flag.String(\"counter\", \"\", \"select a counter with the given name\") <nl> gaugeNames = flag.String(\"gauge\", \"\", \"select a gauge with the given comma separated names (must contain all the names)\") <nl> + <nl> + metricNames = flag.Bool(\"metric-names\", false, \"this is really only useful while trying out deterministic routing. It only grabs counters and gauges and only prints their names\") <nl> ) <nl> func main() { <nl> @@ -82,8 +86,24 @@ func main() { <nl> }) <nl> } <nl> + if *metricNames { <nl> + selectors = []*loggregator_v2.Selector{ <nl> + { <nl> + Message: &loggregator_v2.Selector_Counter{ <nl> + Counter: &loggregator_v2.CounterSelector{}, <nl> + }, <nl> + }, <nl> + { <nl> + Message: &loggregator_v2.Selector_Gauge{ <nl> + Gauge: &loggregator_v2.GaugeSelector{}, <nl> + }, <nl> + }, <nl> + } <nl> + } <nl> + <nl> receiver, err := client.BatchedReceiver(context.TODO(), &loggregator_v2.EgressBatchRequest{ <nl> - ShardId: buildShardID(), <nl> + ShardId: buildShardID(*shardID), <nl> + DeterministicName: *deterministicName, <nl> UsePreferredTags: *preferredTags, <nl> Selectors: selectors, <nl> }) <nl> @@ -98,16 +118,35 @@ func main() { <nl> return <nl> } <nl> for _, e := range batch.Batch { <nl> + if *metricNames { <nl> + if e.GetCounter() != nil { <nl> + fmt.Printf(\"%s\\n\", e.GetCounter().GetName()) <nl> + continue <nl> + } <nl> + <nl> + var names []string <nl> + for name := range e.GetGauge().GetMetrics() { <nl> + names = append(names, name) <nl> + } <nl> + <nl> + fmt.Printf(\"%s\\n\", strings.Join(names, \", \")) <nl> + continue <nl> + } <nl> + <nl> fmt.Printf(\"%+v\\n\", e) <nl> } <nl> time.Sleep(*delay) <nl> } <nl> } <nl> -func buildShardID() string { <nl> +func buildShardID(shardID string) string { <nl> + if shardID == \"\" { <nl> return \"rlp-reader-\" + randString() <nl> } <nl> + return shardID <nl> +} <nl> + <nl> func randString() string { <nl> b := make([]byte, 20) <nl> _, err := rand.Read(b) <nl> ", "msg": "Updates rlpreader to demonstrate deterministic routing"}
{"diff_id": 3594, "repo": "cloudfoundry/loggregator-release", "sha": "afb16e34a5e162562f05ac6858e439e8a13f20f4", "time": "08.01.2020 15:38:44", "diff": "mmm a / src/integration_tests/endtoend/endtoend_test.go <nl> ppp b / src/integration_tests/endtoend/endtoend_test.go <nl>@@ -25,10 +25,9 @@ var _ = Describe(\"End to end tests\", func() { <nl> defer ingressCleanup() <nl> trafficcontrollerCleanup, tcPorts := testservers.StartTrafficController( <nl> - testservers.BuildTrafficControllerConf( <nl> + testservers.BuildTrafficControllerConfWithoutLogCache( <nl> fmt.Sprintf(\"127.0.0.1:%d\", dopplerPorts.GRPC), <nl> 0, <nl> - fmt.Sprintf(\"127.0.0.1:%d\", 0), <nl> ), <nl> ) <nl> defer trafficcontrollerCleanup() <nl> ", "msg": "update test to not fail because blocking on connection to log cache"}
{"diff_id": 3595, "repo": "cloudfoundry/loggregator-release", "sha": "7e772ac62ce888e029c2a5f9806b4b222e22548e", "time": "26.03.2020 14:38:57", "diff": "mmm a / src/trafficcontroller/app/traffic_controller.go <nl> ppp b / src/trafficcontroller/app/traffic_controller.go <nl>package app <nl> import ( <nl> - \"code.cloudfoundry.org/tlsconfig\" <nl> \"crypto/tls\" <nl> \"fmt\" <nl> \"log\" <nl> @@ -11,6 +10,8 @@ import ( <nl> \"os/signal\" <nl> \"time\" <nl> + \"code.cloudfoundry.org/tlsconfig\" <nl> + <nl> logcache \"code.cloudfoundry.org/log-cache/pkg/client\" <nl> \"code.cloudfoundry.org/loggregator/metricemitter\" <nl> \"code.cloudfoundry.org/loggregator/plumbing\" <nl> @@ -89,7 +90,13 @@ func (t *TrafficController) Start() { <nl> Timeout: 20 * time.Second, <nl> PermitWithoutStream: true, <nl> } <nl> - pool := plumbing.NewPool(20, grpc.WithTransportCredentials(creds), grpc.WithKeepaliveParams(kp)) <nl> + <nl> + pool := plumbing.NewPool( <nl> + 20, <nl> + grpc.WithTransportCredentials(creds), <nl> + grpc.WithKeepaliveParams(kp), <nl> + grpc.WithDisableServiceConfig(), <nl> + ) <nl> grpcConnector := plumbing.NewGRPCConnector(1000, pool, f, t.metricClient) <nl> var logCacheClient proxy.LogCacheClient <nl> ", "msg": "remove service config from traffic controller grpc config to reduce dns requests"}
{"diff_id": 3596, "repo": "cloudfoundry/loggregator-release", "sha": "2bb7248e0276214b0246a6ebe81b34bc6a6b0842", "time": "27.04.2020 16:24:04", "diff": "mmm a / src/rlp/main.go <nl> ppp b / src/rlp/main.go <nl>@@ -103,7 +103,17 @@ func main() { <nl> app.WithMaxEgressStreams(conf.MaxEgressStreams), <nl> ) <nl> go rlp.Start() <nl> - defer rlp.Stop() <nl> + <nl> + defer func() { <nl> + go func() { <nl> + // Limit the shutdown to 30 seconds <nl> + <-time.Tick(30 * time.Second) <nl> + os.Exit(0) <nl> + }() <nl> + <nl> + rlp.Stop() <nl> + }() <nl> + <nl> go profiler.New(conf.PProfPort).Start() <nl> killSignal := make(chan os.Signal, 1) <nl> ", "msg": "Fix issue where rlp somethimes doesn't stop\nEnforce exit 30 seconds after calling stop for graceful shutdown"}
{"diff_id": 3599, "repo": "cloudfoundry/loggregator-release", "sha": "f29cb37f3980af9d03f404b92a430b145a155476", "time": "01.02.2022 11:10:10", "diff": "mmm a / src/router/internal/server/v2/egress_server_test.go <nl> ppp b / src/router/internal/server/v2/egress_server_test.go <nl>@@ -236,7 +236,7 @@ var _ = Describe(\"EgressServer\", func() { <nl> Eventually( <nl> egressDropped.GetDelta, <nl> - 10).Should(BeNumerically(\">\", 1)) <nl> + 20).Should(BeNumerically(\">\", 1)) <nl> }) <nl> }) <nl> }) <nl> ", "msg": "Increase timeout for egress server dropped metric test"}
{"diff_id": 3620, "repo": "semi-technologies/weaviate", "sha": "e6d440fd1dc707e42b90e8773a1e43a4e0d2ad5c", "time": "29.05.2017 10:31:45", "diff": "mmm a / restapi/configure_weaviate.go <nl> ppp b / restapi/configure_weaviate.go <nl>@@ -270,15 +270,18 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> // Generate DatabaseObject without JSON-object in it. <nl> dbObject := *dbconnector.NewDatabaseObject(UsersObject.Uuid, refTypeLocation) <nl> - // Set the body-id and generate JSON to save to the database <nl> - params.Body.ID = dbObject.Uuid <nl> + // Set the generate JSON to save to the database <nl> dbObject.MergeRequestBodyIntoObject(params.Body) <nl> // Save to DB, this needs to be a Go routine because we will return an accepted <nl> go databaseConnector.Add(dbObject) <nl> + // Create response Object from create object. <nl> + locationResponseObject := &models.LocationGetResponse{} <nl> + json.Unmarshal([]byte(dbObject.Object), locationResponseObject) <nl> + <nl> // Return SUCCESS (NOTE: this is ACCEPTED, so the databaseConnector.Add should have a go routine) <nl> - return locations.NewWeaviateLocationsInsertAccepted().WithPayload(params.Body) <nl> + return locations.NewWeaviateLocationsInsertAccepted().WithPayload(locationResponseObject) <nl> }) <nl> api.LocationsWeaviateLocationsListHandler = locations.WeaviateLocationsListHandlerFunc(func(params locations.WeaviateLocationsListParams, principal interface{}) middleware.Responder { <nl> @@ -288,18 +291,19 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> } <nl> // Get limit <nl> - limit := getLimit(params.MaxResults) <nl> + //limit := getLimit(params.maxResults) <nl> + limit := int(maxResultsOverride) <nl> // List all results <nl> locationDatabaseObjects, _ := databaseConnector.List(refTypeLocation, limit) <nl> // Convert to an response object <nl> locationsListResponse := &models.LocationsListResponse{} <nl> - locationsListResponse.Locations = make([]*models.Location, limit) <nl> + locationsListResponse.Locations = make([]*models.LocationGetResponse, limit) <nl> // Loop to fill response project <nl> for i, locationDatabaseObject := range locationDatabaseObjects { <nl> - locationObject := &models.Location{} <nl> + locationObject := &models.LocationGetResponse{} <nl> json.Unmarshal([]byte(locationDatabaseObject.Object), locationObject) <nl> locationsListResponse.Locations[i] = locationObject <nl> } <nl> @@ -366,12 +370,9 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> } <nl> // Create object to return <nl> - object := &models.Location{} <nl> + object := &models.LocationGetResponse{} <nl> json.Unmarshal([]byte(dbObject.Object), &object) <nl> - // Overwrite body ID with UUID // TODO??? <nl> - params.Body.ID = UUID <nl> - <nl> // Set the body-id and generate JSON to save to the database <nl> dbObject.MergeRequestBodyIntoObject(params.Body) <nl> dbObject.SetCreateTimeMsToNow() <nl> @@ -380,7 +381,7 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> go databaseConnector.Add(dbObject) <nl> // Return SUCCESS (NOTE: this is ACCEPTED, so the databaseConnector.Add should have a go routine) <nl> - return locations.NewWeaviateLocationsUpdateOK().WithPayload(params.Body) <nl> + return locations.NewWeaviateLocationsUpdateOK().WithPayload(object) <nl> }) <nl> api.ThingTemplatesWeaviateThingTemplatesCreateHandler = thing_templates.WeaviateThingTemplatesCreateHandlerFunc(func(params thing_templates.WeaviateThingTemplatesCreateParams, principal interface{}) middleware.Responder { <nl> ", "msg": "Fix functions to be compilable after new Swagger release."}
{"diff_id": 3624, "repo": "semi-technologies/weaviate", "sha": "5fca92d50d32bb546827b7d0a9e3be9031fa1ef2", "time": "06.06.2017 11:46:39", "diff": "mmm a / restapi/configure_weaviate.go <nl> ppp b / restapi/configure_weaviate.go <nl>@@ -326,6 +326,7 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> locationObject := &models.LocationGetResponse{} <nl> json.Unmarshal([]byte(locationDatabaseObject.Object), locationObject) <nl> locationObject.ID = strfmt.UUID(locationDatabaseObject.Uuid) <nl> + locationObject.Kind = getKind(locationObject) <nl> responseObject.Locations[i] = locationObject <nl> } <nl> @@ -505,6 +506,7 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> thingTemplateObject := &models.ThingTemplateGetResponse{} <nl> json.Unmarshal([]byte(thingTemplatesDatabaseObject.Object), thingTemplateObject) <nl> thingTemplateObject.ID = strfmt.UUID(thingTemplatesDatabaseObject.Uuid) <nl> + thingTemplateObject.Kind = getKind(thingTemplateObject) <nl> responseObject.ThingTemplates[i] = thingTemplateObject <nl> } <nl> ", "msg": "Add kind to inner list response."}
{"diff_id": 3629, "repo": "semi-technologies/weaviate", "sha": "e71210807f0a96e241998b76b67a8f577b59c4f2", "time": "10.07.2017 17:16:56", "diff": "mmm a / test/full_test.go <nl> ppp b / test/full_test.go <nl>@@ -579,13 +579,6 @@ func Test__weaviate_key_delete_JSON(t *testing.T) { <nl> time.Sleep(2 * time.Second) <nl> } <nl> -// weaviate.key.me.delete <nl> -func Test__weaviate_key_me_delete_JSON(t *testing.T) { <nl> - // Delete keyID from database <nl> - responseKeyIDDeleted := doRequest(\"/keys/me\", \"DELETE\", \"application/json\", nil, newAPIToken) <nl> - testStatusCode(t, responseKeyIDDeleted.StatusCode, http.StatusNoContent) <nl> -} <nl> - <nl> /****************** <nl> * LOCATION TESTS <nl> ******************/ <nl> @@ -611,6 +604,12 @@ func Test__weaviate_location_create_JSON(t *testing.T) { <nl> // Check kind <nl> testKind(t, string(*respObject.Kind), \"weaviate#locationGetResponse\") <nl> + // KEY-CHECK: Create request for not allowed write action. Using key without write access. <nl> + jsonStrNotAllowed := bytes.NewBuffer([]byte(`{\"address_components\":[{\"long_name\":\"TEST\",\"short_name\":\"string\",\"types\":[\"UNDEFINED\"]}],\"formatted_address\":\"string\",\"geometry\":{\"location\":{},\"location_type\":\"string\",\"viewport\":{\"northeast\":{},\"southwest\":{}}},\"place_id\":\"string\",\"types\":[\"UNDEFINED\"]} `)) <nl> + responseNotAllowed := doRequest(\"/locations\", \"POST\", \"application/json\", jsonStrNotAllowed, newAPIToken) <nl> + <nl> + testStatusCode(t, responseNotAllowed.StatusCode, http.StatusForbidden) <nl> + <nl> // Test is faster than adding to DB. <nl> time.Sleep(1 * time.Second) <nl> } <nl> @@ -634,6 +633,12 @@ func Test__weaviate_location_get_JSON(t *testing.T) { <nl> // Check kind <nl> testKind(t, string(*respObject.Kind), \"weaviate#locationGetResponse\") <nl> + // // KEY-CHECK: Create get request not allowed: the location is not in the right Key-tree (key is no child of key set at location) <nl> + // responseNotAllowed := doRequest(\"/locations/\"+locationID, \"GET\", \"application/json\", nil, newSubAPIToken) <nl> + <nl> + // // Check status code get request <nl> + // testStatusCode(t, responseNotAllowed.StatusCode, http.StatusForbidden) <nl> + <nl> // Create get request with non-existing ID <nl> testNotExistsRequest(t, \"/locations\", \"GET\", \"application/json\", nil, apiKeyCmdLine) <nl> } <nl> @@ -1790,3 +1795,10 @@ func Test__weaviate_event_get_JSON(t *testing.T) { <nl> // Create get request with non-existing ID <nl> testNotExistsRequest(t, \"/events\", \"GET\", \"application/json\", nil, apiKeyCmdLine) <nl> } <nl> + <nl> +// weaviate.key.me.delete <nl> +func Test__weaviate_key_me_delete_JSON(t *testing.T) { <nl> + // Delete keyID from database <nl> + responseKeyIDDeleted := doRequest(\"/keys/me\", \"DELETE\", \"application/json\", nil, newAPIToken) <nl> + testStatusCode(t, responseKeyIDDeleted.StatusCode, http.StatusNoContent) <nl> +} <nl> ", "msg": "Add test functions for tree reading and editing."}
{"diff_id": 3673, "repo": "semi-technologies/weaviate", "sha": "af860b7ec28ccb9804b721b61ea65110fcc505f3", "time": "20.07.2018 13:18:05", "diff": "mmm a / network/p2p_network.go <nl> ppp b / network/p2p_network.go <nl>@@ -52,10 +52,10 @@ func BootstrapNetwork(m *messages.Messaging, genesis_url strfmt.URI) (Network, e <nl> // Bootstrap the network in the background. <nl> go n.bootstrap() <nl> - return n, nil <nl> + return &n, nil <nl> } <nl> -func (n network) bootstrap() { <nl> +func (n *network) bootstrap() { <nl> time.Sleep(10) //TODO: Use channel close to listen for when complete configuration is done. <nl> n.messaging.InfoMessage(\"Bootstrapping network\") <nl> @@ -74,21 +74,23 @@ func (n network) bootstrap() { <nl> n.peer_id = response.Payload.Peer.ID <nl> n.messaging.InfoMessage(fmt.Sprintf(\"Registered at Genesis server with id '%v'\", n.peer_id)) <nl> } <nl> + <nl> + go n.keep_pinging() <nl> } <nl> -func (n network) IsReady() bool { <nl> +func (n *network) IsReady() bool { <nl> return false <nl> } <nl> -func (n network) GetStatus() string { <nl> +func (n *network) GetStatus() string { <nl> return n.state <nl> } <nl> -func (n network) ListPeers() ([]Peer, error) { <nl> +func (n *network) ListPeers() ([]Peer, error) { <nl> return nil, fmt.Errorf(\"Cannot list peers, because there is no network configured\") <nl> } <nl> -func (n network) UpdatePeers(new_peers []Peer) error { <nl> +func (n *network) UpdatePeers(new_peers []Peer) error { <nl> n.Lock() <nl> defer n.Unlock() <nl> @@ -98,3 +100,19 @@ func (n network) UpdatePeers(new_peers []Peer) error { <nl> return nil <nl> } <nl> + <nl> +func (n *network) keep_pinging() { <nl> + for { <nl> + time.Sleep(30 * time.Second) <nl> + n.messaging.InfoMessage(\"Pinging Genesis server\") <nl> + <nl> + n.Lock() <nl> + params := client_ops.NewGenesisPeersPingParams() <nl> + params.PeerID = n.peer_id <nl> + n.Unlock() <nl> + _, err := n.client.Operations.GenesisPeersPing(params) <nl> + if err != nil { <nl> + n.messaging.InfoMessage(fmt.Sprintf(\"Could not ping Genesis server; %+v\", err)) <nl> + } <nl> + } <nl> +} <nl> ", "msg": "Ping Genesis server to prevent timeouts"}
{"diff_id": 3680, "repo": "semi-technologies/weaviate", "sha": "07b78d740b0d03fb43d1dcfc10b7b313d65e68af", "time": "03.08.2018 13:37:32", "diff": "mmm a / graphqlapi/build_schema.go <nl> ppp b / graphqlapi/build_schema.go <nl>package graphqlapi <nl> import ( <nl> + \"bytes\" <nl> \"fmt\" <nl> + //\"reflect\" <nl> + //\"strconv\" <nl> \"github.com/creativesoftwarefdn/weaviate/models\" <nl> \"github.com/graphql-go/graphql\" <nl> + \"strings\" <nl> ) <nl> // Build the GraphQL schema based on <nl> @@ -24,22 +28,37 @@ import ( <nl> // 2) the (dynamic) database schema from Weaviate <nl> func (g *GraphQL) buildGraphqlSchema() error { <nl> - local_field, err := g.buildLocalField() <nl> + <nl> + rootFieldsObject, err := g.assembleFullSchema() <nl> if err != nil { <nl> return fmt.Errorf(\"Could not build GraphQL schema, because: %v\", err) <nl> } <nl> - var root_fields = graphql.Fields{ <nl> - \"Local\": local_field, <nl> - // \"Network\" : etc <nl> + schemaObject := graphql.ObjectConfig{ <nl> + Name: \"WeaviateObj\", <nl> + Fields: rootFieldsObject, <nl> + Description: \"Location of the root query\", <nl> } <nl> - rootQuery := graphql.ObjectConfig{Name: \"WeaviateObj\", Fields: root_fields} <nl> + // Run grahql.NewSchema in a sub-closure, so that we can recover from panics. <nl> + // We need to use panics to return errors deep inside the dynamic generation of the GraphQL schema, <nl> + // inside the FieldThunks. There is _no_ way to bubble up an error besides panicking. <nl> + func() { <nl> + defer func() { <nl> + if r := recover(); r != nil { <nl> + var ok bool <nl> + err, ok = r.(error) // can't shadow err here; we need the err from outside the function closure. <nl> + if !ok { <nl> + err = fmt.Errorf(\"%v\", err) <nl> + } <nl> + } <nl> + }() <nl> g.weaviateGraphQLSchema, err = graphql.NewSchema(graphql.SchemaConfig{ <nl> - Query: graphql.NewObject(rootQuery), <nl> + Query: graphql.NewObject(schemaObject), <nl> }) <nl> + }() <nl> if err != nil { <nl> return fmt.Errorf(\"Could not build GraphQL schema, because: %v\", err) <nl> @@ -48,60 +67,458 @@ func (g *GraphQL) buildGraphqlSchema() error { <nl> } <nl> } <nl> -func (g *GraphQL) buildLocalField() (*graphql.Field, error) { <nl> - action_class_fields, err := g.buildExampleActionClassFields() <nl> +// check: regel class refs voor meerdere objecten als datatype (union) <nl> +// check: maak dit ook voor Things <nl> +// check: refactor naar objects returnen ipv object configs <nl> +// check: check all Things strings <nl> +// TODO: confirm output of dynamic schema generation; classes as properties in lists y/n? <nl> +// TODO: implement metafetch <nl> +// TODO: implement filters <nl> + <nl> +func (g *GraphQL) assembleFullSchema() (graphql.Fields, error) { <nl> + <nl> + // This map is used to store all the Thing and Action ObjectConfigs, so that we can use them in references. <nl> + convertedFetchActionsAndThings := make(map[string]*graphql.Object) <nl> + <nl> + localConvertedFetchActions, err := g.buildActionClassFieldsFromSchema(&convertedFetchActionsAndThings) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate action fields from schema for local convertedfetch because: %v\", err) <nl> + } <nl> + <nl> + localConvertedFetchThings, err := g.buildThingClassFieldsFromSchema(&convertedFetchActionsAndThings) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate action fields from schema for local convertedfetch because: %v\", err) <nl> + } <nl> + <nl> + localConvertedFetchObject, err := g.genThingsAndActionsFieldsForWeaviateLocalConvertedFetchObj(localConvertedFetchActions, localConvertedFetchThings) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate things and action fields for local convertedfetch because: %v\", err) <nl> + } <nl> + <nl> + localMetaFetchObject, err := g.genThingsAndActionsFieldsForWeaviateLocalMetaFetchGenericsObj() <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate things and action fields for local metafetch because: %v\", err) <nl> + } <nl> + <nl> + localMetaGenericsObject, err := g.genGenericsFieldForWeaviateLocalMetaFetchObj(localMetaFetchObject) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate generics field for local metafetch because: %v\", err) <nl> + } <nl> + <nl> + localMetaAndConvertedFetchObject, err := g.genConvertedFetchAndMetaGenericsFields(localConvertedFetchObject, localMetaGenericsObject) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate meta and convertedfetch fields for local weaviateobject because: %v\", err) <nl> + } <nl> + <nl> + localObject, err := g.buildLocalField(localMetaAndConvertedFetchObject) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate local field for local weaviateobject because: %v\", err) <nl> + } <nl> + <nl> + rootFieldsObject, err := g.genRootQueryFields(localObject) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"Failed to generate root query because: %v\", err) <nl> + } <nl> + <nl> + return rootFieldsObject, nil <nl> +} <nl> + <nl> +// Build the dynamically generated Actions part of the schema <nl> +func (g *GraphQL) buildActionClassFieldsFromSchema(convertedFetchActionsAndThings *map[string]*graphql.Object) (*graphql.Object, error) { <nl> + <nl> + actionClassFields := graphql.Fields{} <nl> + <nl> + for _, class := range g.databaseSchema.ActionSchema.Schema.Classes { <nl> + field, obj, err := buildSingleActionClassField(class, convertedFetchActionsAndThings) <nl> if err != nil { <nl> return nil, err <nl> } <nl> + actionClassFields[class.Class] = field <nl> + (*convertedFetchActionsAndThings)[class.Class] = obj <nl> + } <nl> - local_fields := graphql.Fields{ <nl> - \"ConvertedFetch\": &graphql.Field{ <nl> - Type: graphql.String, <nl> + localConvertedFetchActions := graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalConvertedFetchActionsObj\", <nl> + Fields: actionClassFields, <nl> + Description: \"Fetch Actions on the internal Weaviate\", <nl> + } <nl> + return graphql.NewObject(localConvertedFetchActions), nil <nl> +} <nl> + <nl> +func buildSingleActionClassField(class *models.SemanticSchemaClass, convertedFetchActionsAndThings *map[string]*graphql.Object) (*graphql.Field, *graphql.Object, error) { <nl> + singleActionClassPropertyFieldsObj := graphql.ObjectConfig{ <nl> + Name: class.Class, <nl> + Fields: (graphql.FieldsThunk)(func() graphql.Fields { <nl> + singleActionClassPropertyFields, err := buildSingleActionClassPropertyFields(class, convertedFetchActionsAndThings) <nl> + if err != nil { <nl> + panic(\"oops\") <nl> + } <nl> + return singleActionClassPropertyFields <nl> + }), <nl> + Description: \"Type of fetch on the internal Weaviate\", <nl> + } <nl> + <nl> + obj := graphql.NewObject(singleActionClassPropertyFieldsObj) <nl> + field := &graphql.Field{ <nl> + Type: obj, <nl> + Description: class.Description, <nl> Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> return nil, fmt.Errorf(\"Not supported\") <nl> }, <nl> + } <nl> + return field, obj, nil <nl> +} <nl> + <nl> +func buildSingleActionClassPropertyFields(class *models.SemanticSchemaClass, convertedFetchActionsAndThings *map[string]*graphql.Object) (graphql.Fields, error) { <nl> + <nl> + singleActionClassPropertyFields := graphql.Fields{} <nl> + <nl> + for index, property := range class.Properties { <nl> + <nl> + if propertyDataTypeIsClass(property) { <nl> + numberOfDataTypes := len(property.AtDataType) <nl> + <nl> + dataTypeClasses := make([]*graphql.Object, numberOfDataTypes) <nl> + <nl> + for index, dataType := range property.AtDataType { <nl> + <nl> + thingOrActionType, ok := (*convertedFetchActionsAndThings)[dataType] <nl> + if !ok { <nl> + panic(fmt.Errorf(\"No such thing/action class '%s'\", property.AtDataType[index])) <nl> + } <nl> + <nl> + dataTypeClasses[index] = thingOrActionType <nl> + } <nl> + dataTypeUnionConf := graphql.UnionConfig{ <nl> + Name: genClassPropertyClassName(class, property), <nl> + Types: dataTypeClasses, <nl> + ResolveType: func(p graphql.ResolveTypeParams) *graphql.Object { <nl> + return nil <nl> }, <nl> - \"ActionClasses\": &graphql.Field{ <nl> - Type: graphql.NewObject(graphql.ObjectConfig{Name: \"SampleActionClass\", Fields: action_class_fields}), <nl> + Description: property.Description, <nl> + } <nl> + multipleClassDataTypesUnion := graphql.NewUnion(dataTypeUnionConf) <nl> + <nl> + singleActionClassPropertyFields[property.Name] = &graphql.Field{ <nl> + Type: multipleClassDataTypesUnion, <nl> + Description: property.Description, <nl> Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> return nil, fmt.Errorf(\"Not supported\") <nl> }, <nl> - }, <nl> } <nl> + } else { <nl> + convertedDataType, err := handleNonObjectDataTypes(property.AtDataType[0], property) <nl> - local_object := graphql.ObjectConfig{Name: \"WeaviateLocal\", Fields: local_fields} <nl> - field := graphql.Field{ <nl> - Type: graphql.NewObject(local_object), <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + singleActionClassPropertyFields[property.Name] = convertedDataType <nl> + } <nl> + } <nl> + return singleActionClassPropertyFields, nil <nl> +} <nl> + <nl> +func genClassPropertyClassName(class *models.SemanticSchemaClass, property *models.SemanticSchemaClassProperty) string { <nl> + <nl> + var buffer bytes.Buffer <nl> + <nl> + buffer.WriteString(class.Class) <nl> + buffer.WriteString(property.Name) <nl> + buffer.WriteString(\"Obj\") <nl> + <nl> + return buffer.String() <nl> +} <nl> + <nl> +// Build the dynamically generated Things part of the schema <nl> +func (g *GraphQL) buildThingClassFieldsFromSchema(convertedFetchActionsAndThings *map[string]*graphql.Object) (*graphql.Object, error) { <nl> + <nl> + thingClassFields := graphql.Fields{} <nl> + <nl> + for _, class := range g.databaseSchema.ThingSchema.Schema.Classes { <nl> + field, obj, err := buildSingleThingClassField(class, convertedFetchActionsAndThings) <nl> + <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + thingClassFields[class.Class] = field <nl> + (*convertedFetchActionsAndThings)[class.Class] = obj <nl> + } <nl> + localConvertedFetchThings := graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalConvertedFetchThingsObj\", <nl> + Fields: thingClassFields, <nl> + Description: \"Fetch Things on the internal Weaviate\", <nl> + } <nl> + return graphql.NewObject(localConvertedFetchThings), nil <nl> +} <nl> + <nl> +func buildSingleThingClassField(class *models.SemanticSchemaClass, convertedFetchActionsAndThings *map[string]*graphql.Object) (*graphql.Field, *graphql.Object, error) { <nl> + <nl> + singleThingClassPropertyFieldsObj := graphql.ObjectConfig{ <nl> + Name: class.Class, <nl> + Fields: (graphql.FieldsThunk)(func() graphql.Fields { <nl> + singleThingClassPropertyFields, err := buildSingleThingClassPropertyFields(class, convertedFetchActionsAndThings) <nl> + if err != nil { <nl> + panic(\"oops\") <nl> + } <nl> + return singleThingClassPropertyFields <nl> + }), <nl> + Description: \"Type of fetch on the internal Weaviate\", <nl> + } <nl> + <nl> + obj := graphql.NewObject(singleThingClassPropertyFieldsObj) <nl> + field := &graphql.Field{ <nl> + Type: obj, <nl> + Description: class.Description, <nl> Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> return nil, fmt.Errorf(\"Not supported\") <nl> }, <nl> } <nl> + return field, obj, nil <nl> +} <nl> - return &field, nil <nl> +func buildSingleThingClassPropertyFields(class *models.SemanticSchemaClass, convertedFetchActionsAndThings *map[string]*graphql.Object) (graphql.Fields, error) { <nl> + <nl> + singleThingClassPropertyFields := graphql.Fields{} <nl> + <nl> + for index, property := range class.Properties { <nl> + <nl> + if propertyDataTypeIsClass(property) { <nl> + numberOfDataTypes := len(property.AtDataType) <nl> + <nl> + dataTypeClasses := make([]*graphql.Object, numberOfDataTypes) <nl> + <nl> + for index, dataType := range property.AtDataType { <nl> + <nl> + thingOrActionType, ok := (*convertedFetchActionsAndThings)[dataType] <nl> + if !ok { <nl> + panic(fmt.Errorf(\"No such thing/action class '%s'\", property.AtDataType[index])) <nl> } <nl> -// EXAMPLE: How to iterate through the <nl> -func (g *GraphQL) buildExampleActionClassFields() (graphql.Fields, error) { <nl> - fields := graphql.Fields{} <nl> + dataTypeClasses[index] = thingOrActionType <nl> + } <nl> + <nl> + dataTypeUnionConf := graphql.UnionConfig{ <nl> + Name: genClassPropertyClassName(class, property), <nl> + Types: dataTypeClasses, <nl> + ResolveType: func(p graphql.ResolveTypeParams) *graphql.Object { <nl> + return nil <nl> + }, <nl> + Description: property.Description, <nl> + } <nl> + <nl> + multipleClassDataTypesUnion := graphql.NewUnion(dataTypeUnionConf) <nl> + <nl> + singleThingClassPropertyFields[property.Name] = &graphql.Field{ <nl> + Type: multipleClassDataTypesUnion, <nl> + Description: property.Description, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + } <nl> + } else { <nl> + convertedDataType, err := handleNonObjectDataTypes(property.AtDataType[0], property) <nl> - for _, class := range g.databaseSchema.ActionSchema.Schema.Classes { <nl> - field, err := buildExampleActionClassField(class) <nl> if err != nil { <nl> return nil, err <nl> - } else { <nl> - fields[class.Class] = field <nl> + } <nl> + singleThingClassPropertyFields[property.Name] = convertedDataType <nl> } <nl> } <nl> + return singleThingClassPropertyFields, nil <nl> +} <nl> + <nl> +func propertyDataTypeIsClass(property *models.SemanticSchemaClassProperty) bool { <nl> - return fields, nil <nl> + firstChar := string([]rune(property.AtDataType[0])[0]) // get first char from first element using utf-8 <nl> + <nl> + if firstChar == strings.ToUpper(firstChar) { <nl> + return true <nl> + } <nl> + return false <nl> } <nl> -func buildExampleActionClassField(class *models.SemanticSchemaClass) (*graphql.Field, error) { <nl> +func handleNonObjectDataTypes(dataType string, property *models.SemanticSchemaClassProperty) (*graphql.Field, error) { <nl> + <nl> + switch dataType { <nl> + <nl> + case \"string\": <nl> return &graphql.Field{ <nl> + Description: property.Description, <nl> Type: graphql.String, <nl> Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> return nil, fmt.Errorf(\"Not supported\") <nl> }, <nl> }, nil <nl> + <nl> + case \"int\": <nl> + return &graphql.Field{ <nl> + Description: property.Description, <nl> + Type: graphql.Int, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, nil <nl> + <nl> + case \"number\": <nl> + return &graphql.Field{ <nl> + Description: property.Description, <nl> + Type: graphql.Float, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, nil <nl> + <nl> + case \"boolean\": <nl> + return &graphql.Field{ <nl> + Description: property.Description, <nl> + Type: graphql.Boolean, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, nil <nl> + <nl> + case \"date\": <nl> + return &graphql.Field{ <nl> + Description: property.Description, <nl> + Type: graphql.String, // String since no graphql date datatype exists <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, nil <nl> + <nl> + default: <nl> + return &graphql.Field{ <nl> + Description: property.Description, <nl> + Type: graphql.String, <nl> + }, fmt.Errorf(\"I DON'T KNOW THIS VALUE!\") <nl> + } <nl> +} <nl> + <nl> +func (g *GraphQL) genThingsAndActionsFieldsForWeaviateLocalConvertedFetchObj(localConvertedFetchActions *graphql.Object, <nl> + localConvertedFetchThings *graphql.Object) (*graphql.Object, error) { <nl> + <nl> + convertedFetchThingsAndActionFields := graphql.Fields{ <nl> + \"Actions\": &graphql.Field{ <nl> + Name: \"WeaviateLocalConvertedFetchActions\", <nl> + Description: \"Locate Actions on the local Weaviate\", <nl> + Type: localConvertedFetchActions, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + \"Things\": &graphql.Field{ <nl> + Name: \"WeaviateLocalConvertedFetchThings\", <nl> + Description: \"Locate Things on the local Weaviate\", <nl> + Type: localConvertedFetchThings, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + } <nl> + convertedFetchThingsAndActionFieldsObject := graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalConvertedFetchObj\", <nl> + Fields: convertedFetchThingsAndActionFields, <nl> + Description: \"Fetch things or actions on the internal Weaviate\", <nl> + } <nl> + return graphql.NewObject(convertedFetchThingsAndActionFieldsObject), nil <nl> +} <nl> + <nl> +func (g *GraphQL) genThingsAndActionsFieldsForWeaviateLocalMetaFetchGenericsObj() (*graphql.Object, error) { <nl> + <nl> + metaFetchGenericsThingsAndActionFields := graphql.Fields{ <nl> + \"Actions\": &graphql.Field{ <nl> + Name: \"WeaviateLocalMetaFetchGenericsActions\", <nl> + Description: \"Action to fetch for meta generic fetch\", <nl> + Type: graphql.String, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + \"Things\": &graphql.Field{ <nl> + Name: \"WeaviateLocalMetaFetchGenericsThings\", <nl> + Description: \"Thing to fetch for meta generic fetch\", <nl> + Type: graphql.String, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + } <nl> + metaFetchGenericsThingsAndActionFieldsObject := graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalMetaFetchGenericsObj\", <nl> + Fields: metaFetchGenericsThingsAndActionFields, <nl> + Description: \"Object type to fetch\", <nl> + } <nl> + return graphql.NewObject(metaFetchGenericsThingsAndActionFieldsObject), nil <nl> +} <nl> + <nl> +func (g *GraphQL) genGenericsFieldForWeaviateLocalMetaFetchObj(localMetaFetchObject *graphql.Object) (*graphql.Object, error) { <nl> + <nl> + metaFetchGenericsField := graphql.Fields{ <nl> + \"Generics\": &graphql.Field{ <nl> + Name: \"WeaviateLocalMetaFetchGenericsObj\", <nl> + Description: \"Fetch generic meta information based on the type\", <nl> + Type: localMetaFetchObject, <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + } <nl> + metaFetchGenericsFieldObject := graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalMetaFetchObj\", <nl> + Fields: metaFetchGenericsField, <nl> + Description: \"Fetch things or actions on the internal Weaviate\", <nl> + } <nl> + return graphql.NewObject(metaFetchGenericsFieldObject), nil <nl> +} <nl> + <nl> +func (g *GraphQL) genConvertedFetchAndMetaGenericsFields( <nl> + localConvertedFetchObject *graphql.Object, <nl> + localMetaGenericsObject *graphql.Object) (*graphql.Object, error) { <nl> + <nl> + convertedAndMetaFetchFields := graphql.Fields{ <nl> + \"ConvertedFetch\": &graphql.Field{ <nl> + Name: \"WeaviateLocalConvertedFetch\", <nl> + Type: localConvertedFetchObject, <nl> + Description: \"Do a converted fetch to search Things or Actions on the local weaviate\", <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + \"MetaFetch\": &graphql.Field{ <nl> + Name: \"WeaviateLocalMetaFetch\", <nl> + Type: localMetaGenericsObject, <nl> + Description: \"Fetch meta information about Things or Actions on the local weaviate\", <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + }, <nl> + } <nl> + weaviateLocalObject := &graphql.ObjectConfig{ <nl> + Name: \"WeaviateLocalObj\", <nl> + Fields: convertedAndMetaFetchFields, <nl> + Description: \"Type of fetch on the internal Weaviate\", <nl> + } <nl> + return graphql.NewObject(*weaviateLocalObject), nil <nl> +} <nl> + <nl> +func (g *GraphQL) buildLocalField(localMetaAndConvertedFetchObject *graphql.Object) (*graphql.Field, error) { <nl> + <nl> + field := graphql.Field{ <nl> + Type: localMetaAndConvertedFetchObject, <nl> + Description: \"Locate on the local Weaviate\", <nl> + Resolve: func(p graphql.ResolveParams) (interface{}, error) { <nl> + return nil, fmt.Errorf(\"Not supported\") <nl> + }, <nl> + } <nl> + return &field, nil <nl> +} <nl> + <nl> +func (g *GraphQL) genRootQueryFields(localField *graphql.Field) (graphql.Fields, error) { <nl> + <nl> + var rootQueryFields = graphql.Fields{ <nl> + \"Local\": localField, <nl> + \"Network\": nil, <nl> + } <nl> + return rootQueryFields, nil <nl> } <nl> ", "msg": "implemented dynamic schema generation for things and actions, including unions for things/actions with other things/actions as properties"}
{"diff_id": 3687, "repo": "semi-technologies/weaviate", "sha": "b9871e81b84ec115b0c18f03457f93918494a6bc", "time": "14.08.2018 15:40:48", "diff": "mmm a / test/acceptance/graphql_schema_test.go <nl> ppp b / test/acceptance/graphql_schema_test.go <nl>@@ -264,6 +264,27 @@ func (g *GraphQLResult) AssertKey(t *testing.T, key string) *GraphQLResult { <nl> return &GraphQLResult{Result: x} <nl> } <nl> +// TODO Use this function instead of AssertKey(t, \"x\").AssetKey(t, \"y\"): <nl> +// AssertKeys(t, []string{\"x\",\"y\",}) <nl> +func (g* GraphQLResult) AsserKeyPath(t *testing.T, keys []string) *GraphQLResult { <nl> + // currently found result. <nl> + r := g.Result <nl> + <nl> + for _, key := range keys { <nl> + m, ok := r.(map[string]interface{}) <nl> + if !ok { <nl> + t.Fatalf(\"Can't index into key %s, because this is not a map\", key) <nl> + } <nl> + <nl> + r, ok = m[key] <nl> + if !ok { <nl> + t.Fatalf(\"Can't index into key %s, because no such key exists\", key) <nl> + } <nl> + } <nl> + <nl> + return &GraphQLResult{Result: r} <nl> +} <nl> + <nl> // Assert that this is a slice. <nl> // Wraps a GraphQLResult over all children too. <nl> func (g *GraphQLResult) AssertSlice(t *testing.T) []*GraphQLResult { <nl> ", "msg": "Add helper function to assert a path of keys."}
{"diff_id": 3695, "repo": "semi-technologies/weaviate", "sha": "baf300f7c7f6a79373305d38467a6726a053a4c4", "time": "10.09.2018 16:08:17", "diff": "mmm a / restapi/configure_weaviate.go <nl> ppp b / restapi/configure_weaviate.go <nl>@@ -513,16 +513,17 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> action.LastUpdateTimeUnix = 0 <nl> action.Key = keyRef <nl> - // Save to DB, this needs to be a Go routine because we will return an accepted <nl> - go dbConnector.AddAction(ctx, action, UUID) <nl> - <nl> - // Initialize a response object <nl> responseObject := &models.ActionGetResponse{} <nl> responseObject.Action = *action <nl> responseObject.ActionID = UUID <nl> - // Return SUCCESS (NOTE: this is ACCEPTED, so the databaseConnector.Add should have a go routine) <nl> + if params.Body.Async { <nl> + go dbConnector.AddAction(ctx, action, UUID) <nl> return actions.NewWeaviateActionsCreateAccepted().WithPayload(responseObject) <nl> + } else { <nl> + dbConnector.AddAction(ctx, action, UUID) <nl> + return actions.NewWeaviateActionsCreateOK().WithPayload(responseObject) <nl> + } <nl> }) <nl> api.ActionsWeaviateActionsDeleteHandler = actions.WeaviateActionsDeleteHandlerFunc(func(params actions.WeaviateActionsDeleteParams, principal interface{}) middleware.Responder { <nl> // Initialize response <nl> @@ -811,16 +812,17 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> thing.LastUpdateTimeUnix = 0 <nl> thing.Key = keyRef <nl> - // Save to DB, this needs to be a Go routine because we will return an accepted <nl> - go dbConnector.AddThing(ctx, thing, UUID) <nl> - <nl> - // Create response Object from create object. <nl> responseObject := &models.ThingGetResponse{} <nl> responseObject.Thing = *thing <nl> responseObject.ThingID = UUID <nl> - // Return SUCCESS (NOTE: this is ACCEPTED, so the dbConnector.Add should have a go routine) <nl> + if params.Body.Async { <nl> + go dbConnector.AddThing(ctx, thing, UUID) <nl> return things.NewWeaviateThingsCreateAccepted().WithPayload(responseObject) <nl> + } else { <nl> + dbConnector.AddThing(ctx, thing, UUID) <nl> + return things.NewWeaviateThingsCreateOK().WithPayload(responseObject) <nl> + } <nl> }) <nl> api.ThingsWeaviateThingsDeleteHandler = things.WeaviateThingsDeleteHandlerFunc(func(params things.WeaviateThingsDeleteParams, principal interface{}) middleware.Responder { <nl> // Initialize response <nl> ", "msg": "Add code to respect the async flag in creation of Things/Actions"}
{"diff_id": 3704, "repo": "semi-technologies/weaviate", "sha": "344b751257bb6bec644cc2b2d7a0c24b6ead889d", "time": "08.10.2018 17:40:38", "diff": "mmm a / database/schema_manager/local/local.go <nl> ppp b / database/schema_manager/local/local.go <nl>@@ -27,16 +27,16 @@ type LocalSchemaManager struct { <nl> // The state that will be serialized to/from disk. <nl> // TODO: refactor to database/schema_manager, so that it can be re-used for distributed version. <nl> type localSchemaState struct { <nl> - actionSchema *models.SemanticSchema `json:\"action\"` <nl> - thingSchema *models.SemanticSchema `json:\"thing\"` <nl> + ActionSchema *models.SemanticSchema `json:\"action\"` <nl> + ThingSchema *models.SemanticSchema `json:\"thing\"` <nl> } <nl> func (l *localSchemaState) SchemaFor(k kind.Kind) *models.SemanticSchema { <nl> switch k { <nl> case kind.THING_KIND: <nl> - return l.thingSchema <nl> + return l.ThingSchema <nl> case kind.ACTION_KIND: <nl> - return l.actionSchema <nl> + return l.ActionSchema <nl> default: <nl> log.Fatal(\"Passed wrong neither thing nor kind, but %v\", k) <nl> return nil <nl> @@ -96,11 +96,11 @@ func (l *LocalSchemaManager) load() error { <nl> // Fill in empty ontologies if there is no schema file. <nl> if !stateFileExists { <nl> - l.schemaState.actionSchema = &models.SemanticSchema{ <nl> + l.schemaState.ActionSchema = &models.SemanticSchema{ <nl> Type: \"action\", <nl> } <nl> - l.schemaState.thingSchema = &models.SemanticSchema{ <nl> + l.schemaState.ThingSchema = &models.SemanticSchema{ <nl> Type: \"thing\", <nl> } <nl> @@ -130,6 +130,7 @@ func (l *LocalSchemaManager) statePath() string { <nl> // Save the schema to the local disk. <nl> func (l *LocalSchemaManager) saveToDisk() error { <nl> + log.Info(\"Updating local schema on disk\") <nl> // TODO not 100% robust against failures. <nl> // we don't check IO errors yet <nl> @@ -149,6 +150,7 @@ func (l *LocalSchemaManager) saveToDisk() error { <nl> } <nl> l.stateFile.Write(stateBytes) <nl> + l.stateFile.Sync() <nl> return nil <nl> } <nl> ", "msg": "make field members public, so that json (de)seralization works"}
{"diff_id": 3724, "repo": "semi-technologies/weaviate", "sha": "55a13b66975daeae74768e468cd2ae00b437391f", "time": "13.11.2018 17:08:55", "diff": "mmm a / database/connectors/janusgraph/resolver.go <nl> ppp b / database/connectors/janusgraph/resolver.go <nl>@@ -26,7 +26,14 @@ func (j *Janusgraph) LocalGetClass(params *graphql_local_get.LocalGetClassParams <nl> ch := make(chan resolveResult, 1) <nl> go func() { <nl> - defer close(ch) <nl> + defer func() { <nl> + if r := recover(); r != nil { <nl> + // send error over the channel <nl> + ch <- resolveResult{err: fmt.Errorf(\"Janusgraph.LocalGetClass paniced: %#v\", r)} <nl> + } <nl> + close(ch) <nl> + }() <nl> + <nl> results := []interface{}{} <nl> className := schema.AssertValidClassName(params.ClassName) <nl> ", "msg": "Capture panics so that Weaviate won't crash"}
{"diff_id": 3731, "repo": "semi-technologies/weaviate", "sha": "9d63d00633a77528a8f3aea72137e9ac1dd7ad11", "time": "26.11.2018 14:32:54", "diff": "mmm a / database/connectors/janusgraph/resolver.go <nl> ppp b / database/connectors/janusgraph/resolver.go <nl>@@ -51,18 +51,25 @@ func (j *Janusgraph) LocalGetClass(params *graphql_local_get.LocalGetClassParams <nl> ) <nl> err := j.getClass(params.Kind, uuid, &atClass, &atContext, &foundUUID, &creationTimeUnix, &lastUpdateTimeUnix, &properties, &key) <nl> + <nl> if err == nil { <nl> + propertiesMap := properties.(map[string]interface{}) <nl> + <nl> result := map[string]interface{}{} <nl> for _, selectProperty := range params.Properties { <nl> - // Primitive properties are trivial; just copy them. <nl> - if selectProperty.IsPrimitive { <nl> if selectProperty.Name == \"uuid\" { <nl> result[\"uuid\"] = interface{}(foundUUID) <nl> - } else { <nl> - propertiesMap := properties.(map[string]interface{}) <nl> - result[selectProperty.Name] = propertiesMap[selectProperty.Name] <nl> + continue <nl> + } <nl> + <nl> + // Primitive properties are trivial; just copy them. <nl> + if selectProperty.IsPrimitive { <nl> + _, isPresent := propertiesMap[selectProperty.Name] <nl> + if !isPresent { <nl> + continue <nl> } <nl> + result[selectProperty.Name] = propertiesMap[selectProperty.Name] <nl> } else { <nl> // For relations we need to do a bit more work. <nl> propertyName := schema.AssertValidPropertyName(strings.ToLower(selectProperty.Name[0:1]) + selectProperty.Name[1:len(selectProperty.Name)]) <nl> ", "msg": "Move presence check into the case of primitive types.\nRef types start with a capital in GraphQL, and will therefore not be\nfound."}
{"diff_id": 3733, "repo": "semi-technologies/weaviate", "sha": "82dd9fa1aad0f2f6d8f16b648ba0c1cc5921f972", "time": "27.11.2018 11:49:40", "diff": "mmm a / database/schema_manager/local/validation.go <nl> ppp b / database/schema_manager/local/validation.go <nl>@@ -39,7 +39,7 @@ func (l *localSchemaManager) validateCanAddClass(knd kind.Kind, class *models.Se <nl> // Validate data type of property. <nl> schema := l.GetSchema() <nl> - err, _ := (&schema).FindPropertyDataType(property.AtDataType) <nl> + _, err := (&schema).FindPropertyDataType(property.AtDataType) <nl> if err != nil { <nl> return fmt.Errorf(\"Data type fo property '%s' is invalid; %v\", property.Name, err) <nl> } <nl> ", "msg": "Update call to FindPropertyDataType to reflect change in ret types.\nMissed this one last location of call to FindPropertyDataType"}
{"diff_id": 3749, "repo": "semi-technologies/weaviate", "sha": "6e315f51ca30feed809ce980c4ad6ab96228b529", "time": "22.12.2018 00:24:21", "diff": "mmm a / database/connectors/janusgraph/helper_update_class.go <nl> ppp b / database/connectors/janusgraph/helper_update_class.go <nl>@@ -225,6 +225,9 @@ func addPrimitivePropToQuery(q *gremlin.Query, propType schema.PropertyDataType, <nl> default: <nl> return q, fmt.Errorf(\"Illegal value for property %s\", sanitizedPropertyName) <nl> } <nl> - } <nl> + default: <nl> panic(fmt.Sprintf(\"Unkown primitive datatype %s\", propType.AsPrimitive())) <nl> } <nl> + <nl> + return q, nil <nl> +} <nl> ", "msg": "(refactor) fix incorrect error handling on updateClass\nThe previous refactor introduced a small issue where after successful\ntype assertion, we would still panic because we didn't return before."}
{"diff_id": 3762, "repo": "semi-technologies/weaviate", "sha": "0de84a8760aa3a80b314884def89283ccbee7ed4", "time": "23.01.2019 15:17:05", "diff": "mmm a / graphqlapi/local/aggregate/resolver_test.go <nl> ppp b / graphqlapi/local/aggregate/resolver_test.go <nl>@@ -28,6 +28,7 @@ type testCase struct { <nl> resolverReturn interface{} <nl> expectedResults []result <nl> expectedGroupBy *common_filters.Path <nl> + expectedWhereFilter *common_filters.LocalFilter <nl> } <nl> type testCases []testCase <nl> @@ -73,6 +74,57 @@ func Test_Resolve(t *testing.T) { <nl> }}, <nl> }, <nl> + testCase{ <nl> + name: \"single prop: mean with a where filter\", <nl> + query: `{ <nl> + Aggregate { <nl> + Things { <nl> + Car( <nl> + groupBy:[\"madeBy\", \"Manufacturer\", \"name\"] <nl> + where: { <nl> + operator: LessThan, <nl> + valueInt: 200, <nl> + path: [\"horsepower\"], <nl> + } <nl> + ) { <nl> + horsepower { <nl> + mean <nl> + } <nl> + } <nl> + } <nl> + } <nl> + }`, <nl> + expectedProps: []Property{ <nl> + { <nl> + Name: \"horsepower\", <nl> + Aggregators: []Aggregator{Mean}, <nl> + }, <nl> + }, <nl> + resolverReturn: map[string]interface{}{ <nl> + \"horsepower\": map[string]interface{}{ <nl> + \"mean\": 275.7773, <nl> + }, <nl> + }, <nl> + expectedGroupBy: groupCarByMadeByManufacturerName(), <nl> + expectedResults: []result{{ <nl> + pathToField: []string{\"Aggregate\", \"Things\", \"Car\", \"horsepower\", \"mean\"}, <nl> + expectedValue: 275.7773, <nl> + }}, <nl> + expectedWhereFilter: &common_filters.LocalFilter{ <nl> + Root: &common_filters.Clause{ <nl> + On: &common_filters.Path{ <nl> + Class: schema.ClassName(\"Car\"), <nl> + Property: schema.PropertyName(\"horsepower\"), <nl> + }, <nl> + Value: &common_filters.Value{ <nl> + Value: 200, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + Operator: common_filters.OperatorLessThan, <nl> + }, <nl> + }, <nl> + }, <nl> + <nl> testCase{ <nl> name: \"all int props\", <nl> query: `{ Aggregate { Things { Car(groupBy:[\"madeBy\", \"Manufacturer\", \"name\"]) { horsepower { mean, median, mode, maximum, minimum, count, sum } } } } }`, <nl> @@ -153,6 +205,7 @@ func (tests testCases) AssertExtraction(t *testing.T, k kind.Kind, className str <nl> ClassName: schema.ClassName(className), <nl> Properties: testCase.expectedProps, <nl> GroupBy: testCase.expectedGroupBy, <nl> + Filters: testCase.expectedWhereFilter, <nl> } <nl> resolver.On(\"LocalAggregate\", expectedParams). <nl> ", "msg": "extract and parse where filter from Local.Aggregate\nThis was already working previously due to copy/paste from a place where\nit had been already implemented, however it wasn't tested yet, so there\nwas no proper way of knowing for sure. Now we do."}
{"diff_id": 3773, "repo": "semi-technologies/weaviate", "sha": "df975dc4976d1e708badd76c1ad87805b9ebadcd", "time": "19.02.2019 10:22:19", "diff": "mmm a / database/connectors/janusgraph/helper_add_class.go <nl> ppp b / database/connectors/janusgraph/helper_add_class.go <nl>@@ -51,7 +51,11 @@ type batchChunk struct { <nl> func (j *Janusgraph) addThingsBatch(things []*models.Thing, uuids []strfmt.UUID) error { <nl> chunkSize := MaximumBatchItemsPerQuery <nl> - chunked := make([][]batchChunk, len(things)/chunkSize) <nl> + chunks := len(things) / chunkSize <nl> + if len(things) < chunkSize { <nl> + chunks = 1 <nl> + } <nl> + chunked := make([][]batchChunk, chunks) <nl> chunk := 0 <nl> for i := 0; i < len(things); i++ { <nl> ", "msg": "fix issue if batch size < max chunk size\nPreviously we'd create an array of length 0"}
{"diff_id": 3785, "repo": "semi-technologies/weaviate", "sha": "fb2768cc6a7f4548bcecac3c5f13ac19182585f9", "time": "22.03.2019 12:42:08", "diff": "mmm a / restapi/configure_api.go <nl> ppp b / restapi/configure_api.go <nl>@@ -3,6 +3,7 @@ package restapi <nl> import ( <nl> \"net/http\" <nl> + \"github.com/creativesoftwarefdn/weaviate/models\" <nl> \"github.com/creativesoftwarefdn/weaviate/restapi/batch\" <nl> \"github.com/creativesoftwarefdn/weaviate/restapi/operations\" <nl> \"github.com/go-openapi/errors\" <nl> @@ -14,7 +15,9 @@ func configureAPI(api *operations.WeaviateAPI) http.Handler { <nl> api.JSONConsumer = runtime.JSONConsumer() <nl> - api.OidcAuth = appState.OIDC.ValidateAndExtract <nl> + api.OidcAuth = func(token string, scopes []string) (*models.Principal, error) { <nl> + return appState.OIDC.ValidateAndExtract(token, scopes) <nl> + } <nl> setupSchemaHandlers(api) <nl> setupThingsHandlers(api) <nl> ", "msg": "set OidcAuth to anonymous closure\nOtherwise the value of appState.OIDC will forever stay null. This way we\nwill only retrieve appState.OIDC whenever the auth middleware gets\ncalled."}
{"diff_id": 3796, "repo": "semi-technologies/weaviate", "sha": "edeb0871739d9d258d05ba26bfefba9fef4539ca", "time": "02.04.2019 13:36:57", "diff": "mmm a / database/connectors/janusgraph/fetchfuzzy/query.go <nl> ppp b / database/connectors/janusgraph/fetchfuzzy/query.go <nl>@@ -69,11 +69,23 @@ func (b *Query) predicates() ([]*gremlin.Query, error) { <nl> return nil, fmt.Errorf(\"could not get mapped names: %v\", err) <nl> } <nl> + // janusgraph does not allow more than 253 arguments, so we must abort once <nl> + // we hit too many <nl> + argsCounter := 0 <nl> + limit := 253 <nl> + <nl> +outer: <nl> for _, prop := range mappedProps { <nl> for _, searchterm := range b.params { <nl> + if argsCounter >= limit { <nl> + break outer <nl> + } <nl> + <nl> result = append(result, <nl> gremlin.New().Has(string(prop), gremlin.New().TextContainsFuzzy(searchterm)), <nl> ) <nl> + <nl> + argsCounter++ <nl> } <nl> } <nl> ", "msg": "add hard-query parameter limit\njanusgraph claims the limit is 256, but trial and error has shown that\nanything above 253 will cause errors"}
{"diff_id": 3802, "repo": "semi-technologies/weaviate", "sha": "637a16be973280a53892faacf294547b2b58f7bc", "time": "11.04.2019 18:28:14", "diff": "mmm a / test/acceptance/database_schema/add_class_test.go <nl> ppp b / test/acceptance/database_schema/add_class_test.go <nl>@@ -128,6 +128,17 @@ func TestDeleteSingleProperties(t *testing.T) { <nl> _, err = helper.Client(t).Things.WeaviateThingsList(things.NewWeaviateThingsListParams(), nil) <nl> assert.Nil(t, err, \"listing things should not error\") <nl> + t.Log(\"verifying we could re-add the property with the same name\") <nl> + readdParams := schema.NewWeaviateSchemaThingsPropertiesAddParams(). <nl> + WithClassName(randomThingClassName). <nl> + WithBody(&models.SemanticSchemaClassProperty{ <nl> + Name: \"description\", <nl> + DataType: []string{\"string\"}, <nl> + }) <nl> + <nl> + _, err = helper.Client(t).Schema.WeaviateSchemaThingsPropertiesAdd(readdParams, nil) <nl> + assert.Nil(t, err, \"adding the previously deleted property again should not error\") <nl> + <nl> // Now clean up this class. <nl> t.Log(\"Remove the class\") <nl> delParams := schema.NewWeaviateSchemaThingsDeleteParams().WithClassName(randomThingClassName) <nl> ", "msg": "extend test to also cover\nWe had the assumption that the fix for the former might also help with\nfixing the latter. This test proves that re-adding a previously deleted\nproperty should be possible now.\ncloses"}
{"diff_id": 3807, "repo": "semi-technologies/weaviate", "sha": "4deec6cec8ebabbc74714c9cff3e11a79399f8a0", "time": "21.05.2019 17:00:17", "diff": "mmm a / adapters/handlers/rest/handlers_graphql.go <nl> ppp b / adapters/handlers/rest/handlers_graphql.go <nl>@@ -124,6 +124,12 @@ func setupGraphQLHandlers(api *operations.WeaviateAPI, requestsLog *telemetry.Re <nl> ctx = context.WithValue(ctx, \"principal\", principal) <nl> graphQL := gqlProvider.GetGraphQL() <nl> + if graphQL == nil { <nl> + errRes := errPayloadFromSingleErr(fmt.Errorf(\"no graphql provider present, \" + <nl> + \"this is most likely because no schema is present. Import a schema first!\")) <nl> + return graphql.NewWeaviateGraphqlBatchUnprocessableEntity().WithPayload(errRes) <nl> + } <nl> + <nl> // Generate a goroutine for each separate request <nl> for requestIndex, unbatchedRequest := range params.Body { <nl> wg.Add(1) <nl> @@ -174,9 +180,6 @@ func handleUnbatchedGraphQLRequest(ctx context.Context, wg *sync.WaitGroup, grap <nl> variables = unbatchedRequest.Variables.(map[string]interface{}) <nl> } <nl> - if graphQL == nil { <nl> - panic(\"graphql is nil!\") <nl> - } <nl> result := graphQL.Resolve(ctx, query, operationName, variables) <nl> // Marshal the JSON <nl> ", "msg": "don't panic without gql provider in gql batch\ninstead return a proper error message"}
{"diff_id": 3811, "repo": "semi-technologies/weaviate", "sha": "18348413dbc8d9c063e41af24052c6d0738d73f1", "time": "25.07.2019 09:27:09", "diff": "mmm a / adapters/repos/esvector/filters_integration_test.go <nl> ppp b / adapters/repos/esvector/filters_integration_test.go <nl>@@ -8,6 +8,7 @@ import ( <nl> \"testing\" <nl> \"github.com/elastic/go-elasticsearch/v5\" <nl> + \"github.com/go-openapi/strfmt\" <nl> \"github.com/semi-technologies/weaviate/entities/filters\" <nl> \"github.com/semi-technologies/weaviate/entities/schema\" <nl> \"github.com/semi-technologies/weaviate/entities/schema/kind\" <nl> @@ -31,12 +32,43 @@ func Test_Filters(t *testing.T) { <nl> t.Run(\"prepare test schema and data \", <nl> prepareTestSchemaAndData(repo, migrator)) <nl> - f := buildFilter(\"horsepower\", 130, filters.OperatorEqual, schema.DataTypeInt) <nl> + type test struct { <nl> + name string <nl> + filter *filters.LocalFilter <nl> + expectedLen int <nl> + expectedIDs []strfmt.UUID <nl> + } <nl> + <nl> + // operators <nl> + eq := filters.OperatorEqual <nl> + <nl> + // datatypes <nl> + dtInt := schema.DataTypeInt <nl> + tests := []test{ <nl> + { <nl> + name: \"horsepower == 130\", <nl> + filter: buildFilter(\"horsepower\", 130, eq, dtInt), <nl> + expectedLen: 1, <nl> + expectedIDs: []strfmt.UUID{carSprinterID}, <nl> + }, <nl> + } <nl> + <nl> + for i, test := range tests { <nl> res, err := repo.VectorClassSearch(context.Background(), kind.Thing, <nl> - carClass.Class, []float32{0.1, 0.1, 0.1, 1.1, 0.1}, 100, f) <nl> + carClass.Class, []float32{0.1, 0.1, 0.1, 1.1, 0.1}, 100, test.filter) <nl> require.Nil(t, err) <nl> - assert.Len(t, res, 1) <nl> - assert.Equal(t, carSprinterID, res[0].ID) <nl> + require.Len(t, res, test.expectedLen) <nl> + if len(test.expectedIDs) != test.expectedLen { <nl> + t.Fatalf(\"wrong test setup at pos %d: lens dont match: %d and %d\", <nl> + i, test.expectedLen, len(test.expectedIDs)) <nl> + } <nl> + <nl> + ids := make([]strfmt.UUID, test.expectedLen, test.expectedLen) <nl> + for pos, concept := range res { <nl> + ids[pos] = concept.ID <nl> + } <nl> + assert.ElementsMatch(t, ids, test.expectedIDs, \"ids dont match\") <nl> + } <nl> } <nl> func prepareTestSchemaAndData(repo *Repo, <nl> ", "msg": "restructure esvector filter integration tests\nto make them easier to extend"}
{"diff_id": 3825, "repo": "semi-technologies/weaviate", "sha": "62355ad53a2a0ce47044b91ad319682f71ec7865", "time": "07.10.2019 12:35:05", "diff": "mmm a / adapters/repos/esvector/classifications.go <nl> ppp b / adapters/repos/esvector/classifications.go <nl>@@ -120,15 +120,10 @@ func checkClassificationCount(res map[string]interface{}) error { <nl> func (r *Repo) AggregateNeighbors(ctx context.Context, vector []float32, kind kind.Kind, class string, <nl> properties []string, k int) ([]classification.NeighborRef, error) { <nl> - propertyAggregations := map[string]interface{}{} <nl> mustExist := []map[string]interface{}{} <nl> + var propNames []string <nl> for _, prop := range properties { <nl> - propertyAggregations[prop] = map[string]interface{}{ <nl> - \"terms\": map[string]interface{}{ <nl> - \"size\": 1, <nl> - \"field\": fmt.Sprintf(\"%s.beacon\", prop), <nl> - }, <nl> - } <nl> + propNames = append(propNames, prop) <nl> mustExist = append(mustExist, map[string]interface{}{ <nl> \"exists\": map[string]interface{}{ <nl> \"field\": prop, <nl> @@ -136,15 +131,6 @@ func (r *Repo) AggregateNeighbors(ctx context.Context, vector []float32, kind ki <nl> }) <nl> } <nl> - aggregations := map[string]interface{}{ <nl> - \"sample\": map[string]interface{}{ <nl> - \"sampler\": map[string]interface{}{ <nl> - \"shard_size\": k, <nl> - }, <nl> - \"aggregations\": propertyAggregations, <nl> - }, <nl> - } <nl> - <nl> query := map[string]interface{}{ <nl> \"function_score\": map[string]interface{}{ <nl> \"boost_mode\": \"replace\", <nl> @@ -173,8 +159,8 @@ func (r *Repo) AggregateNeighbors(ctx context.Context, vector []float32, kind ki <nl> body := map[string]interface{}{ <nl> \"query\": query, <nl> - \"aggregations\": aggregations, <nl> - \"size\": 0, <nl> + \"size\": k, <nl> + \"_source\": append(propNames, keyVector.String()), <nl> } <nl> var buf bytes.Buffer <nl> @@ -216,72 +202,93 @@ func (r *Repo) aggregateNeighborsResponse(res *esapi.Response) ([]classification <nl> } <nl> func (r *Repo) aggregationsToClassificationNeighborRefs(input searchResponse) ([]classification.NeighborRef, error) { <nl> - sample, ok := input.Aggregations[\"sample\"] <nl> - if !ok { <nl> - return nil, fmt.Errorf(\"expected aggregation response to contain agg 'sample'\") <nl> + hits := input.Hits.Hits <nl> + <nl> + aggregations, err := extractRefNeighborsFromHits(hits) <nl> + if err != nil { <nl> + return nil, err <nl> } <nl> - asMap, ok := sample.(map[string]interface{}) <nl> - if !ok { <nl> - return nil, fmt.Errorf(\"expected 'sample' to be map, got %T\", sample) <nl> + return aggregateRefNeighbors(aggregations) <nl> + <nl> } <nl> +func aggregateRefNeighbors(props map[string]map[string][]float64) ([]classification.NeighborRef, error) { <nl> var out []classification.NeighborRef <nl> - for key, value := range asMap { <nl> - if key == \"doc_count\" { <nl> - continue <nl> - } <nl> + for prop, beacons := range props { <nl> + var winningBeacon string <nl> + var winningCount int <nl> - inner, err := extractInnerNeighborAgg(key, value) <nl> - if err != nil { <nl> - return nil, fmt.Errorf(\"for prop %s: %v\", key, err) <nl> + for beacon, distances := range beacons { <nl> + if len(distances) > winningCount { <nl> + winningBeacon = beacon <nl> + winningCount = len(distances) <nl> + } <nl> } <nl> - out = append(out, inner) <nl> + out = append(out, classification.NeighborRef{ <nl> + Beacon: strfmt.URI(winningBeacon), <nl> + Count: winningCount, <nl> + Property: prop, <nl> + }) <nl> + <nl> } <nl> return out, nil <nl> } <nl> -func extractInnerNeighborAgg(prop string, agg interface{}) (classification.NeighborRef, error) { <nl> - var out classification.NeighborRef <nl> - asMap, ok := agg.(map[string]interface{}) <nl> - if !ok { <nl> - return out, fmt.Errorf(\"expected inner agg to be map, got %T\", agg) <nl> +func extractRefNeighborsFromHits(hits []hit) (map[string]map[string][]float64, error) { <nl> + // structure is [prop][beacon][[]distance] <nl> + aggregations := map[string]map[string][]float64{} <nl> + <nl> + for _, hit := range hits { <nl> + <nl> + for key, value := range hit.Source { <nl> + if key == keyVector.String() { <nl> + // ignore for now <nl> + continue <nl> } <nl> - buckets, ok := asMap[\"buckets\"] <nl> + // assume is a ref <nl> + prop, ok := aggregations[key] <nl> if !ok { <nl> - return out, fmt.Errorf(\"expected key 'buckets', got %v\", asMap) <nl> + prop = map[string][]float64{} <nl> } <nl> - bucketsSlice, ok := buckets.([]interface{}) <nl> - if !ok { <nl> - return out, fmt.Errorf(\"expected buckets to be a slice, got %T\", buckets) <nl> + beacon, err := extractBeaconFromProp(value) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"prop %s: %v\", key, err) <nl> } <nl> - if len(bucketsSlice) != 1 { <nl> - return out, fmt.Errorf(\"expected buckets to have len=1, got %#v\", bucketsSlice) <nl> + prop[beacon] = append(prop[beacon], 100) <nl> + aggregations[key] = prop <nl> + <nl> + } <nl> } <nl> - bucketMap, ok := bucketsSlice[0].(map[string]interface{}) <nl> - if !ok { <nl> - return out, fmt.Errorf(\"expected key 'buckets' to be map, got %T\", bucketsSlice[0]) <nl> + return aggregations, nil <nl> } <nl> - beacon, ok := bucketMap[\"key\"] <nl> +func extractBeaconFromProp(prop interface{}) (string, error) { <nl> + propSlice, ok := prop.([]interface{}) <nl> if !ok { <nl> - return out, fmt.Errorf(\"expected bucket to have key 'key', got %v\", bucketMap) <nl> + return \"\", fmt.Errorf(\"expected refs to be slice, got %T\", prop) <nl> + } <nl> + <nl> + if len(propSlice) != 1 { <nl> + return \"\", fmt.Errorf(\"expected refs to have len 1, got %d\", len(propSlice)) <nl> } <nl> - count, ok := bucketMap[\"doc_count\"] <nl> + ref := propSlice[0] <nl> + refMap, ok := ref.(map[string]interface{}) <nl> if !ok { <nl> - return out, fmt.Errorf(\"expected bucket to have key 'doc_count', got %v\", bucketMap) <nl> + return \"\", fmt.Errorf(\"expected ref to be map, got %T\", ref) <nl> } <nl> - out.Beacon = strfmt.URI(beacon.(string)) <nl> - out.Count = int(count.(float64)) <nl> - out.Property = prop <nl> + beacon, ok := refMap[\"beacon\"] <nl> + if !ok { <nl> + return \"\", fmt.Errorf(\"expected ref (map) to have field 'beacon', got %v\", refMap) <nl> + } <nl> - return out, nil <nl> + return beacon.(string), nil <nl> } <nl> ", "msg": "aggregate knn neighbors in classification in weaviate\npreviously we were relying on elasticsearch to do this for us, however,\nthat way the winning and losing groups distances were opaque to us. Now\nthat we are aggregating ourselves, we can also extract the distances in\nthe next step"}
{"diff_id": 3829, "repo": "semi-technologies/weaviate", "sha": "9834f04f7fa72ab3172f725c780718d7b3801a29", "time": "11.11.2019 14:31:37", "diff": "mmm a / adapters/repos/esvector/cache_integration_test.go <nl> ppp b / adapters/repos/esvector/cache_integration_test.go <nl>@@ -298,7 +298,7 @@ func testEsVectorCache(t *testing.T) { <nl> }) <nl> // wait for both es indexing as well as esvector caching to be complete <nl> - time.Sleep(1000 * time.Millisecond) <nl> + time.Sleep(2000 * time.Millisecond) <nl> t.Run(\"all 3 (outer) things must now have a hot cache\", func(t *testing.T) { <nl> res, err := repo.ThingByID(context.Background(), \"18c80a16-346a-477d-849d-9d92e5040ac9\", <nl> @@ -426,7 +426,7 @@ func testEsVectorCache(t *testing.T) { <nl> refreshAll(t, client) <nl> // wait for both es indexing as well as esvector caching to be complete <nl> - time.Sleep(1000 * time.Millisecond) <nl> + time.Sleep(2000 * time.Millisecond) <nl> t.Run(\"the newly added place must have a hot cache by now\", func(t *testing.T) { <nl> res, err := repo.ThingByID(context.Background(), \"0f02d525-902d-4dc0-8052-647cb420c1a6\", traverser.SelectProperties{}, <nl> ", "msg": "increase wait for cache periods in integration tests\nas we are adding more integration tests it seems they take longer on CI,\nso we need to wait longer, too."}
{"diff_id": 3831, "repo": "semi-technologies/weaviate", "sha": "c7de4e79fe71ea45d328dc9e4d3abdca638a3e65", "time": "21.11.2019 14:33:19", "diff": "mmm a / usecases/classification/classifier_run_contextual.go <nl> ppp b / usecases/classification/classifier_run_contextual.go <nl>@@ -14,20 +14,89 @@ import ( <nl> \"github.com/semi-technologies/weaviate/usecases/traverser\" <nl> ) <nl> +type contextualItemClassifier struct { <nl> + item search.Result <nl> + kind kind.Kind <nl> + params models.Classification <nl> + classifier *Classifier <nl> + schema schema.Schema <nl> +} <nl> + <nl> func (c *Classifier) classifyItemContextual(item search.Result, kind kind.Kind, params models.Classification) error { <nl> - s := c.schemaGetter.GetSchemaSkipAuth() <nl> - // safe to skip nil-check as we passed validation <nl> + schema := c.schemaGetter.GetSchemaSkipAuth() <nl> + run := &contextualItemClassifier{ <nl> + item: item, <nl> + kind: kind, <nl> + params: params, <nl> + classifier: c, <nl> + schema: schema, <nl> + } <nl> + <nl> + err := run.do() <nl> + if err != nil { <nl> + return fmt.Errorf(\"contextual: %v\", err) <nl> + } <nl> + return nil <nl> +} <nl> + <nl> +func (c *contextualItemClassifier) do() error { <nl> var classified []string <nl> - for _, propName := range params.ClassifyProperties { <nl> - prop, err := s.GetProperty(kind, schema.ClassName(params.Class), schema.PropertyName(propName)) <nl> + for _, propName := range c.params.ClassifyProperties { <nl> + current, err := c.property(propName) <nl> + if err != nil { <nl> + return fmt.Errorf(\"prop '%s': %v\", propName, err) <nl> + } <nl> + <nl> + // append list of actually classified (can differ from scope!) properties, <nl> + // so we can build the object meta information <nl> + classified = append(classified, current) <nl> + } <nl> + <nl> + c.classifier.extendItemWithObjectMeta(&c.item, c.params, classified) <nl> + err := c.classifier.store(c.item) <nl> + if err != nil { <nl> + return fmt.Errorf(\"store %s/%s: %v\", c.item.ClassName, c.item.ID, err) <nl> + } <nl> + <nl> + return nil <nl> +} <nl> + <nl> +func (c *contextualItemClassifier) property(propName string) (string, error) { <nl> + targetClass, targetKind, err := c.classAndKindOfTarget(propName) <nl> + if err != nil { <nl> + return \"\", fmt.Errorf(\"inspect target: %v\", err) <nl> + } <nl> + <nl> + res, err := c.findTarget(targetClass, targetKind) <nl> + if err != nil { <nl> + return \"\", fmt.Errorf(\"find target: %v\", err) <nl> + } <nl> + <nl> + targetBeacon := crossref.New(\"localhost\", res.ID, res.Kind).String() <nl> + c.item.Schema.(map[string]interface{})[propName] = models.MultipleRef{ <nl> + &models.SingleRef{ <nl> + Beacon: strfmt.URI(targetBeacon), <nl> + Meta: &models.ReferenceMeta{ <nl> + Classification: &models.ReferenceMetaClassification{ <nl> + WinningDistance: float64(9000), // TODO <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + return propName, nil <nl> +} <nl> + <nl> +func (c *contextualItemClassifier) classAndKindOfTarget(propName string) (schema.ClassName, kind.Kind, error) { <nl> + prop, err := c.schema.GetProperty(c.kind, schema.ClassName(c.params.Class), schema.PropertyName(propName)) <nl> if err != nil { <nl> - return fmt.Errorf(\"contextual: get target prop '%s': %v\", propName, err) <nl> + return \"\", \"\", fmt.Errorf(\"get target prop '%s': %v\", propName, err) <nl> } <nl> - dataType, err := s.FindPropertyDataType(prop.DataType) <nl> + dataType, err := c.schema.FindPropertyDataType(prop.DataType) <nl> if err != nil { <nl> - return fmt.Errorf(\"contextual: extract dataType of prop '%s': %v\", propName, err) <nl> + return \"\", \"\", fmt.Errorf(\"extract dataType of prop '%s': %v\", propName, err) <nl> } <nl> // we have passed validation, so it is safe to assume that this is a ref prop <nl> @@ -35,10 +104,14 @@ func (c *Classifier) classifyItemContextual(item search.Result, kind kind.Kind, <nl> // len=1 is guaranteed from validation <nl> targetClass := targetClasses[0] <nl> - targetKind, _ := s.GetKindOfClass(targetClass) <nl> + targetKind, _ := c.schema.GetKindOfClass(targetClass) <nl> + <nl> + return targetClass, targetKind, nil <nl> +} <nl> - res, err := c.vectorRepo.VectorClassSearch(context.Background(), traverser.GetParams{ <nl> - SearchVector: item.Vector, <nl> +func (c *contextualItemClassifier) findTarget(targetClass schema.ClassName, targetKind kind.Kind) (*search.Result, error) { <nl> + res, err := c.classifier.vectorRepo.VectorClassSearch(context.Background(), traverser.GetParams{ <nl> + SearchVector: c.item.Vector, <nl> ClassName: targetClass.String(), <nl> Kind: targetKind, <nl> Pagination: &filters.Pagination{ <nl> @@ -51,35 +124,12 @@ func (c *Classifier) classifyItemContextual(item search.Result, kind kind.Kind, <nl> }, <nl> }) <nl> if err != nil { <nl> - return fmt.Errorf(\"contextual: search closest target: %v\", err) <nl> + return nil, fmt.Errorf(\"search closest target: %v\", err) <nl> } <nl> if res == nil || len(res) == 0 { <nl> - return fmt.Errorf(\"contexual: no potential targets found of class '%s' (%s)\", targetClass, targetKind) <nl> - } <nl> - <nl> - targetBeacon := crossref.New(\"localhost\", res[0].ID, res[0].Kind).String() <nl> - item.Schema.(map[string]interface{})[propName] = models.MultipleRef{ <nl> - &models.SingleRef{ <nl> - Beacon: strfmt.URI(targetBeacon), <nl> - Meta: &models.ReferenceMeta{ <nl> - Classification: &models.ReferenceMetaClassification{ <nl> - WinningDistance: float64(9000), // TODO <nl> - }, <nl> - }, <nl> - }, <nl> + return nil, fmt.Errorf(\"no potential targets found of class '%s' (%s)\", targetClass, targetKind) <nl> } <nl> - // append list of actually classified (can differ from scope!) properties, <nl> - // so we can build the object meta information <nl> - classified = append(classified, propName) <nl> - } <nl> - <nl> - c.extendItemWithObjectMeta(&item, params, classified) <nl> - err := c.store(item) <nl> - if err != nil { <nl> - return fmt.Errorf(\"store %s/%s: %v\", item.ClassName, item.ID, err) <nl> - } <nl> - <nl> - return nil <nl> + return &res[0], nil <nl> } <nl> ", "msg": "clean up classify item contextually\nfrom a massive almost 100line method to a few smaller, much clearer (and\ncleaner) methods"}
{"diff_id": 3833, "repo": "semi-technologies/weaviate", "sha": "8a89eb4330861f616626f6ad776187dd23b850bb", "time": "06.12.2019 17:19:31", "diff": "mmm a / usecases/classification/classifier.go <nl> ppp b / usecases/classification/classifier.go <nl>@@ -191,9 +191,23 @@ func (c *Classifier) setDefaultValuesForOptionalFields(params *models.Classifica <nl> params.Type = &defaultType <nl> } <nl> + if *params.Type == \"knn\" { <nl> + c.setDefaultsForKNN(params) <nl> + } <nl> + <nl> + if *params.Type == \"contextual\" { <nl> + c.setDefaultsForContextual(params) <nl> + } <nl> + <nl> +} <nl> + <nl> +func (c *Classifier) setDefaultsForKNN(params *models.Classification) { <nl> if params.K == nil { <nl> defaultK := int32(3) <nl> params.K = &defaultK <nl> } <nl> +} <nl> +func (c *Classifier) setDefaultsForContextual(params *models.Classification) { <nl> + // none at the moment <nl> } <nl> ", "msg": "only set defaults for respective types\nfixes"}
{"diff_id": 3834, "repo": "semi-technologies/weaviate", "sha": "07cd24462307055e4dea6846c4e81c7b7165f6fa", "time": "09.12.2019 18:13:03", "diff": "mmm a / test/acceptance/graphql_resolvers/setup_test.go <nl> ppp b / test/acceptance/graphql_resolvers/setup_test.go <nl>@@ -28,17 +28,20 @@ import ( <nl> func Test_GraphQL(t *testing.T) { <nl> t.Run(\"setup test schema\", addTestSchema) <nl> - t.Run(\"import test data\", addTestData) <nl> + t.Run(\"import test data (city, country, airport)\", addTestDataCityAirport) <nl> + t.Run(\"import test data (companies)\", addTestDataCompanies) <nl> // tests <nl> t.Run(\"getting objects\", gettingObjects) <nl> t.Run(\"getting objects with filters\", gettingObjectsWithFilters) <nl> t.Run(\"getting objects with geo filters\", gettingObjectsWithGeoFilters) <nl> + t.Run(\"getting objects with grouping\", gettingObjectsWithGrouping) <nl> // tear down <nl> deleteThingClass(t, \"Country\") <nl> deleteThingClass(t, \"City\") <nl> deleteThingClass(t, \"Airport\") <nl> + deleteThingClass(t, \"Company\") <nl> } <nl> func createThingClass(t *testing.T, class *models.Class) { <nl> @@ -105,9 +108,23 @@ func addTestSchema(t *testing.T) { <nl> }, <nl> }, <nl> }) <nl> + <nl> + createThingClass(t, &models.Class{ <nl> + Class: \"Company\", <nl> + Properties: []*models.Property{ <nl> + &models.Property{ <nl> + Name: \"name\", <nl> + DataType: []string{\"string\"}, <nl> + }, <nl> + &models.Property{ <nl> + Name: \"inCity\", <nl> + DataType: []string{\"City\"}, <nl> + }, <nl> + }, <nl> + }) <nl> } <nl> -func addTestData(t *testing.T) { <nl> +func addTestDataCityAirport(t *testing.T) { <nl> var ( <nl> netherlands strfmt.UUID = \"67b79643-cf8b-4b22-b206-6e63dbb4e57a\" <nl> @@ -259,3 +276,47 @@ func addTestData(t *testing.T) { <nl> // give cache some time to become hot <nl> time.Sleep(2 * time.Second) <nl> } <nl> + <nl> +func addTestDataCompanies(t *testing.T) { <nl> + var ( <nl> + microsoft1 strfmt.UUID = \"cfa3b21e-ca4f-4db7-a432-7fc6a23c534d\" <nl> + microsoft2 strfmt.UUID = \"8f75ed97-39dd-4294-bff7-ecabd7923062\" <nl> + microsoft3 strfmt.UUID = \"f343f51d-7e05-4084-bd66-d504db3b6bec\" <nl> + apple1 strfmt.UUID = \"477fec91-1292-4928-8f53-f0ff49c76900\" <nl> + apple2 strfmt.UUID = \"bb2cfdba-d4ba-4cf8-abda-e719ef35ac33\" <nl> + apple3 strfmt.UUID = \"b71d2b4c-3da1-4684-9c5e-aabd2a4f2998\" <nl> + google1 strfmt.UUID = \"8c2e21fc-46fe-4999-b41c-a800595129af\" <nl> + google2 strfmt.UUID = \"62b969c6-f184-4be0-8c40-7470af417cfc\" <nl> + google3 strfmt.UUID = \"c7829929-2037-4420-acbc-a433269feb93\" <nl> + ) <nl> + <nl> + type nameAndID struct { <nl> + id strfmt.UUID <nl> + name string <nl> + } <nl> + <nl> + companies := []nameAndID{ <nl> + nameAndID{id: microsoft1, name: \"Microsoft Inc.\"}, <nl> + nameAndID{id: microsoft2, name: \"Microsoft Incorporated\"}, <nl> + nameAndID{id: microsoft3, name: \"Microsoft\"}, <nl> + nameAndID{id: apple1, name: \"Apple Inc.\"}, <nl> + nameAndID{id: apple2, name: \"Apple Incorporated\"}, <nl> + nameAndID{id: apple3, name: \"Apple\"}, <nl> + nameAndID{id: google1, name: \"Google Inc.\"}, <nl> + nameAndID{id: google2, name: \"Google Incorporated\"}, <nl> + nameAndID{id: google3, name: \"Google\"}, <nl> + } <nl> + <nl> + // companies <nl> + for _, company := range companies { <nl> + createThing(t, &models.Thing{ <nl> + Class: \"Company\", <nl> + ID: company.id, <nl> + Schema: map[string]interface{}{ <nl> + \"name\": company.name, <nl> + }, <nl> + }) <nl> + } <nl> + <nl> + assertGetThingEventually(t, companies[len(companies)-1].id) <nl> +} <nl> ", "msg": "prepare journey test for complete feature\nnon-control test is currently skipped as feature is not implemented yet"}
{"diff_id": 3841, "repo": "semi-technologies/weaviate", "sha": "a3e1b052f23cdec3a79d8d80e44086d2897f24ee", "time": "23.01.2020 10:13:58", "diff": "mmm a / adapters/repos/esvector/cache_integration_test.go <nl> ppp b / adapters/repos/esvector/nested_references_integration_test.go <nl>@@ -20,7 +20,6 @@ import ( <nl> \"fmt\" <nl> \"sync\" <nl> \"testing\" <nl> - \"time\" <nl> \"github.com/elastic/go-elasticsearch/v5\" <nl> \"github.com/semi-technologies/weaviate/entities/models\" <nl> @@ -33,7 +32,7 @@ import ( <nl> \"github.com/stretchr/testify/require\" <nl> ) <nl> -func testEsVectorCache(t *testing.T) { <nl> +func TestNestedReferences(t *testing.T) { <nl> client, err := elasticsearch.NewClient(elasticsearch.Config{ <nl> Addresses: []string{\"http://localhost:9201\"}, <nl> }) <nl> @@ -197,8 +196,6 @@ func testEsVectorCache(t *testing.T) { <nl> refreshAll(t, client) <nl> - var before *search.Result <nl> - _ = before <nl> t.Run(\"fully resolving the place before we have cache\", func(t *testing.T) { <nl> expectedSchema := map[string]interface{}{ <nl> \"InCity\": []interface{}{ <nl> @@ -251,10 +248,9 @@ func testEsVectorCache(t *testing.T) { <nl> // yet. Quering the initial places is a single request, each nested level <nl> // is another request; 1+4=5 <nl> assert.Equal(t, 5, requestCounter.count) <nl> - before = res <nl> }) <nl> - t.Run(\"partially resolving the place before we have cache\", func(t *testing.T) { <nl> + t.Run(\"partially resolving the place\", func(t *testing.T) { <nl> expectedSchema := map[string]interface{}{ <nl> \"InCity\": []interface{}{ <nl> search.LocalRef{ <nl> @@ -292,77 +288,6 @@ func testEsVectorCache(t *testing.T) { <nl> assert.Equal(t, 2, requestCounter.count) <nl> }) <nl> - t.Run(\"init caching state machine\", func(t *testing.T) { <nl> - repo.InitCacheIndexing(50, 200*time.Millisecond, 200*time.Millisecond) <nl> - }) <nl> - <nl> - // wait for both es indexing as well as esvector caching to be complete <nl> - time.Sleep(2000 * time.Millisecond) <nl> - <nl> - t.Run(\"all 3 (outer) things must now have a hot cache\", func(t *testing.T) { <nl> - res, err := repo.ThingByID(context.Background(), \"18c80a16-346a-477d-849d-9d92e5040ac9\", <nl> - traverser.SelectProperties{}, false) <nl> - require.Nil(t, err) <nl> - assert.Equal(t, true, res.CacheHot) <nl> - <nl> - res, err = repo.ThingByID(context.Background(), \"18c80a16-346a-477d-849d-9d92e5040ac9\", <nl> - traverser.SelectProperties{}, false) <nl> - require.Nil(t, err) <nl> - assert.Equal(t, true, res.CacheHot) <nl> - <nl> - res, err = repo.ThingByID(context.Background(), \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> - traverser.SelectProperties{}, false) <nl> - require.Nil(t, err) <nl> - assert.Equal(t, true, res.CacheHot) <nl> - }) <nl> - <nl> - t.Run(\"inspecting the cache for the place to make sure caching stopped at the configured boundary\", <nl> - func(t *testing.T) { <nl> - requestCounter.reset() <nl> - res, err := repo.ThingByID(context.Background(), \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> - traverser.SelectProperties{}, false) <nl> - require.Nil(t, err) <nl> - // we didn't specify any selectProperties, so there shouldn't be any <nl> - // additional requests at all, only the initial request to get the place <nl> - assert.Equal(t, 1, requestCounter.count) <nl> - <nl> - // our desired depth is 2, this means 2 refs should be fully <nl> - // resolved, whereas the 3rd one is merely referenced by a beacon. <nl> - // This means we should see fully resolved <nl> - // inCity/City->inCountry/Country->, the next level <nl> - // (onContinent/continent) should merely be referenced by an unresolved beacon <nl> - <nl> - inCity := res.CacheSchema[\"inCity\"].(map[string]interface{}) <nl> - city := inCity[\"City\"].([]interface{})[0].(map[string]interface{}) <nl> - inCountry := city[\"inCountry\"].(map[string]interface{}) <nl> - country := inCountry[\"Country\"].([]interface{})[0].(map[string]interface{}) <nl> - <nl> - refs, ok := country[\"onContinent\"].([]interface{}) <nl> - // if onPlanet were resolved it would be a map. The fact that it's a <nl> - // slice is the first indication that it was unresolved <nl> - assert.True(t, ok) <nl> - require.Len(t, refs, 1) <nl> - <nl> - firstRef, ok := refs[0].(map[string]interface{}) <nl> - assert.True(t, ok) <nl> - <nl> - beacon, ok := firstRef[\"beacon\"] <nl> - assert.True(t, ok) <nl> - assert.Equal(t, \"weaviate://localhost/things/4aad8154-e7f3-45b8-81a6-725171419e55\", beacon) <nl> - }) <nl> - <nl> - t.Run(\"fully resolving the place after cache is hot\", func(t *testing.T) { <nl> - requestCounter.reset() <nl> - res, err := repo.ThingByID(context.Background(), \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> - fullyNestedSelectProperties(), false) <nl> - // we are crossing the cache boundary, so we are expecting an additional request <nl> - assert.Equal(t, 2, requestCounter.count) <nl> - <nl> - require.Nil(t, err) <nl> - <nl> - assert.Equal(t, before.Schema, res.Schema, \"result without a cache and with a cache should look the same\") <nl> - }) <nl> - <nl> t.Run(\"resolving without any refs\", func(t *testing.T) { <nl> res, err := repo.ThingByID(context.Background(), \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> traverser.SelectProperties{}, false) <nl> @@ -382,28 +307,6 @@ func testEsVectorCache(t *testing.T) { <nl> assert.Equal(t, expectedSchema, res.Schema, \"does not contain any resolved refs\") <nl> }) <nl> - t.Run(\"partially resolving the place after cache is hot\", func(t *testing.T) { <nl> - res, err := repo.ThingByID(context.Background(), \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> - partiallyNestedSelectProperties(), false) <nl> - expectedSchema := map[string]interface{}{ <nl> - \"InCity\": []interface{}{ <nl> - search.LocalRef{ <nl> - Class: \"City\", <nl> - Fields: map[string]interface{}{ <nl> - \"name\": \"San Francisco\", <nl> - \"uuid\": \"2297e094-6218-43d4-85b1-3d20af752f23\", <nl> - }, <nl> - }, <nl> - }, <nl> - \"name\": \"Tim Apple's Fruit Bar\", <nl> - \"uuid\": \"4ef47fb0-3cf5-44fc-b378-9e217dff13ac\", <nl> - } <nl> - <nl> - require.Nil(t, err) <nl> - <nl> - assert.Equal(t, expectedSchema, res.Schema, \"result without a cache and with a cache should look the same\") <nl> - }) <nl> - <nl> t.Run(\"adding a new place to verify idnexing is constantly happening in the background\", func(t *testing.T) { <nl> newPlace := models.Thing{ <nl> Class: \"Place\", <nl> @@ -424,51 +327,6 @@ func testEsVectorCache(t *testing.T) { <nl> }) <nl> refreshAll(t, client) <nl> - // wait for both es indexing as well as esvector caching to be complete <nl> - time.Sleep(2000 * time.Millisecond) <nl> - <nl> - t.Run(\"the newly added place must have a hot cache by now\", func(t *testing.T) { <nl> - res, err := repo.ThingByID(context.Background(), \"0f02d525-902d-4dc0-8052-647cb420c1a6\", traverser.SelectProperties{}, <nl> - false) <nl> - require.Nil(t, err) <nl> - assert.Equal(t, true, res.CacheHot) <nl> - }) <nl> - <nl> - t.Run(\"the newly added place respects cache boundaries\", func(t *testing.T) { <nl> - requestCounter.reset() <nl> - res, err := repo.ThingByID(context.Background(), \"0f02d525-902d-4dc0-8052-647cb420c1a6\", <nl> - traverser.SelectProperties{}, false) <nl> - require.Nil(t, err) <nl> - // we didn't specify any selectProperties, so there shouldn't be any <nl> - // additional requests at all, only the initial request to get the place <nl> - assert.Equal(t, 1, requestCounter.count) <nl> - <nl> - // our desired depth is 2, this means 2 refs should be fully <nl> - // resolved, whereas the 3rd one is merely referenced by a beacon. <nl> - // This means we should see fully resolved <nl> - // inCity/City->inCountry/Country->, the next level <nl> - // (onContinent/continent) should merely be referenced by an unresolved beacon <nl> - <nl> - inCity := res.CacheSchema[\"inCity\"].(map[string]interface{}) <nl> - city := inCity[\"City\"].([]interface{})[0].(map[string]interface{}) <nl> - inCountry := city[\"inCountry\"].(map[string]interface{}) <nl> - country := inCountry[\"Country\"].([]interface{})[0].(map[string]interface{}) <nl> - <nl> - refs, ok := country[\"onContinent\"].([]interface{}) <nl> - // if onPlanet were resolved it would be a map. The fact that it's a <nl> - // slice is the first indication that it was unresolved <nl> - require.True(t, ok, fmt.Sprintf(\"should be a slice, got %#v\", country[\"onContinent\"])) <nl> - require.Len(t, refs, 1) <nl> - <nl> - firstRef, ok := refs[0].(map[string]interface{}) <nl> - assert.True(t, ok) <nl> - <nl> - beacon, ok := firstRef[\"beacon\"] <nl> - assert.True(t, ok) <nl> - assert.Equal(t, \"weaviate://localhost/things/4aad8154-e7f3-45b8-81a6-725171419e55\", beacon) <nl> - }) <nl> - <nl> - repo.StopCacheIndexing() <nl> } <nl> func fullyNestedSelectProperties() traverser.SelectProperties { <nl> ", "msg": "reenable (and fix) tests for deeply nested queries"}
{"diff_id": 3851, "repo": "semi-technologies/weaviate", "sha": "58daae76813e9b14b5f6cce90071029d40db7bba", "time": "19.06.2020 14:49:11", "diff": "mmm a / adapters/repos/esvector/batch.go <nl> ppp b / adapters/repos/esvector/batch.go <nl>@@ -91,7 +91,7 @@ func (r Repo) encodeBatchActions(enc *json.Encoder, batch kinds.BatchActions) er <nl> vectorWeights = a.VectorWeights.(map[string]string) <nl> } <nl> bucket := r.objectBucket(kind.Action, a.ID.String(), a.Class, a.Schema, <nl> - nil, vectorWeights, single.Vector, a.CreationTimeUnix, a.LastUpdateTimeUnix) <nl> + a.Meta, vectorWeights, single.Vector, a.CreationTimeUnix, a.LastUpdateTimeUnix) <nl> index := classIndexFromClassName(kind.Action, a.Class) <nl> control := r.bulkIndexControlObject(index, a.ID.String()) <nl> @@ -180,7 +180,7 @@ func (r Repo) encodeBatchThings(enc *json.Encoder, batch kinds.BatchThings) erro <nl> vectorWeights = t.VectorWeights.(map[string]string) <nl> } <nl> bucket := r.objectBucket(kind.Thing, t.ID.String(), t.Class, t.Schema, <nl> - nil, vectorWeights, single.Vector, t.CreationTimeUnix, t.LastUpdateTimeUnix) <nl> + t.Meta, vectorWeights, single.Vector, t.CreationTimeUnix, t.LastUpdateTimeUnix) <nl> index := classIndexFromClassName(kind.Thing, t.Class) <nl> control := r.bulkIndexControlObject(index, t.ID.String()) <nl> ", "msg": "fix issue where meta info is ignored on batch imports"}
{"diff_id": 3852, "repo": "semi-technologies/weaviate", "sha": "5a3492f899e2ed8b63c7cca5fa3186decc8ed5c2", "time": "23.06.2020 12:21:06", "diff": "mmm a / test/acceptance/graphql_resolvers/local_get_with_underscores.go <nl> ppp b / test/acceptance/graphql_resolvers/local_get_with_underscores.go <nl>@@ -154,4 +154,51 @@ func gettingObjectsWithUnderscoreProps(t *testing.T) { <nl> assert.ElementsMatch(t, expected, companies) <nl> }) <nl> + <nl> + t.Run(\"with _nearestNeighbors set\", func(t *testing.T) { <nl> + query := ` <nl> + { <nl> + Get { <nl> + Things { <nl> + Company { <nl> + _nearestNeighbors{ <nl> + neighbors { <nl> + concept <nl> + distance <nl> + } <nl> + } <nl> + name <nl> + } <nl> + } <nl> + } <nl> + } <nl> + ` <nl> + result := AssertGraphQL(t, helper.RootAuth, query) <nl> + companies := result.Get(\"Get\", \"Things\", \"Company\").AsSlice() <nl> + <nl> + extractNeighbors := func(in interface{}) []interface{} { <nl> + return in.(map[string]interface{})[\"_nearestNeighbors\"].(map[string]interface{})[\"neighbors\"].([]interface{}) <nl> + } <nl> + <nl> + neighbors0 := extractNeighbors(companies[0]) <nl> + neighbors1 := extractNeighbors(companies[1]) <nl> + neighbors2 := extractNeighbors(companies[2]) <nl> + <nl> + validateNeighbors(t, neighbors0, neighbors1, neighbors2) <nl> + }) <nl> +} <nl> + <nl> +func validateNeighbors(t *testing.T, neighborsGroups ...[]interface{}) { <nl> + for i, group := range neighborsGroups { <nl> + if len(group) == 0 { <nl> + t.Fatalf(\"group %d: length of neighbors is 0\", i) <nl> + } <nl> + <nl> + for j, neighbor := range group { <nl> + asMap := neighbor.(map[string]interface{}) <nl> + if len(asMap[\"concept\"].(string)) == 0 { <nl> + t.Fatalf(\"group %d: element %d: concept has length 0\", i, j) <nl> + } <nl> + } <nl> + } <nl> } <nl> ", "msg": "add journey test for _nearestNeighbors in GraphQL Get"}
{"diff_id": 3854, "repo": "semi-technologies/weaviate", "sha": "bc7fd7e3d9f80bdf565969142a4386a465003657", "time": "08.07.2020 16:35:51", "diff": "mmm a / usecases/sempath/builder.go <nl> ppp b / usecases/sempath/builder.go <nl>@@ -64,15 +64,24 @@ func (f *PathBuilder) CalculatePath(in []search.Result, params *Params) ([]searc <nl> return nil, err <nl> } <nl> - for _, obj := range in { <nl> - f.calculatePathPerObject(obj, in, params, searchNeighbors) <nl> + for i, obj := range in { <nl> + path, err := f.calculatePathPerObject(obj, in, params, searchNeighbors) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"object %d: %v\", i, err) <nl> + } <nl> + <nl> + if in[i].UnderscoreProperties == nil { <nl> + in[i].UnderscoreProperties = &models.UnderscoreProperties{} <nl> + } <nl> + <nl> + in[i].UnderscoreProperties.SemanticPath = path <nl> } <nl> return in, nil <nl> } <nl> func (f *PathBuilder) calculatePathPerObject(obj search.Result, allObjects []search.Result, params *Params, <nl> - searchNeighbors []*models.NearestNeighbor) ([]search.Result, error) { <nl> + searchNeighbors []*models.NearestNeighbor) (*models.SemanticPath, error) { <nl> dims := len(obj.Vector) <nl> matrix, neighbors, err := f.vectorsToMatrix(obj, allObjects, dims, params, searchNeighbors) <nl> if err != nil { <nl> @@ -88,6 +97,9 @@ func (f *PathBuilder) calculatePathPerObject(obj search.Result, allObjects []sea <nl> return nil, fmt.Errorf(\"have different output results than input %d != %d\", inputRows, rows) <nl> } <nl> + // create an explicit copy of the neighbors, so we don't mutate them. <nl> + // Otherwise the 2nd round will have been influenced by the first <nl> + projectedNeighbors := copyNeighbors(searchNeighbors) <nl> var projectedSearchVector []float32 <nl> var projectedTargetVector []float32 <nl> for i := 0; i < rows; i++ { <nl> @@ -99,16 +111,14 @@ func (f *PathBuilder) calculatePathPerObject(obj search.Result, allObjects []sea <nl> projectedTargetVector = vector <nl> } else if i < 1+len(neighbors) { <nl> // these must be neighbor props <nl> - neighbors[i-1].Vector = vector <nl> + projectedNeighbors[i-1].Vector = vector <nl> } else { <nl> // is now the very last element which is the search vector <nl> projectedSearchVector = vector <nl> } <nl> } <nl> - f.buildPath(neighbors, projectedSearchVector, projectedTargetVector) <nl> - <nl> - return nil, nil <nl> + return f.buildPath(projectedNeighbors, projectedSearchVector, projectedTargetVector), nil <nl> } <nl> func (f *PathBuilder) addSearchNeighbors(params *Params) ([]*models.NearestNeighbor, error) { <nl> @@ -133,7 +143,7 @@ func (f *PathBuilder) vectorsToMatrix(obj search.Result, allObjects []search.Res <nl> // concat all vectors to build gonum dense matrix <nl> mergedVectors := make([]float64, items*dims) <nl> if l := len(obj.Vector); l != dims { <nl> - return nil, nil, fmt.Errorf(\"inconsistent vector lengths found: %d and %d\", dims, l) <nl> + return nil, nil, fmt.Errorf(\"object: inconsistent vector lengths found: dimensions=%d and object=%d\", dims, l) <nl> } <nl> for j, dim := range obj.Vector { <nl> @@ -149,7 +159,7 @@ func (f *PathBuilder) vectorsToMatrix(obj search.Result, allObjects []search.Res <nl> } <nl> if l := len(neighborVector); l != dims { <nl> - return nil, nil, fmt.Errorf(\"inconsistent vector lengths found: %d and %d\", dims, l) <nl> + return nil, nil, fmt.Errorf(\"neighbor: inconsistent vector lengths found: dimensions=%d and object=%d\", dims, l) <nl> } <nl> for j, dim := range neighborVector { <nl> @@ -249,8 +259,9 @@ func (ec *errorCompounder) toError() error { <nl> return errors.New(msg.String()) <nl> } <nl> -func (f *PathBuilder) buildPath(neighbors []*models.NearestNeighbor, searchVector []float32, target []float32) { <nl> - var path []*models.NearestNeighbor <nl> +func (f *PathBuilder) buildPath(neighbors []*models.NearestNeighbor, searchVector []float32, <nl> + target []float32) *models.SemanticPath { <nl> + var path []*models.SemanticPathElement <nl> var minDist = float32(math.MaxFloat32) <nl> @@ -266,12 +277,14 @@ func (f *PathBuilder) buildPath(neighbors []*models.NearestNeighbor, searchVecto <nl> current = nn[0].Vector <nl> minDist = f.distance(current, target) <nl> - path = append(path, nn[0]) <nl> + path = append(path, &models.SemanticPathElement{ <nl> + Concept: nn[0].Concept, <nl> + // TODO: add distances <nl> + }) <nl> } <nl> - fmt.Printf(\"\\n\\n\\npath:\\n\") <nl> - for _, elem := range path { <nl> - fmt.Printf(\" - %s\", elem.Concept) <nl> + return &models.SemanticPath{ <nl> + Path: path, <nl> } <nl> } <nl> @@ -306,3 +319,17 @@ func (f *PathBuilder) discardFurtherThan(candidates []*models.NearestNeighbor, t <nl> return out[:i] <nl> } <nl> + <nl> +// craete an explicit deep copy that does not keep any references <nl> +func copyNeighbors(in []*models.NearestNeighbor) []*models.NearestNeighbor { <nl> + out := make([]*models.NearestNeighbor, len(in)) <nl> + for i, n := range in { <nl> + out[i] = &models.NearestNeighbor{ <nl> + Concept: n.Concept, <nl> + Distance: n.Distance, <nl> + Vector: n.Vector, <nl> + } <nl> + } <nl> + <nl> + return out <nl> +} <nl> ", "msg": "explicitly copy neighbors to avoid leaking mutations"}
{"diff_id": 3864, "repo": "semi-technologies/weaviate", "sha": "e6ef25d1114848233dc817d3e115d3b3b42b5f42", "time": "04.08.2020 15:30:31", "diff": "mmm a / adapters/repos/db/shard_write.go <nl> ppp b / adapters/repos/db/shard_write.go <nl>@@ -34,33 +34,36 @@ func (s *Shard) putObject(ctx context.Context, object *storobj.Object) error { <nl> return err <nl> } <nl> - data, err := object.MarshalBinary() <nl> - if err != nil { <nl> - return errors.Wrapf(err, \"marshal object %s to binary\", object.ID()) <nl> - } <nl> - <nl> invertProps, err := s.analyzeObject(object) <nl> if err != nil { <nl> return err <nl> } <nl> var docID uint32 <nl> + var isUpdate bool <nl> if err := s.db.Batch(func(tx *bolt.Tx) error { <nl> bucket := tx.Bucket(helpers.ObjectsBucket) <nl> existing := bucket.Get([]byte(idBytes)) <nl> if existing == nil { <nl> + isUpdate = false <nl> docID, err = s.counter.GetAndInc() <nl> if err != nil { <nl> return errors.Wrap(err, \"get new doc id from counter\") <nl> } <nl> - object.SetIndexID(docID) <nl> } else { <nl> + isUpdate = true <nl> docID, err = storobj.DocIDFromBinary(existing) <nl> if err != nil { <nl> return errors.Wrap(err, \"get existing doc id from object binary\") <nl> } <nl> } <nl> + object.SetIndexID(docID) <nl> + <nl> + data, err := object.MarshalBinary() <nl> + if err != nil { <nl> + return errors.Wrapf(err, \"marshal object %s to binary\", object.ID()) <nl> + } <nl> // insert data object <nl> if err := bucket.Put([]byte(idBytes), data); err != nil { <nl> @@ -72,10 +75,18 @@ func (s *Shard) putObject(ctx context.Context, object *storobj.Object) error { <nl> return errors.Wrap(err, \"put inverted indices props\") <nl> } <nl> + if !isUpdate { <nl> + // TODO gh-1221: the above is an over-simplification to make sure that on <nl> + // an update we don't add index id duplicates, so instaed we simply don't <nl> + // touch the invertied index at all. This essentially means right now <nl> + // updates aren't indexed. Instead we should (as outlined in #1221) <nl> + // calculate the delta, then explicitly add/remove where necessary <nl> + <nl> // insert inverted index props <nl> if err := s.extendInvertedIndices(tx, invertProps, docID); err != nil { <nl> return errors.Wrap(err, \"put inverted indices props\") <nl> } <nl> + } <nl> return nil <nl> }); err != nil { <nl> ", "msg": "fix update issues\n* fixes issue where an update would set the docID to 0\n* fixes issue where an update would add a duplicate entry into the\ninverted indices. Note: Currently udpates won't be reflected in the\ninverted indices at all. will address this"}
{"diff_id": 3867, "repo": "semi-technologies/weaviate", "sha": "117814087de86a911cb673148c0ad278665a67f0", "time": "17.08.2020 13:53:23", "diff": "mmm a / usecases/classification/classifier_run_knn.go <nl> ppp b / usecases/classification/classifier_run_knn.go <nl>@@ -26,7 +26,6 @@ func (c *Classifier) classifyItemUsingKNN(item search.Result, itemIndex int, kin <nl> defer cancel() <nl> // K is guaranteed to be set by now, no danger in dereferencing the pointer <nl> - // beforeSearch := time.Now() <nl> res, err := c.vectorRepo.AggregateNeighbors(ctx, item.Vector, <nl> kind, item.ClassName, <nl> params.ClassifyProperties, int(*params.K), filters.trainingSet) <nl> @@ -34,7 +33,6 @@ func (c *Classifier) classifyItemUsingKNN(item search.Result, itemIndex int, kin <nl> if err != nil { <nl> return fmt.Errorf(\"classify %s/%s: %v\", item.ClassName, item.ID, err) <nl> } <nl> - // fmt.Printf(\"time spent searching: %s\\n\", time.Since(beforeSearch)) <nl> var classified []string <nl> @@ -62,14 +60,11 @@ func (c *Classifier) classifyItemUsingKNN(item search.Result, itemIndex int, kin <nl> } <nl> c.extendItemWithObjectMeta(&item, params, classified) <nl> - go func() { <nl> - // beforeStore := time.Now() <nl> + // TODO: send back over channel for batched storage <nl> err = c.store(item) <nl> if err != nil { <nl> fmt.Printf(\"store %s/%s: %v\", item.ClassName, item.ID, err) <nl> } <nl> - // fmt.Printf(\"time spent storing: %s\\n\", time.Since(beforeStore)) <nl> - }() <nl> return nil <nl> } <nl> ", "msg": "remove concurrent storage of kNN results\nThis will slow down classifications considerably, a more permanent\nsolution should follow as part of"}
{"diff_id": 3871, "repo": "semi-technologies/weaviate", "sha": "6752eeaebeb3108a4bd970cae2491e25e4995c13", "time": "03.09.2020 19:58:40", "diff": "mmm a / entities/schema/crossref/crossref_source_test.go <nl> ppp b / entities/schema/crossref/crossref_source_test.go <nl>@@ -51,6 +51,12 @@ func Test_Source_ParsingFromString(t *testing.T) { <nl> t.Run(\"the property name is correct\", func(t *testing.T) { <nl> assert.Equal(t, ref.Property, schema.PropertyName(\"myRefProp\")) <nl> }) <nl> + <nl> + t.Run(\"assembling a new source and comparing if the match\", func(t *testing.T) { <nl> + alt := NewSource(kind.Thing, \"MyClassName\", \"myRefProp\", <nl> + \"c2cd3f91-0160-477e-869a-8da8829e0a4d\") <nl> + assert.Equal(t, ref, alt) <nl> + }) <nl> }) <nl> t.Run(\"from a local action ref that is well-formed\", func(t *testing.T) { <nl> @@ -82,6 +88,12 @@ func Test_Source_ParsingFromString(t *testing.T) { <nl> t.Run(\"the property name is correct\", func(t *testing.T) { <nl> assert.Equal(t, ref.Property, schema.PropertyName(\"myRefProp\")) <nl> }) <nl> + <nl> + t.Run(\"assembling a new source and comparing if the match\", func(t *testing.T) { <nl> + alt := NewSource(kind.Action, \"MyActionClass\", \"myRefProp\", <nl> + \"c2cd3f91-0160-477e-869a-8da8829e0a4d\") <nl> + assert.Equal(t, ref, alt) <nl> + }) <nl> }) <nl> t.Run(\"from a network action ref that is well-formed\", func(t *testing.T) { <nl> ", "msg": "increase test cov for crossref sources"}
{"diff_id": 3889, "repo": "semi-technologies/weaviate", "sha": "5ad9a9af068d4e24f19191952b123ea3a8173ee6", "time": "15.10.2020 14:36:26", "diff": "mmm a / None <nl> ppp b / adapters/repos/db/inverted/row_reader.go <nl>+package inverted <nl> + <nl> +import ( <nl> + \"bytes\" <nl> + \"context\" <nl> + \"fmt\" <nl> + <nl> + \"github.com/boltdb/bolt\" <nl> + \"github.com/semi-technologies/weaviate/adapters/repos/db/notimplemented\" <nl> + \"github.com/semi-technologies/weaviate/entities/filters\" <nl> +) <nl> + <nl> +// RowReader reads one or many row(s) depending on the specified operator <nl> +type RowReader struct { <nl> + // prop []byte <nl> + value []byte <nl> + bucket *bolt.Bucket <nl> + operator filters.Operator <nl> + // hasFrequency bool <nl> +} <nl> + <nl> +func NewRowReader(bucket *bolt.Bucket, value []byte, <nl> + operator filters.Operator) *RowReader { <nl> + return &RowReader{ <nl> + bucket: bucket, <nl> + value: value, <nl> + operator: operator, <nl> + } <nl> +} <nl> + <nl> +// ReadFn will be called 1..n times per match. This means it will also be <nl> +// called on a non-match, in this case v == nil. <nl> +// It is up to the caller to decide if that is an error case or not. <nl> +// <nl> +// Note that because what we are parsing is an inverted index row, it can <nl> +// sometimes become confusing what a key and value actually resembles. The <nl> +// variables k and v are the literal row key and value. So this means, the <nl> +// data-value as in \"less than 17\" where 17 would be the \"value\" is in the key <nl> +// variable \"k\". The value will contain the docCount, hash and list of pointers <nl> +// (with optional frequency) to the docIDs <nl> +type ReadFn func(k, v []byte) error <nl> + <nl> +func (rr *RowReader) Read(ctx context.Context, readFn ReadFn) error { <nl> + switch rr.operator { <nl> + case filters.OperatorEqual: <nl> + return rr.equal(ctx, readFn) <nl> + case filters.OperatorNotEqual: <nl> + return rr.notEqual(ctx, readFn) <nl> + case filters.OperatorGreaterThan: <nl> + return rr.greaterThan(ctx, readFn, false) <nl> + case filters.OperatorGreaterThanEqual: <nl> + return rr.greaterThan(ctx, readFn, true) <nl> + case filters.OperatorLessThan: <nl> + return rr.lessThan(ctx, readFn, false) <nl> + case filters.OperatorLessThanEqual: <nl> + return rr.lessThan(ctx, readFn, true) <nl> + default: <nl> + return fmt.Errorf(\"operator not supported (yet) in standalone \"+ <nl> + \"mode, see %s for details\", notimplemented.Link) <nl> + } <nl> +} <nl> + <nl> +// equal is a special case, as we don't need to iterate, but just read a single <nl> +// row <nl> +func (rr *RowReader) equal(ctx context.Context, readFn ReadFn) error { <nl> + if err := ctx.Err(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + v := rr.bucket.Get(rr.value) <nl> + return readFn(rr.value, v) <nl> +} <nl> + <nl> +// greaterThan reads from the specified value to the end. The first row is only <nl> +// included if allowEqual==true, otherwise it starts with the next one <nl> +func (rr *RowReader) greaterThan(ctx context.Context, readFn ReadFn, <nl> + allowEqual bool) error { <nl> + c := rr.bucket.Cursor() <nl> + <nl> + for k, v := c.Seek(rr.value); k != nil; k, v = c.Next() { <nl> + if err := ctx.Err(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + if bytes.Equal(k, rr.value) && !allowEqual { <nl> + continue <nl> + } <nl> + <nl> + if err := readFn(k, v); err != nil { <nl> + return err <nl> + } <nl> + } <nl> + <nl> + return nil <nl> +} <nl> + <nl> +// lessThan reads from the very begging to the specified value. The last <nl> +// matching row is only included if allowEqual==true, otherwise it ends one <nl> +// prior to that. <nl> +func (rr *RowReader) lessThan(ctx context.Context, readFn ReadFn, <nl> + allowEqual bool) error { <nl> + c := rr.bucket.Cursor() <nl> + <nl> + for k, v := c.First(); k != nil && bytes.Compare(k, rr.value) != 1; k, v = c.Next() { <nl> + if err := ctx.Err(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + if bytes.Equal(k, rr.value) && !allowEqual { <nl> + continue <nl> + } <nl> + <nl> + if err := readFn(k, v); err != nil { <nl> + return err <nl> + } <nl> + } <nl> + <nl> + return nil <nl> +} <nl> + <nl> +// notEqual is another special case, as it's the opposite of equal. So instead <nl> +// of reading just one row, we read all but one row. <nl> +func (rr *RowReader) notEqual(ctx context.Context, readFn ReadFn) error { <nl> + c := rr.bucket.Cursor() <nl> + for k, v := c.First(); k != nil; k, v = c.Next() { <nl> + if err := ctx.Err(); err != nil { <nl> + return err <nl> + } <nl> + <nl> + if bytes.Equal(k, rr.value) { <nl> + continue <nl> + } <nl> + <nl> + if err := readFn(k, v); err != nil { <nl> + return err <nl> + } <nl> + <nl> + } <nl> + <nl> + return nil <nl> +} <nl> ", "msg": "extract existing operator-specific row reading logic\nwithin the \"inverted\" package"}
{"diff_id": 3898, "repo": "semi-technologies/weaviate", "sha": "f4a54e927836ae979d97559f8de269a97a085e51", "time": "17.11.2020 09:01:36", "diff": "mmm a / adapters/repos/db/inverted/objects.go <nl> ppp b / adapters/repos/db/inverted/objects.go <nl>@@ -29,46 +29,40 @@ func (a *Analyzer) Object(input map[string]interface{}, props []*models.Property <nl> propsMap[prop.Name] = prop <nl> } <nl> + properties, err := a.analyzeProps(propsMap, input) <nl> + if err != nil { <nl> + return nil, errors.Wrap(err, \"analyze props\") <nl> + } <nl> + <nl> + property, err := a.analyzeUUIDProp(uuid) <nl> + if err != nil { <nl> + return nil, errors.Wrap(err, \"analyze uuid prop\") <nl> + } <nl> + <nl> + properties = append(properties, *property) <nl> + <nl> + return properties, nil <nl> +} <nl> + <nl> +func (a *Analyzer) analyzeProps(propsMap map[string]*models.Property, <nl> + input map[string]interface{}) ([]Property, error) { <nl> var out []Property <nl> for key, prop := range propsMap { <nl> if len(prop.DataType) < 1 { <nl> return nil, fmt.Errorf(\"prop %q has no datatype\", prop.Name) <nl> } <nl> - var property *Property <nl> - var err error <nl> if schema.IsRefDataType(prop.DataType) { <nl> - value, ok := input[key] <nl> - if !ok { <nl> - // explicitly set zero-value, so we can index for \"ref not set\" <nl> - value = make(models.MultipleRef, 0) <nl> + if err := a.extendPropertiesWithReference(&out, prop, input, key); err != nil { <nl> + return nil, err <nl> } <nl> - property, err = a.analyzeRefProp(prop, value) <nl> } else { <nl> - value, ok := input[key] <nl> - if !ok { <nl> - // skip any primitive prop that's not set <nl> - continue <nl> + if err := a.extendPropertiesWithPrimitive(&out, prop, input, key); err != nil { <nl> + return nil, err <nl> } <nl> - property, err = a.analyzePrimitiveProp(prop, value) <nl> - } <nl> - if err != nil { <nl> - return nil, errors.Wrap(err, \"analyze primitive prop\") <nl> - } <nl> - if property == nil { <nl> - continue <nl> - } <nl> - <nl> - out = append(out, *property) <nl> } <nl> - property, err := a.analyzeUUIDProp(uuid) <nl> - if err != nil { <nl> - return nil, errors.Wrap(err, \"analyze uuid prop\") <nl> } <nl> - <nl> - out = append(out, *property) <nl> - <nl> return out, nil <nl> } <nl> @@ -88,6 +82,30 @@ func (a *Analyzer) analyzeUUIDProp(uuid strfmt.UUID) (*Property, error) { <nl> }, nil <nl> } <nl> +// extendPropertiesWithPrimitive mutates the passed in properties, by extending <nl> +// it with an additional property - if applicable <nl> +func (a *Analyzer) extendPropertiesWithPrimitive(properties *[]Property, <nl> + prop *models.Property, input map[string]interface{}, propName string) error { <nl> + var property *Property <nl> + var err error <nl> + <nl> + value, ok := input[propName] <nl> + if !ok { <nl> + // skip any primitive prop that's not set <nl> + return nil <nl> + } <nl> + property, err = a.analyzePrimitiveProp(prop, value) <nl> + if err != nil { <nl> + errors.Wrap(err, \"analyze primitive prop\") <nl> + } <nl> + if property == nil { <nl> + return nil <nl> + } <nl> + <nl> + *properties = append(*properties, *property) <nl> + return nil <nl> +} <nl> + <nl> func (a *Analyzer) analyzePrimitiveProp(prop *models.Property, value interface{}) (*Property, error) { <nl> var hasFrequency bool <nl> var items []Countable <nl> @@ -173,7 +191,28 @@ func (a *Analyzer) analyzePrimitiveProp(prop *models.Property, value interface{} <nl> }, nil <nl> } <nl> -func (a *Analyzer) analyzeRefProp(prop *models.Property, value interface{}) (*Property, error) { <nl> +// extendPropertiesWithReference extends the specified properties arrays with <nl> +// either 1 or 2 entries: If the ref is not set, only the ref-count property <nl> +// will be added. If the ref is set the ref-prop itself will also be added and <nl> +// contain all references as values <nl> +func (a *Analyzer) extendPropertiesWithReference(properties *[]Property, <nl> + prop *models.Property, input map[string]interface{}, propName string) error { <nl> + value, ok := input[propName] <nl> + if !ok { <nl> + // explicitly set zero-value, so we can index for \"ref not set\" <nl> + value = make(models.MultipleRef, 0) <nl> + } <nl> + <nl> + property, err := a.analyzeRefPropCount(prop, value) <nl> + if err != nil { <nl> + return errors.Wrap(err, \"ref count\") <nl> + } <nl> + <nl> + *properties = append(*properties, *property) <nl> + return nil <nl> +} <nl> + <nl> +func (a *Analyzer) analyzeRefPropCount(prop *models.Property, value interface{}) (*Property, error) { <nl> // TODO: return multiple properties when support ref-indexing. For now we <nl> // only support counting the refs <nl> ", "msg": "(refactor) split up inverted.Analyzer.Objects()\nIt had grown quite large and it wasn't flexible enough to support the\nfact that ref props will set a variable amount of properties (only the\ncount if no ref is set vs. both the count and the value if refs are\nset)."}
{"diff_id": 3904, "repo": "semi-technologies/weaviate", "sha": "14d5470568bbfd78e10b8457756da9584337602b", "time": "04.12.2020 11:49:40", "diff": "mmm a / adapters/repos/db/migrator.go <nl> ppp b / adapters/repos/db/migrator.go <nl>@@ -13,7 +13,6 @@ package db <nl> import ( <nl> \"context\" <nl> - \"fmt\" <nl> \"github.com/pkg/errors\" <nl> \"github.com/semi-technologies/weaviate/entities/models\" <nl> @@ -67,29 +66,41 @@ func (m *Migrator) DropClass(ctx context.Context, kind kind.Kind, className stri <nl> } <nl> func (m *Migrator) UpdateClass(ctx context.Context, kind kind.Kind, className string, newClassName *string, newKeywords *models.Keywords) error { <nl> - return fmt.Errorf(\"updating a class not (yet) supported\") <nl> + if newClassName != nil { <nl> + return errors.New(\"weaviate does not support renaming of classes\") <nl> + } <nl> + <nl> + return nil <nl> } <nl> func (m *Migrator) AddProperty(ctx context.Context, kind kind.Kind, className string, prop *models.Property) error { <nl> idx := m.db.GetIndex(kind, schema.ClassName(className)) <nl> if idx == nil { <nl> - return fmt.Errorf(\"cannot add property to a non-existing index for %s/%s\", <nl> + return errors.Errorf(\"cannot add property to a non-existing index for %s/%s\", <nl> kind.Name(), className) <nl> } <nl> return idx.addProperty(ctx, prop) <nl> } <nl> +// DropProperty is ignored, API compliant change <nl> func (m *Migrator) DropProperty(ctx context.Context, kind kind.Kind, className string, propertyName string) error { <nl> - return fmt.Errorf(\"dropping a property not (yet) supported\") <nl> + // ignore but don't error <nl> + return nil <nl> } <nl> func (m *Migrator) UpdateProperty(ctx context.Context, kind kind.Kind, className string, propName string, newName *string, newKeywords *models.Keywords) error { <nl> - return fmt.Errorf(\"changing a property not (yet) supported\") <nl> + if newName != nil { <nl> + return errors.New(\"weaviate does not support renaming of properties\") <nl> } <nl> + return nil <nl> +} <nl> + <nl> +// UpdatePropertyAddDataType is ignored, API compliant change <nl> func (m *Migrator) UpdatePropertyAddDataType(ctx context.Context, kind kind.Kind, className string, propName string, newDataType string) error { <nl> - return fmt.Errorf(\"changing a property not (yet) supported\") <nl> + // ignore but don't error <nl> + return nil <nl> } <nl> func NewMigrator(db *DB, logger logrus.FieldLogger) *Migrator { <nl> ", "msg": "Investigate which other (if any) db.Migrator methods need to be implemented"}
{"diff_id": 3907, "repo": "semi-technologies/weaviate", "sha": "11ae68c74c86a1692cf02da8ad3a5b1074518147", "time": "10.12.2020 16:34:03", "diff": "mmm a / modules/text2vec-contextionary/extensions/usecase.go <nl> ppp b / modules/text2vec-contextionary/extensions/usecase.go <nl>@@ -57,13 +57,13 @@ func (uc *UseCase) LoadAll() ([]byte, error) { <nl> _, err = buf.Write([]byte(\"\\n\")) <nl> if err != nil { <nl> - return false, errors.Wrapf(err, \"write newline separator\") <nl> + return false, errors.Wrap(err, \"write newline separator\") <nl> } <nl> return true, nil <nl> }) <nl> if err != nil { <nl> - return nil, errors.Wrapf(err, \"load all concepts\") <nl> + return nil, errors.Wrap(err, \"load all concepts\") <nl> } <nl> return buf.Bytes(), nil <nl> ", "msg": "use Wrap instead of Wrapf where no formatting is required"}
{"diff_id": 3913, "repo": "semi-technologies/weaviate", "sha": "60e5a02e4e74f155bb6377779bacc03305d908a6", "time": "22.12.2020 11:47:23", "diff": "mmm a / adapters/repos/db/vector/hnsw/search.go <nl> ppp b / adapters/repos/db/vector/hnsw/search.go <nl>@@ -321,10 +321,17 @@ func (h *hnsw) knnSearchByVector(searchVec []float32, k int, <nl> if err != nil { <nl> return nil, errors.Wrapf(err, \"knn search: search layer at level %d\", level) <nl> } <nl> + <nl> + // There might be situations where we did not find a better entrypoint at <nl> + // that particular level, so instead we're keeping whatever entrypoint we <nl> + // had before (i.e. either from a previous level or even the main <nl> + // entrypoint) <nl> + if res.root != nil { <nl> best := res.minimum() <nl> entryPointID = best.index <nl> entryPointDistance = best.dist <nl> } <nl> + } <nl> eps := &binarySearchTreeGeneric{} <nl> eps.insert(entryPointID, entryPointDistance) <nl> ", "msg": "add entrypoint nil-check to hnsw\nThis might otherwise lead to panics"}
{"diff_id": 3914, "repo": "semi-technologies/weaviate", "sha": "d6ee5f56f78bbc45772b5c4873608279b294ee07", "time": "22.12.2020 13:33:54", "diff": "mmm a / adapters/repos/db/inverted/searcher.go <nl> ppp b / adapters/repos/db/inverted/searcher.go <nl>@@ -76,6 +76,11 @@ func (f *Searcher) Object(ctx context.Context, limit int, <nl> return errors.Wrap(err, \"merge doc ids by operator\") <nl> } <nl> + // cutoff if required, e.g. after merging unlimted filters <nl> + if len(pointers.docIDs) > limit { <nl> + pointers.docIDs = pointers.docIDs[:limit] <nl> + } <nl> + <nl> res, err := docid.ObjectsInTx(tx, pointers.IDs()) <nl> if err != nil { <nl> return errors.Wrap(err, \"resolve doc ids to objects\") <nl> ", "msg": "provide missing test and fix ignored limit on chained filters"}
{"diff_id": 3922, "repo": "semi-technologies/weaviate", "sha": "be3d52a188552313e07f0ce13f72ca6953df2b19", "time": "09.01.2021 18:32:35", "diff": "mmm a / test/acceptance/batch_request_endpoints/batch_journey_test.go <nl> ppp b / test/acceptance/batch_request_endpoints/batch_journey_test.go <nl>@@ -105,3 +105,87 @@ func batchJourney(t *testing.T) { <nl> func mustNewUUID() strfmt.UUID { <nl> return strfmt.UUID(uuid.New().String()) <nl> } <nl> + <nl> +func Test_BugFlakyResultCountWithVectorSearch(t *testing.T) { <nl> + className := \"FlakyBugTestClass\" <nl> + <nl> + // since this bug occurs only in around 1 in 25 cases, we run the test <nl> + // multiple times to increase the chance we're running into it <nl> + amount := 100 <nl> + for i := 0; i < amount; i++ { <nl> + t.Run(\"create schema\", func(t *testing.T) { <nl> + createObjectClass(t, &models.Class{ <nl> + Class: className, <nl> + Properties: []*models.Property{ <nl> + &models.Property{ <nl> + Name: \"title\", <nl> + DataType: []string{\"string\"}, <nl> + }, <nl> + &models.Property{ <nl> + Name: \"url\", <nl> + DataType: []string{\"string\"}, <nl> + }, <nl> + &models.Property{ <nl> + Name: \"wordCount\", <nl> + DataType: []string{\"int\"}, <nl> + }, <nl> + }, <nl> + }) <nl> + }) <nl> + <nl> + t.Run(\"create and import some data\", func(t *testing.T) { <nl> + objects := []*models.Object{ <nl> + &models.Object{ <nl> + Class: className, <nl> + Properties: map[string]interface{}{ <nl> + \"title\": \"article 1\", <nl> + \"url\": \"http://articles.local/my-article-1\", <nl> + \"wordCount\": 60, <nl> + }, <nl> + }, <nl> + &models.Object{ <nl> + Class: className, <nl> + Properties: map[string]interface{}{ <nl> + \"title\": \"article 2\", <nl> + \"url\": \"http://articles.local/my-article-2\", <nl> + \"wordCount\": 40, <nl> + }, <nl> + }, <nl> + &models.Object{ <nl> + Class: className, <nl> + Properties: map[string]interface{}{ <nl> + \"title\": \"article 3\", <nl> + \"url\": \"http://articles.local/my-article-3\", <nl> + \"wordCount\": 600, <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + params := batch.NewBatchObjectsCreateParams().WithBody( <nl> + batch.BatchObjectsCreateBody{ <nl> + Objects: objects, <nl> + }, <nl> + ) <nl> + res, err := helper.Client(t).Batch.BatchObjectsCreate(params, nil) <nl> + require.Nil(t, err) <nl> + <nl> + for _, elem := range res.Payload { <nl> + assert.Nil(t, elem.Result.Errors) <nl> + } <nl> + }) <nl> + <nl> + t.Run(\"verify using GraphQL\", func(t *testing.T) { <nl> + result := AssertGraphQL(t, helper.RootAuth, fmt.Sprintf(` <nl> + { Get { %s(nearText: {concepts: [\"news\"]}, limit:7) { <nl> + wordCount title url <nl> + } } } <nl> + `, className)) <nl> + items := result.Get(\"Get\", className).AsSlice() <nl> + assert.Len(t, items, 3) <nl> + }) <nl> + <nl> + t.Run(\"cleanup\", func(t *testing.T) { <nl> + deleteObjectClass(t, className) <nl> + }) <nl> + } <nl> +} <nl> ", "msg": "add (failing) journey test to reproduce bug"}
{"diff_id": 3947, "repo": "semi-technologies/weaviate", "sha": "92cecf7b0abbbcb7379438205d2e2151f439447c", "time": "26.04.2021 11:11:20", "diff": "mmm a / adapters/repos/db/vector/hnsw/search_with_max_dist.go <nl> ppp b / adapters/repos/db/vector/hnsw/search_with_max_dist.go <nl>@@ -42,7 +42,7 @@ func (h *hnsw) KnnSearchByVectorMaxDist(searchVec []float32, dist float32, <nl> return nil, errors.Wrapf(err, \"knn search: search layer at level %d\", level) <nl> } <nl> if res.Len() > 0 { <nl> - best := eps.Pop() <nl> + best := res.Pop() <nl> entryPointID = best.ID <nl> entryPointDistance = best.Dist <nl> } <nl> ", "msg": "fix broken geo distance search"}
{"diff_id": 3951, "repo": "semi-technologies/weaviate", "sha": "fe336380ac36bf2be2202fc1ba98d4c725afc2a6", "time": "04.05.2021 12:05:33", "diff": "mmm a / adapters/repos/db/lsmkv/strategies_replace_integration_test.go <nl> ppp b / adapters/repos/db/lsmkv/strategies_replace_integration_test.go <nl>@@ -443,6 +443,7 @@ func TestReplaceStrategy_InsertAndDelete(t *testing.T) { <nl> } <nl> func TestReplaceStrategy_Cursors(t *testing.T) { <nl> + t.Run(\"memtable-only\", func(t *testing.T) { <nl> rand.Seed(time.Now().UnixNano()) <nl> dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> os.MkdirAll(dirName, 0o777) <nl> @@ -451,7 +452,6 @@ func TestReplaceStrategy_Cursors(t *testing.T) { <nl> fmt.Println(err) <nl> }() <nl> - t.Run(\"memtable-only\", func(t *testing.T) { <nl> b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> require.Nil(t, err) <nl> @@ -565,6 +565,14 @@ func TestReplaceStrategy_Cursors(t *testing.T) { <nl> }) <nl> t.Run(\"with a single flush\", func(t *testing.T) { <nl> + rand.Seed(time.Now().UnixNano()) <nl> + dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> + os.MkdirAll(dirName, 0o777) <nl> + defer func() { <nl> + err := os.RemoveAll(dirName) <nl> + fmt.Println(err) <nl> + }() <nl> + <nl> b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> require.Nil(t, err) <nl> @@ -650,163 +658,167 @@ func TestReplaceStrategy_Cursors(t *testing.T) { <nl> assert.Equal(t, expectedValues, retrievedValues) <nl> }) <nl> - // t.Run(\"replace a key\", func(t *testing.T) { <nl> - // key := []byte(\"key-002\") <nl> - // value := []byte(\"value-002-updated\") <nl> + // TODO: update <nl> - // err = b.Put(key, value) <nl> - // require.Nil(t, err) <nl> + // TODO: delete <nl> + }) <nl> - // expectedKeys := [][]byte{ <nl> - // []byte(\"key-001\"), <nl> - // []byte(\"key-002\"), <nl> - // } <nl> - // expectedValues := [][]byte{ <nl> - // []byte(\"value-001\"), <nl> - // []byte(\"value-002-updated\"), <nl> - // } <nl> - <nl> - // var retrievedKeys [][]byte <nl> - // var retrievedValues [][]byte <nl> - // c := b.Cursor() <nl> - // retrieved := 0 <nl> - // for k, v := c.Seek([]byte(\"key-001\")); k != nil && retrieved < 2; k, v = c.Next() { <nl> - // retrieved++ <nl> - // retrievedKeys = append(retrievedKeys, k) <nl> - // retrievedValues = append(retrievedValues, v) <nl> - // } <nl> - <nl> - // assert.Equal(t, expectedKeys, retrievedKeys) <nl> - // assert.Equal(t, expectedValues, retrievedValues) <nl> - // }) <nl> + t.Run(\"mixing several disk segments and memtable\", func(t *testing.T) { <nl> + rand.Seed(time.Now().UnixNano()) <nl> + dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> + os.MkdirAll(dirName, 0o777) <nl> + defer func() { <nl> + err := os.RemoveAll(dirName) <nl> + fmt.Println(err) <nl> + }() <nl> + <nl> + b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> + require.Nil(t, err) <nl> + <nl> + // so big it effectively never triggers as part of this test <nl> + b.SetMemtableThreshold(1e9) <nl> + <nl> + t.Run(\"first third (%3==0)\", func(t *testing.T) { <nl> + pairs := 20 <nl> + var keys [][]byte <nl> + var values [][]byte <nl> + <nl> + for i := 0; i < pairs; i++ { <nl> + if i%3 == 0 { <nl> + keys = append(keys, []byte(fmt.Sprintf(\"key-%03d\", i))) <nl> + values = append(values, []byte(fmt.Sprintf(\"value-%03d\", i))) <nl> + } <nl> + } <nl> + <nl> + // shuffle to make sure the BST isn't accidentally in order <nl> + rand.Seed(time.Now().UnixNano()) <nl> + rand.Shuffle(len(keys), func(i, j int) { <nl> + keys[i], keys[j] = keys[j], keys[i] <nl> + values[i], values[j] = values[j], values[i] <nl> }) <nl> - // t.Run(\"with single flush in between updates\", func(t *testing.T) { <nl> - // b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> - // require.Nil(t, err) <nl> + for i := range keys { <nl> + err = b.Put(keys[i], values[i]) <nl> + require.Nil(t, err) <nl> + } <nl> + }) <nl> - // // so big it effectively never triggers as part of this test <nl> - // b.SetMemtableThreshold(1e9) <nl> + t.Run(\"flush to disk\", func(t *testing.T) { <nl> + require.Nil(t, b.FlushAndSwitch()) <nl> + }) <nl> - // t.Run(\"set original values and verify\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // orig2 := []byte(\"original value for key2\") <nl> - // orig3 := []byte(\"original value for key3\") <nl> + t.Run(\"second third (%3==1)\", func(t *testing.T) { <nl> + pairs := 20 <nl> + var keys [][]byte <nl> + var values [][]byte <nl> - // err = b.Put(key1, orig1) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key2, orig2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, orig3) <nl> - // require.Nil(t, err) <nl> + for i := 0; i < pairs; i++ { <nl> + if i%3 == 1 { <nl> + keys = append(keys, []byte(fmt.Sprintf(\"key-%03d\", i))) <nl> + values = append(values, []byte(fmt.Sprintf(\"value-%03d\", i))) <nl> + } <nl> + } <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig3) <nl> - // }) <nl> + // shuffle to make sure the BST isn't accidentally in order <nl> + rand.Seed(time.Now().UnixNano()) <nl> + rand.Shuffle(len(keys), func(i, j int) { <nl> + keys[i], keys[j] = keys[j], keys[i] <nl> + values[i], values[j] = values[j], values[i] <nl> + }) <nl> - // t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> - // require.Nil(t, b.FlushAndSwitch()) <nl> - // }) <nl> + for i := range keys { <nl> + err = b.Put(keys[i], values[i]) <nl> + require.Nil(t, err) <nl> + } <nl> + }) <nl> - // t.Run(\"replace some, keep one\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // replaced2 := []byte(\"updated value for key2\") <nl> - // replaced3 := []byte(\"updated value for key3\") <nl> + t.Run(\"flush to disk\", func(t *testing.T) { <nl> + require.Nil(t, b.FlushAndSwitch()) <nl> + }) <nl> - // err = b.Put(key2, replaced2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, replaced3) <nl> - // require.Nil(t, err) <nl> + t.Run(\"third third (%3==2) memtable only\", func(t *testing.T) { <nl> + pairs := 20 <nl> + var keys [][]byte <nl> + var values [][]byte <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, orig1, res) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, replaced2, res) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, replaced3, res) <nl> - // }) <nl> - // }) <nl> + for i := 0; i < pairs; i++ { <nl> + if i%3 == 2 { <nl> + keys = append(keys, []byte(fmt.Sprintf(\"key-%03d\", i))) <nl> + values = append(values, []byte(fmt.Sprintf(\"value-%03d\", i))) <nl> + } <nl> + } <nl> - // t.Run(\"with a flush after the initial write and after the update\", func(t *testing.T) { <nl> - // b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> - // require.Nil(t, err) <nl> + // shuffle to make sure the BST isn't accidentally in order <nl> + rand.Seed(time.Now().UnixNano()) <nl> + rand.Shuffle(len(keys), func(i, j int) { <nl> + keys[i], keys[j] = keys[j], keys[i] <nl> + values[i], values[j] = values[j], values[i] <nl> + }) <nl> - // // so big it effectively never triggers as part of this test <nl> - // b.SetMemtableThreshold(1e9) <nl> + for i := range keys { <nl> + err = b.Put(keys[i], values[i]) <nl> + require.Nil(t, err) <nl> + } <nl> - // t.Run(\"set original values and verify\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // orig2 := []byte(\"original value for key2\") <nl> - // orig3 := []byte(\"original value for key3\") <nl> + // no flush for this one, so this segment stays in the memtable <nl> + }) <nl> - // err = b.Put(key1, orig1) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key2, orig2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, orig3) <nl> - // require.Nil(t, err) <nl> + t.Run(\"seek from somewhere in the middle\", func(t *testing.T) { <nl> + expectedKeys := [][]byte{ <nl> + []byte(\"key-016\"), <nl> + []byte(\"key-017\"), <nl> + []byte(\"key-018\"), <nl> + []byte(\"key-019\"), <nl> + } <nl> + expectedValues := [][]byte{ <nl> + []byte(\"value-016\"), <nl> + []byte(\"value-017\"), <nl> + []byte(\"value-018\"), <nl> + []byte(\"value-019\"), <nl> + } <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig3) <nl> - // }) <nl> + var retrievedKeys [][]byte <nl> + var retrievedValues [][]byte <nl> + c := b.Cursor() <nl> + for k, v := c.Seek([]byte(\"key-016\")); k != nil; k, v = c.Next() { <nl> + retrievedKeys = append(retrievedKeys, k) <nl> + retrievedValues = append(retrievedValues, v) <nl> + } <nl> - // t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> - // require.Nil(t, b.FlushAndSwitch()) <nl> - // }) <nl> + assert.Equal(t, expectedKeys, retrievedKeys) <nl> + assert.Equal(t, expectedValues, retrievedValues) <nl> + }) <nl> - // t.Run(\"replace some, keep one\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // replaced2 := []byte(\"updated value for key2\") <nl> - // replaced3 := []byte(\"updated value for key3\") <nl> + t.Run(\"start from the beginning\", func(t *testing.T) { <nl> + expectedKeys := [][]byte{ <nl> + []byte(\"key-000\"), <nl> + []byte(\"key-001\"), <nl> + []byte(\"key-002\"), <nl> + } <nl> + expectedValues := [][]byte{ <nl> + []byte(\"value-000\"), <nl> + []byte(\"value-001\"), <nl> + []byte(\"value-002\"), <nl> + } <nl> - // err = b.Put(key2, replaced2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, replaced3) <nl> - // require.Nil(t, err) <nl> + var retrievedKeys [][]byte <nl> + var retrievedValues [][]byte <nl> + c := b.Cursor() <nl> + retrieved := 0 <nl> + for k, v := c.First(); k != nil && retrieved < 3; k, v = c.Next() { <nl> + retrieved++ <nl> + retrievedKeys = append(retrievedKeys, k) <nl> + retrievedValues = append(retrievedValues, v) <nl> + } <nl> - // // Flush before verifying! <nl> - // require.Nil(t, b.FlushAndSwitch()) <nl> + assert.Equal(t, expectedKeys, retrievedKeys) <nl> + assert.Equal(t, expectedValues, retrievedValues) <nl> + }) <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced3) <nl> - // }) <nl> - // }) <nl> + // TODO: update <nl> + <nl> + // TODO: delete <nl> + }) <nl> // t.Run(\"update in memtable, then do an orderly shutdown, and re-init\", func(t *testing.T) { <nl> // b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> ", "msg": "add more tests for mixed disk/mem range queries"}
{"diff_id": 3956, "repo": "semi-technologies/weaviate", "sha": "860fdefb45e54c4f20290db59cca6129c4fe0fa3", "time": "26.05.2021 14:53:58", "diff": "mmm a / adapters/repos/db/vector/hnsw/commit_logger.go <nl> ppp b / adapters/repos/db/vector/hnsw/commit_logger.go <nl>package hnsw <nl> import ( <nl> - \"bufio\" <nl> \"bytes\" <nl> \"encoding/binary\" <nl> \"fmt\" <nl> @@ -58,7 +57,6 @@ func NewCommitLogger(rootPath, name string, <nl> return nil, err <nl> } <nl> l.logFile = fd <nl> - l.logFileWriter = bufio.NewWriterSize(fd, 1*1024*1024) <nl> l.StartLogging() <nl> return l, nil <nl> @@ -183,7 +181,6 @@ type hnswCommitLogger struct { <nl> sync.Mutex <nl> cancel chan struct{} <nl> logFile *os.File <nl> - logFileWriter *bufio.Writer <nl> rootPath string <nl> id string <nl> condensor condensor <nl> @@ -212,9 +209,9 @@ func (l *hnswCommitLogger) AddNode(node *vertex) error { <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, AddNode)) <nl> - ec.add(l.writeUint64(l.logFileWriter, node.id)) <nl> - ec.add(l.writeUint16(l.logFileWriter, uint16(node.level))) <nl> + ec.add(l.writeCommitType(l.logFile, AddNode)) <nl> + ec.add(l.writeUint64(l.logFile, node.id)) <nl> + ec.add(l.writeUint16(l.logFile, uint16(node.level))) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, \"write node %d to commit log\", node.id) <nl> @@ -228,9 +225,9 @@ func (l *hnswCommitLogger) SetEntryPointWithMaxLayer(id uint64, level int) error <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, SetEntryPointMaxLevel)) <nl> - ec.add(l.writeUint64(l.logFileWriter, id)) <nl> - ec.add(l.writeUint16(l.logFileWriter, uint16(level))) <nl> + ec.add(l.writeCommitType(l.logFile, SetEntryPointMaxLevel)) <nl> + ec.add(l.writeUint64(l.logFile, id)) <nl> + ec.add(l.writeUint16(l.logFile, uint16(level))) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, \"write entrypoint %d (%d) to commit log\", id, level) <nl> @@ -244,9 +241,9 @@ func (l *hnswCommitLogger) ReplaceLinksAtLevel(nodeid uint64, level int, targets <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, ReplaceLinksAtLevel)) <nl> - ec.add(l.writeUint64(l.logFileWriter, nodeid)) <nl> - ec.add(l.writeUint16(l.logFileWriter, uint16(level))) <nl> + ec.add(l.writeCommitType(l.logFile, ReplaceLinksAtLevel)) <nl> + ec.add(l.writeUint64(l.logFile, nodeid)) <nl> + ec.add(l.writeUint16(l.logFile, uint16(level))) <nl> targetLength := len(targets) <nl> if targetLength > math.MaxUint16 { <nl> // TODO: investigate why we get such massive connections <nl> @@ -257,8 +254,8 @@ func (l *hnswCommitLogger) ReplaceLinksAtLevel(nodeid uint64, level int, targets <nl> WithField(\"maximum_length\", targetLength). <nl> Warning(\"condensor length of connections would overflow uint16, cutting off\") <nl> } <nl> - ec.add(l.writeUint16(l.logFileWriter, uint16(targetLength))) <nl> - ec.add(l.writeUint64Slice(l.logFileWriter, targets[:targetLength])) <nl> + ec.add(l.writeUint16(l.logFile, uint16(targetLength))) <nl> + ec.add(l.writeUint64Slice(l.logFile, targets[:targetLength])) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, <nl> @@ -274,8 +271,8 @@ func (l *hnswCommitLogger) AddTombstone(nodeid uint64) error { <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, AddTombstone)) <nl> - ec.add(l.writeUint64(l.logFileWriter, nodeid)) <nl> + ec.add(l.writeCommitType(l.logFile, AddTombstone)) <nl> + ec.add(l.writeUint64(l.logFile, nodeid)) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, <nl> @@ -290,8 +287,8 @@ func (l *hnswCommitLogger) RemoveTombstone(nodeid uint64) error { <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, RemoveTombstone)) <nl> - ec.add(l.writeUint64(l.logFileWriter, nodeid)) <nl> + ec.add(l.writeCommitType(l.logFile, RemoveTombstone)) <nl> + ec.add(l.writeUint64(l.logFile, nodeid)) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, <nl> @@ -306,8 +303,8 @@ func (l *hnswCommitLogger) ClearLinks(nodeid uint64) error { <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, ClearLinks)) <nl> - ec.add(l.writeUint64(l.logFileWriter, nodeid)) <nl> + ec.add(l.writeCommitType(l.logFile, ClearLinks)) <nl> + ec.add(l.writeUint64(l.logFile, nodeid)) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, <nl> \"write clear links of node %d to commit log\", nodeid) <nl> @@ -321,8 +318,8 @@ func (l *hnswCommitLogger) DeleteNode(nodeid uint64) error { <nl> defer l.Unlock() <nl> ec := &errorCompounder{} <nl> - ec.add(l.writeCommitType(l.logFileWriter, DeleteNode)) <nl> - ec.add(l.writeUint64(l.logFileWriter, nodeid)) <nl> + ec.add(l.writeCommitType(l.logFile, DeleteNode)) <nl> + ec.add(l.writeUint64(l.logFile, nodeid)) <nl> if err := ec.toError(); err != nil { <nl> return errors.Wrapf(err, <nl> \"write delete node %d to commit log\", nodeid) <nl> @@ -335,7 +332,7 @@ func (l *hnswCommitLogger) Reset() error { <nl> l.Lock() <nl> defer l.Unlock() <nl> - err := l.writeCommitType(l.logFileWriter, ResetIndex) <nl> + err := l.writeCommitType(l.logFile, ResetIndex) <nl> if err != nil { <nl> return errors.Wrap(err, \"reset to commit log\") <nl> } <nl> @@ -428,9 +425,9 @@ func (l *hnswCommitLogger) maintenance() error { <nl> } <nl> if i.Size() > l.maxSize { <nl> - if err := l.logFileWriter.Flush(); err != nil { <nl> - return err <nl> - } <nl> + // if err := l.logFile.Flush(); err != nil { <nl> + // return err <nl> + // } <nl> if err := l.logFile.Close(); err != nil { <nl> return err <nl> @@ -453,7 +450,6 @@ func (l *hnswCommitLogger) maintenance() error { <nl> } <nl> l.logFile = fd <nl> - l.logFileWriter = bufio.NewWriter(fd) <nl> } <nl> return nil <nl> ", "msg": "[skip ci] remove buffered file in hnsw commit logger again\nIt seems the benefits are limited at the moment and it causes other\nissues. Rather than investigating those issues, I'm removing this and\ncalling it premature optimization. Might return to this when needed in\nthe future."}
{"diff_id": 3963, "repo": "semi-technologies/weaviate", "sha": "dc14c61a18412b5d4a6d974eb62826b49559c320", "time": "14.06.2021 17:21:51", "diff": "mmm a / adapters/repos/db/lsmkv/compaction_integration_test.go <nl> ppp b / adapters/repos/db/lsmkv/compaction_integration_test.go <nl>@@ -215,6 +215,194 @@ func Test_CompactionReplaceStrategy(t *testing.T) { <nl> }) <nl> } <nl> +func Test_CompactionReplaceStrategy_RemoveUnnecessaryDeletes(t *testing.T) { <nl> + // in this test each segment reverses the action of the previous segment so <nl> + // that in the end a lot of information is present in the indivudal segments <nl> + // which is no longer needed. We then verify that after all compaction this <nl> + // information is gone, thus freeing up disk space <nl> + size := 100 <nl> + <nl> + type kv struct { <nl> + key []byte <nl> + value []byte <nl> + } <nl> + <nl> + key := []byte(\"my-key\") <nl> + <nl> + var bucket *Bucket <nl> + dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> + os.MkdirAll(dirName, 0o777) <nl> + defer func() { <nl> + err := os.RemoveAll(dirName) <nl> + fmt.Println(err) <nl> + }() <nl> + <nl> + t.Run(\"init bucket\", func(t *testing.T) { <nl> + b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> + require.Nil(t, err) <nl> + <nl> + // so big it effectively never triggers as part of this test <nl> + b.SetMemtableThreshold(1e9) <nl> + <nl> + bucket = b <nl> + }) <nl> + <nl> + t.Run(\"write segments\", func(t *testing.T) { <nl> + for i := 0; i < size; i++ { <nl> + if i != 0 { <nl> + // we can only update an existing value if this isn't the first write <nl> + err := bucket.Delete(key) <nl> + require.Nil(t, err) <nl> + } <nl> + <nl> + err := bucket.Put(key, []byte(fmt.Sprintf(\"set in round %d\", i))) <nl> + require.Nil(t, err) <nl> + <nl> + require.Nil(t, bucket.FlushAndSwitch()) <nl> + } <nl> + }) <nl> + <nl> + expected := []kv{ <nl> + { <nl> + key: key, <nl> + value: []byte(fmt.Sprintf(\"set in round %d\", size-1)), <nl> + }, <nl> + } <nl> + <nl> + t.Run(\"verify control before compaction\", func(t *testing.T) { <nl> + var retrieved []kv <nl> + <nl> + c := bucket.Cursor() <nl> + defer c.Close() <nl> + <nl> + for k, v := c.First(); k != nil; k, v = c.Next() { <nl> + retrieved = append(retrieved, kv{ <nl> + key: k, <nl> + value: v, <nl> + }) <nl> + } <nl> + <nl> + assert.Equal(t, expected, retrieved) <nl> + }) <nl> + <nl> + t.Run(\"check if eligble for compaction\", func(t *testing.T) { <nl> + assert.True(t, bucket.disk.eligbleForCompaction(), \"check eligle before\") <nl> + }) <nl> + <nl> + t.Run(\"compact until no longer eligble\", func(t *testing.T) { <nl> + for bucket.disk.eligbleForCompaction() { <nl> + require.Nil(t, bucket.disk.compactOnce()) <nl> + } <nl> + }) <nl> + <nl> + t.Run(\"verify control before compaction\", func(t *testing.T) { <nl> + var retrieved []kv <nl> + <nl> + c := bucket.Cursor() <nl> + defer c.Close() <nl> + <nl> + for k, v := c.First(); k != nil; k, v = c.Next() { <nl> + retrieved = append(retrieved, kv{ <nl> + key: k, <nl> + value: v, <nl> + }) <nl> + } <nl> + <nl> + assert.Equal(t, expected, retrieved) <nl> + }) <nl> +} <nl> + <nl> +func Test_CompactionReplaceStrategy_RemoveUnnecessaryUpdates(t *testing.T) { <nl> + // in this test each segment reverses the action of the previous segment so <nl> + // that in the end a lot of information is present in the indivudal segments <nl> + // which is no longer needed. We then verify that after all compaction this <nl> + // information is gone, thus freeing up disk space <nl> + size := 100 <nl> + <nl> + type kv struct { <nl> + key []byte <nl> + value []byte <nl> + } <nl> + <nl> + key := []byte(\"my-key\") <nl> + <nl> + var bucket *Bucket <nl> + dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> + os.MkdirAll(dirName, 0o777) <nl> + defer func() { <nl> + err := os.RemoveAll(dirName) <nl> + fmt.Println(err) <nl> + }() <nl> + <nl> + t.Run(\"init bucket\", func(t *testing.T) { <nl> + b, err := NewBucketWithStrategy(dirName, StrategyReplace) <nl> + require.Nil(t, err) <nl> + <nl> + // so big it effectively never triggers as part of this test <nl> + b.SetMemtableThreshold(1e9) <nl> + <nl> + bucket = b <nl> + }) <nl> + <nl> + t.Run(\"write segments\", func(t *testing.T) { <nl> + for i := 0; i < size; i++ { <nl> + err := bucket.Put(key, []byte(fmt.Sprintf(\"set in round %d\", i))) <nl> + require.Nil(t, err) <nl> + <nl> + require.Nil(t, bucket.FlushAndSwitch()) <nl> + } <nl> + }) <nl> + <nl> + expected := []kv{ <nl> + { <nl> + key: key, <nl> + value: []byte(fmt.Sprintf(\"set in round %d\", size-1)), <nl> + }, <nl> + } <nl> + <nl> + t.Run(\"verify control before compaction\", func(t *testing.T) { <nl> + var retrieved []kv <nl> + <nl> + c := bucket.Cursor() <nl> + defer c.Close() <nl> + <nl> + for k, v := c.First(); k != nil; k, v = c.Next() { <nl> + retrieved = append(retrieved, kv{ <nl> + key: k, <nl> + value: v, <nl> + }) <nl> + } <nl> + <nl> + assert.Equal(t, expected, retrieved) <nl> + }) <nl> + <nl> + t.Run(\"check if eligble for compaction\", func(t *testing.T) { <nl> + assert.True(t, bucket.disk.eligbleForCompaction(), \"check eligle before\") <nl> + }) <nl> + <nl> + t.Run(\"compact until no longer eligble\", func(t *testing.T) { <nl> + for bucket.disk.eligbleForCompaction() { <nl> + require.Nil(t, bucket.disk.compactOnce()) <nl> + } <nl> + }) <nl> + <nl> + t.Run(\"verify control after compaction\", func(t *testing.T) { <nl> + var retrieved []kv <nl> + <nl> + c := bucket.Cursor() <nl> + defer c.Close() <nl> + <nl> + for k, v := c.First(); k != nil; k, v = c.Next() { <nl> + retrieved = append(retrieved, kv{ <nl> + key: k, <nl> + value: v, <nl> + }) <nl> + } <nl> + <nl> + assert.Equal(t, expected, retrieved) <nl> + }) <nl> +} <nl> + <nl> func Test_CompactionSetStrategy(t *testing.T) { <nl> size := 30 <nl> ", "msg": "make sure updates/deletes get compaction properly in \"Replace\""}
{"diff_id": 3966, "repo": "semi-technologies/weaviate", "sha": "146f1d3fe31f1b74dc5ca4552d0672c9e53b225f", "time": "16.06.2021 09:01:44", "diff": "mmm a / adapters/repos/db/lsmkv/strategies_replace_integration_test.go <nl> ppp b / adapters/repos/db/lsmkv/strategies_replace_integration_test.go <nl>@@ -455,144 +455,145 @@ func TestReplaceStrategy_InsertAndUpdate_WithSecondaryKeys(t *testing.T) { <nl> }) <nl> }) <nl> - // t.Run(\"with a flush after the initial write and after the update\", func(t *testing.T) { <nl> - // b, err := NewBucket(dirName, WithStrategy(StrategyReplace)) <nl> - // require.Nil(t, err) <nl> - <nl> - // // so big it effectively never triggers as part of this test <nl> - // b.SetMemtableThreshold(1e9) <nl> - <nl> - // t.Run(\"set original values and verify\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // orig2 := []byte(\"original value for key2\") <nl> - // orig3 := []byte(\"original value for key3\") <nl> - <nl> - // err = b.Put(key1, orig1) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key2, orig2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, orig3) <nl> - // require.Nil(t, err) <nl> - <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig3) <nl> - // }) <nl> - <nl> - // t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> - // require.Nil(t, b.FlushAndSwitch()) <nl> - // }) <nl> - <nl> - // t.Run(\"replace some, keep one\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // replaced2 := []byte(\"updated value for key2\") <nl> - // replaced3 := []byte(\"updated value for key3\") <nl> - <nl> - // err = b.Put(key2, replaced2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, replaced3) <nl> - // require.Nil(t, err) <nl> - <nl> - // // Flush before verifying! <nl> - // require.Nil(t, b.FlushAndSwitch()) <nl> - <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced3) <nl> - // }) <nl> - // }) <nl> - <nl> - // t.Run(\"update in memtable, then do an orderly shutdown, and re-init\", func(t *testing.T) { <nl> - // b, err := NewBucket(dirName, WithStrategy(StrategyReplace)) <nl> - // require.Nil(t, err) <nl> - <nl> - // // so big it effectively never triggers as part of this test <nl> - // b.SetMemtableThreshold(1e9) <nl> - <nl> - // t.Run(\"set original values and verify\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // orig2 := []byte(\"original value for key2\") <nl> - // orig3 := []byte(\"original value for key3\") <nl> - <nl> - // err = b.Put(key1, orig1) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key2, orig2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, orig3) <nl> - // require.Nil(t, err) <nl> - // }) <nl> - <nl> - // t.Run(\"replace some, keep one\", func(t *testing.T) { <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // replaced2 := []byte(\"updated value for key2\") <nl> - // replaced3 := []byte(\"updated value for key3\") <nl> - <nl> - // err = b.Put(key2, replaced2) <nl> - // require.Nil(t, err) <nl> - // err = b.Put(key3, replaced3) <nl> - // require.Nil(t, err) <nl> - <nl> - // res, err := b.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced2) <nl> - // res, err = b.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced3) <nl> - // }) <nl> - <nl> - // t.Run(\"orderly shutdown\", func(t *testing.T) { <nl> - // b.Shutdown(context.Background()) <nl> - // }) <nl> - <nl> - // t.Run(\"init another bucket on the same files\", func(t *testing.T) { <nl> - // b2, err := NewBucket(dirName, WithStrategy(StrategyReplace)) <nl> - // require.Nil(t, err) <nl> - <nl> - // key1 := []byte(\"key-1\") <nl> - // key2 := []byte(\"key-2\") <nl> - // key3 := []byte(\"key-3\") <nl> - // orig1 := []byte(\"original value for key1\") <nl> - // replaced2 := []byte(\"updated value for key2\") <nl> - // replaced3 := []byte(\"updated value for key3\") <nl> - <nl> - // res, err := b2.Get(key1) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, orig1) <nl> - // res, err = b2.Get(key2) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced2) <nl> - // res, err = b2.Get(key3) <nl> - // require.Nil(t, err) <nl> - // assert.Equal(t, res, replaced3) <nl> - // }) <nl> - // }) <nl> + t.Run(\"with a flush after initial write and update\", func(t *testing.T) { <nl> + b, err := NewBucket(dirName, WithStrategy(StrategyReplace), <nl> + WithSecondaryIndicies(1)) <nl> + require.Nil(t, err) <nl> + <nl> + // so big it effectively never triggers as part of this test <nl> + b.SetMemtableThreshold(1e9) <nl> + <nl> + t.Run(\"set original values\", func(t *testing.T) { <nl> + key1 := []byte(\"key-1\") <nl> + key2 := []byte(\"key-2\") <nl> + key3 := []byte(\"key-3\") <nl> + secondaryKey1 := []byte(\"secondary-key-1\") <nl> + secondaryKey2 := []byte(\"secondary-key-2\") <nl> + secondaryKey3 := []byte(\"secondary-key-3\") <nl> + orig1 := []byte(\"original value for key1\") <nl> + orig2 := []byte(\"original value for key2\") <nl> + orig3 := []byte(\"original value for key3\") <nl> + <nl> + err = b.Put(key1, orig1, WithSecondaryKey(0, secondaryKey1)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key2, orig2, WithSecondaryKey(0, secondaryKey2)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key3, orig3, WithSecondaryKey(0, secondaryKey3)) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> + t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> + require.Nil(t, b.FlushAndSwitch()) <nl> + }) <nl> + <nl> + t.Run(\"replace the secondary keys on an update\", func(t *testing.T) { <nl> + key2 := []byte(\"key-2\") <nl> + key3 := []byte(\"key-3\") <nl> + secondaryKey2 := []byte(\"secondary-key-2-updated\") <nl> + secondaryKey3 := []byte(\"secondary-key-3-updated\") <nl> + replaced2 := []byte(\"twice updated value for key2\") <nl> + replaced3 := []byte(\"twice updated value for key3\") <nl> + <nl> + err = b.Put(key2, replaced2, WithSecondaryKey(0, secondaryKey2)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key3, replaced3, WithSecondaryKey(0, secondaryKey3)) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> + t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> + require.Nil(t, b.FlushAndSwitch()) <nl> + }) <nl> + <nl> + t.Run(\"verify again\", func(t *testing.T) { <nl> + secondaryKey1 := []byte(\"secondary-key-1\") <nl> + secondaryKey2 := []byte(\"secondary-key-2-updated\") <nl> + secondaryKey3 := []byte(\"secondary-key-3-updated\") <nl> + orig1 := []byte(\"original value for key1\") <nl> + replaced2 := []byte(\"twice updated value for key2\") <nl> + replaced3 := []byte(\"twice updated value for key3\") <nl> + <nl> + // verify you can find by updated secondary keys <nl> + res, err := b.GetBySecondary(0, secondaryKey1) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, orig1) <nl> + res, err = b.GetBySecondary(0, secondaryKey2) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, replaced2) <nl> + res, err = b.GetBySecondary(0, secondaryKey3) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, replaced3) <nl> + }) <nl> + }) <nl> + <nl> + t.Run(\"update in memtable then do an orderly shutdown and reinit\", func(t *testing.T) { <nl> + b, err := NewBucket(dirName, WithStrategy(StrategyReplace), <nl> + WithSecondaryIndicies(1)) <nl> + require.Nil(t, err) <nl> + <nl> + // so big it effectively never triggers as part of this test <nl> + b.SetMemtableThreshold(1e9) <nl> + <nl> + t.Run(\"set original values\", func(t *testing.T) { <nl> + key1 := []byte(\"key-1\") <nl> + key2 := []byte(\"key-2\") <nl> + key3 := []byte(\"key-3\") <nl> + secondaryKey1 := []byte(\"secondary-key-1\") <nl> + secondaryKey2 := []byte(\"secondary-key-2\") <nl> + secondaryKey3 := []byte(\"secondary-key-3\") <nl> + orig1 := []byte(\"original value for key1\") <nl> + orig2 := []byte(\"original value for key2\") <nl> + orig3 := []byte(\"original value for key3\") <nl> + <nl> + err = b.Put(key1, orig1, WithSecondaryKey(0, secondaryKey1)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key2, orig2, WithSecondaryKey(0, secondaryKey2)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key3, orig3, WithSecondaryKey(0, secondaryKey3)) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> + t.Run(\"replace the secondary keys on an update\", func(t *testing.T) { <nl> + key2 := []byte(\"key-2\") <nl> + key3 := []byte(\"key-3\") <nl> + secondaryKey2 := []byte(\"secondary-key-2-updated\") <nl> + secondaryKey3 := []byte(\"secondary-key-3-updated\") <nl> + replaced2 := []byte(\"twice updated value for key2\") <nl> + replaced3 := []byte(\"twice updated value for key3\") <nl> + <nl> + err = b.Put(key2, replaced2, WithSecondaryKey(0, secondaryKey2)) <nl> + require.Nil(t, err) <nl> + err = b.Put(key3, replaced3, WithSecondaryKey(0, secondaryKey3)) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> + t.Run(\"flush memtable to disk\", func(t *testing.T) { <nl> + require.Nil(t, b.Shutdown(context.Background())) <nl> + }) <nl> + <nl> + t.Run(\"init a new one and verify\", func(t *testing.T) { <nl> + b2, err := NewBucket(dirName, WithStrategy(StrategyReplace), <nl> + WithSecondaryIndicies(1)) <nl> + require.Nil(t, err) <nl> + <nl> + secondaryKey1 := []byte(\"secondary-key-1\") <nl> + secondaryKey2 := []byte(\"secondary-key-2-updated\") <nl> + secondaryKey3 := []byte(\"secondary-key-3-updated\") <nl> + orig1 := []byte(\"original value for key1\") <nl> + replaced2 := []byte(\"twice updated value for key2\") <nl> + replaced3 := []byte(\"twice updated value for key3\") <nl> + <nl> + // verify you can find by updated secondary keys <nl> + res, err := b2.GetBySecondary(0, secondaryKey1) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, orig1) <nl> + res, err = b2.GetBySecondary(0, secondaryKey2) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, replaced2) <nl> + res, err = b2.GetBySecondary(0, secondaryKey3) <nl> + require.Nil(t, err) <nl> + assert.Equal(t, res, replaced3) <nl> + }) <nl> + }) <nl> } <nl> func TestReplaceStrategy_InsertAndDelete(t *testing.T) { <nl> ", "msg": "add remaining integration tests for replace with 2ndary"}
{"diff_id": 3979, "repo": "semi-technologies/weaviate", "sha": "8acc06255996fbd7213d55f23ca520e5ee9ee763", "time": "28.07.2021 16:44:43", "diff": "mmm a / adapters/repos/db/lsmkv/segmentindex/disk_tree.go <nl> ppp b / adapters/repos/db/lsmkv/segmentindex/disk_tree.go <nl>@@ -89,33 +89,27 @@ func (t *DiskTree) readNodeAt(offset int64) (dtNode, error) { <nl> func (t *DiskTree) readNode(r io.Reader) (dtNode, error) { <nl> var out dtNode <nl> + tmpBuf := make([]byte, 32) // 32 bytes (4x uint64 is the most we'll ever read at the same time) <nl> - var keyLen uint32 <nl> - if err := binary.Read(r, binary.LittleEndian, &keyLen); err != nil { <nl> + if _, err := r.Read(tmpBuf[0:4]); err != nil { <nl> return out, err <nl> } <nl> + keyLen := binary.LittleEndian.Uint32(tmpBuf[0:4]) <nl> out.key = make([]byte, keyLen) <nl> if _, err := r.Read(out.key); err != nil { <nl> return out, err <nl> } <nl> - if err := binary.Read(r, binary.LittleEndian, &out.startPos); err != nil { <nl> - return out, err <nl> - } <nl> - <nl> - if err := binary.Read(r, binary.LittleEndian, &out.endPos); err != nil { <nl> - return out, err <nl> - } <nl> - <nl> - if err := binary.Read(r, binary.LittleEndian, &out.leftChild); err != nil { <nl> - return out, err <nl> - } <nl> - <nl> - if err := binary.Read(r, binary.LittleEndian, &out.rightChild); err != nil { <nl> + // read the next four 8 byte numbers at once <nl> + if _, err := r.Read(tmpBuf[0:32]); err != nil { <nl> return out, err <nl> } <nl> + out.startPos = binary.LittleEndian.Uint64(tmpBuf[0:8]) <nl> + out.endPos = binary.LittleEndian.Uint64(tmpBuf[8:16]) <nl> + out.leftChild = int64(binary.LittleEndian.Uint64(tmpBuf[16:24])) <nl> + out.rightChild = int64(binary.LittleEndian.Uint64(tmpBuf[24:32])) <nl> return out, nil <nl> } <nl> ", "msg": "reduce unnecessary allocs in lsmkv/segmentindex readNode"}
{"diff_id": 3986, "repo": "semi-technologies/weaviate", "sha": "ae308e9f14507538efa35846ac3f6b7fbaa2993c", "time": "10.08.2021 13:11:46", "diff": "mmm a / adapters/repos/db/vector/hnsw/deserializer2.go <nl> ppp b / adapters/repos/db/vector/hnsw/deserializer2.go <nl>@@ -266,7 +266,7 @@ func (c *Deserializer2) ReadDeleteNode(r io.Reader, res *DeserializationResult) <nl> func (c *Deserializer2) readUint64(r io.Reader) (uint64, error) { <nl> var value uint64 <nl> tmpBuf := make([]byte, 8) <nl> - if _, err := io.ReadFull(r, tmpBuf); err != nil { <nl> + if _, err := r.Read(tmpBuf); err != nil { <nl> return 0, err <nl> } <nl> @@ -278,7 +278,7 @@ func (c *Deserializer2) readUint64(r io.Reader) (uint64, error) { <nl> func (c *Deserializer2) readUint16(r io.Reader) (uint16, error) { <nl> var value uint16 <nl> tmpBuf := make([]byte, 2) <nl> - if _, err := io.ReadFull(r, tmpBuf); err != nil { <nl> + if _, err := r.Read(tmpBuf); err != nil { <nl> return 0, err <nl> } <nl> value = binary.LittleEndian.Uint16(tmpBuf) <nl> ", "msg": "Replace io.ReadAll() usage with r.Read() to fix failing integration tests"}
{"diff_id": 3995, "repo": "semi-technologies/weaviate", "sha": "f87f485c0e9e42d9fd553ae89b3ea2aa4a1a73a3", "time": "14.09.2021 23:12:27", "diff": "mmm a / usecases/cluster/state.go <nl> ppp b / usecases/cluster/state.go <nl>@@ -2,6 +2,7 @@ package cluster <nl> import ( <nl> \"fmt\" <nl> + \"net\" <nl> \"strings\" <nl> \"github.com/hashicorp/memberlist\" <nl> @@ -43,11 +44,20 @@ func Init(userConfig Config, logger logrus.FieldLogger) (*State, error) { <nl> } <nl> if len(joinAddr) > 0 { <nl> + <nl> + _, err := net.LookupIP(strings.Split(joinAddr[0], \":\")[0]) <nl> + if err != nil { <nl> + logger.WithField(\"action\", \"cluster_attempt_join\"). <nl> + WithField(\"remote_hostname\", joinAddr[0]). <nl> + WithError(err). <nl> + Warn(\"specified hostname to join cluster cannot be resolved. This is fine\" + <nl> + \"if this is the first node of a new cluster, but problematic otherwise.\") <nl> + } else { <nl> _, err := list.Join(joinAddr) <nl> if err != nil { <nl> return nil, errors.Wrap(err, \"join cluster\") <nl> } <nl> - <nl> + } <nl> } <nl> return &State{list: list}, nil <nl> ", "msg": "try to resolve hostname, skip if not resolvable"}
{"diff_id": 3998, "repo": "semi-technologies/weaviate", "sha": "539354ba2a3bf4a7e0a6e36060ddfe000a207139", "time": "30.09.2021 11:39:37", "diff": "mmm a / adapters/repos/db/aggregations_integration_test.go <nl> ppp b / adapters/repos/db/aggregations_integration_test.go <nl>@@ -926,6 +926,9 @@ func testNumericalAggregationsWithGrouping(repo *DB, exact bool) func(t *testing <nl> }) <nl> t.Run(\"array types, single aggregator strings\", func(t *testing.T) { <nl> + if !exact { <nl> + t.Skip() <nl> + } <nl> params := aggregation.Params{ <nl> ClassName: schema.ClassName(arrayTypesClass.Class), <nl> GroupBy: &filters.Path{ <nl> @@ -971,6 +974,9 @@ func testNumericalAggregationsWithGrouping(repo *DB, exact bool) func(t *testing <nl> }) <nl> t.Run(\"array types, single aggregator numbers\", func(t *testing.T) { <nl> + if !exact { <nl> + t.Skip() <nl> + } <nl> params := aggregation.Params{ <nl> ClassName: schema.ClassName(arrayTypesClass.Class), <nl> GroupBy: &filters.Path{ <nl> @@ -1757,6 +1763,9 @@ func testNumericalAggregationsWithoutGrouping(repo *DB, <nl> // }) <nl> t.Run(\"array types, single aggregator strings\", func(t *testing.T) { <nl> + if !exact { <nl> + t.Skip() <nl> + } <nl> params := aggregation.Params{ <nl> ClassName: schema.ClassName(arrayTypesClass.Class), <nl> GroupBy: nil, // explicitly set to nil <nl> ", "msg": "temporarily deactivate flaky aggregation tests for multi shard"}
{"diff_id": 4000, "repo": "semi-technologies/weaviate", "sha": "8ca2eae6c1e5d355b9076d05314c09fbfc2f7fa0", "time": "05.10.2021 15:29:29", "diff": "mmm a / adapters/repos/db/lsmkv/segment_serialization.go <nl> ppp b / adapters/repos/db/lsmkv/segment_serialization.go <nl>@@ -145,6 +145,21 @@ func (s segmentIndices) WriteTo(w io.Writer) (int64, error) { <nl> currentOffset := uint64(s.keys[len(s.keys)-1].valueEnd) <nl> var written int64 <nl> + if _, err := os.Stat(s.scratchSpacePath); err == nil { <nl> + // exists, we need to delete <nl> + // This could be the case if Weaviate shut down unexpectedly (i.e. crashed) <nl> + // while a compaction was running. We can safely discard the contents of <nl> + // the scratch space. <nl> + <nl> + if err := os.RemoveAll(s.scratchSpacePath); err != nil { <nl> + return written, errors.Wrap(err, \"clean up previous scratch space\") <nl> + } <nl> + } else if os.IsNotExist(err) { <nl> + // does not exist yet, nothing to - will be created in the next step <nl> + } else { <nl> + return written, errors.Wrap(err, \"check for scratch space directory\") <nl> + } <nl> + <nl> if err := os.Mkdir(s.scratchSpacePath, 0o777); err != nil { <nl> return written, errors.Wrap(err, \"create scratch space\") <nl> } <nl> ", "msg": "remove lsm compaction scratch space if it exists\nThis would be the case if a crash interrupted a previous compaction. In\nthis case we simply start over and it's safe to remove the old scratch\nspace. However, we should not error, blocking the compaction forever."}
{"diff_id": 4002, "repo": "semi-technologies/weaviate", "sha": "6ab2b95f08b59cd12ff8e957ec33d85f20d759c4", "time": "13.10.2021 11:47:45", "diff": "mmm a / adapters/repos/db/lsmkv/segment_collection_strategy.go <nl> ppp b / adapters/repos/db/lsmkv/segment_collection_strategy.go <nl>@@ -55,35 +55,24 @@ func (i *segment) collectionStratParseData(in []byte) ([]value, error) { <nl> return nil, NotFound <nl> } <nl> - r := bytes.NewReader(in) <nl> - <nl> - readSoFar := 0 <nl> + offset := 0 <nl> - var valuesLen uint64 <nl> - if err := binary.Read(r, binary.LittleEndian, &valuesLen); err != nil { <nl> - return nil, errors.Wrap(err, \"read values len\") <nl> - } <nl> - readSoFar += 8 <nl> + valuesLen := binary.LittleEndian.Uint64(in[offset : offset+8]) <nl> + offset += 8 <nl> values := make([]value, valuesLen) <nl> - for i := range values { <nl> - if err := binary.Read(r, binary.LittleEndian, &values[i].tombstone); err != nil { <nl> - return nil, errors.Wrap(err, \"read value tombstone\") <nl> - } <nl> - readSoFar += 1 <nl> + valueIndex := 0 <nl> + for valueIndex < int(valuesLen) { <nl> + values[valueIndex].tombstone = in[offset] == 0x01 <nl> + offset += 1 <nl> - var valueLen uint64 <nl> - if err := binary.Read(r, binary.LittleEndian, &valueLen); err != nil { <nl> - return nil, errors.Wrap(err, \"read value len\") <nl> - } <nl> - readSoFar += 8 <nl> + valueLen := binary.LittleEndian.Uint64(in[offset : offset+8]) <nl> + offset += 8 <nl> - values[i].value = make([]byte, valueLen) <nl> - n, err := r.Read(values[i].value) <nl> - if err != nil { <nl> - return nil, errors.Wrap(err, \"read value\") <nl> - } <nl> - readSoFar += n <nl> + values[valueIndex].value = in[offset : offset+int(valueLen)] <nl> + offset += int(valueLen) <nl> + <nl> + valueIndex++ <nl> } <nl> fmt.Printf(\"collection parse took %s\\n\", time.Since(before)) <nl> ", "msg": "improve allocation efficiency of lsm collection strat parse"}
{"diff_id": 4009, "repo": "semi-technologies/weaviate", "sha": "d533c69b7fde975c6467db697a642aa11f921e94", "time": "16.11.2021 18:03:51", "diff": "mmm a / adapters/repos/db/inverted/cached_filters_integration_test.go <nl> ppp b / adapters/repos/db/inverted/cached_filters_integration_test.go <nl>@@ -20,7 +20,7 @@ import ( <nl> \"github.com/stretchr/testify/require\" <nl> ) <nl> -func Test_CachedFilters(t *testing.T) { <nl> +func Test_CachedFilters_String(t *testing.T) { <nl> dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> os.MkdirAll(dirName, 0o777) <nl> defer func() { <nl> @@ -290,8 +290,278 @@ func Test_CachedFilters(t *testing.T) { <nl> } <nl> } <nl> +func Test_CachedFilters_Int(t *testing.T) { <nl> + dirName := fmt.Sprintf(\"./testdata/%d\", rand.Intn(10000000)) <nl> + os.MkdirAll(dirName, 0o777) <nl> + defer func() { <nl> + err := os.RemoveAll(dirName) <nl> + fmt.Println(err) <nl> + }() <nl> + <nl> + logger, _ := test.NewNullLogger() <nl> + store, err := lsmkv.New(dirName, logger) <nl> + require.Nil(t, err) <nl> + <nl> + propName := \"inverted-without-frequency\" <nl> + <nl> + require.Nil(t, store.CreateOrLoadBucket(context.Background(), <nl> + helpers.BucketFromPropNameLSM(propName), <nl> + lsmkv.WithStrategy(lsmkv.StrategySetCollection))) <nl> + require.Nil(t, store.CreateOrLoadBucket(context.Background(), <nl> + helpers.HashBucketFromPropNameLSM(propName), <nl> + lsmkv.WithStrategy(lsmkv.StrategyReplace))) <nl> + <nl> + bucket := store.Bucket(helpers.BucketFromPropNameLSM(propName)) <nl> + bHashes := store.Bucket(helpers.HashBucketFromPropNameLSM(propName)) <nl> + <nl> + defer store.Shutdown(context.Background()) <nl> + <nl> + fakeInvertedIndex := map[int64][]uint64{ <nl> + 2: []uint64{2, 4, 6, 8, 10, 12, 14, 16}, <nl> + 3: []uint64{3, 6, 9, 12, 15}, <nl> + 4: []uint64{4, 8, 12, 16}, <nl> + 5: []uint64{5, 10, 15}, <nl> + 6: []uint64{6, 12}, <nl> + 7: []uint64{7, 14}, <nl> + 8: []uint64{8, 16}, <nl> + 9: []uint64{9}, <nl> + 10: []uint64{10}, <nl> + 11: []uint64{11}, <nl> + 12: []uint64{12}, <nl> + 13: []uint64{13}, <nl> + 14: []uint64{14}, <nl> + 15: []uint64{15}, <nl> + 16: []uint64{16}, <nl> + } <nl> + <nl> + t.Run(\"import data\", func(t *testing.T) { <nl> + for value, ids := range fakeInvertedIndex { <nl> + idValues := idsToBinaryList(ids) <nl> + hash := make([]byte, 16) <nl> + _, err := rand.Read(hash) <nl> + require.Nil(t, err) <nl> + <nl> + valueBytes, err := LexicographicallySortableInt64(value) <nl> + require.Nil(t, err) <nl> + <nl> + require.Nil(t, bucket.SetAdd(valueBytes, idValues)) <nl> + require.Nil(t, bHashes.Put([]byte(valueBytes), hash)) <nl> + } <nl> + <nl> + require.Nil(t, bucket.FlushAndSwitch()) <nl> + }) <nl> + <nl> + rowCacher := newRowCacherSpy() <nl> + searcher := NewSearcher(store, schema.Schema{}, rowCacher, nil, nil, nil) <nl> + <nl> + type test struct { <nl> + name string <nl> + filter *filters.LocalFilter <nl> + expectedListBeforeUpdate func() helpers.AllowList <nl> + expectedListAfterUpdate func() helpers.AllowList <nl> + } <nl> + <nl> + tests := []test{ <nl> + { <nl> + name: \"exact match - single level\", <nl> + filter: &filters.LocalFilter{ <nl> + Root: &filters.Clause{ <nl> + Operator: filters.OperatorEqual, <nl> + On: &filters.Path{ <nl> + Class: \"foo\", <nl> + Property: schema.PropertyName(propName), <nl> + }, <nl> + Value: &filters.Value{ <nl> + Value: 7, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + }, <nl> + }, <nl> + expectedListBeforeUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(7) <nl> + list.Insert(14) <nl> + return list <nl> + }, <nl> + expectedListAfterUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(7) <nl> + list.Insert(14) <nl> + list.Insert(21) <nl> + return list <nl> + }, <nl> + }, <nl> + { <nl> + name: \"exact match - or filter\", <nl> + filter: &filters.LocalFilter{ <nl> + Root: &filters.Clause{ <nl> + Operator: filters.OperatorOr, <nl> + Operands: []filters.Clause{ <nl> + { <nl> + Operator: filters.OperatorEqual, <nl> + On: &filters.Path{ <nl> + Class: \"foo\", <nl> + Property: schema.PropertyName(propName), <nl> + }, <nl> + Value: &filters.Value{ <nl> + Value: 7, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + }, <nl> + { <nl> + Operator: filters.OperatorEqual, <nl> + On: &filters.Path{ <nl> + Class: \"foo\", <nl> + Property: schema.PropertyName(propName), <nl> + }, <nl> + Value: &filters.Value{ <nl> + Value: 8, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + expectedListBeforeUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(7) <nl> + list.Insert(8) <nl> + list.Insert(14) <nl> + list.Insert(16) <nl> + return list <nl> + }, <nl> + expectedListAfterUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(7) <nl> + list.Insert(8) <nl> + list.Insert(14) <nl> + list.Insert(16) <nl> + list.Insert(21) <nl> + return list <nl> + }, <nl> + }, <nl> + { <nl> + name: \"exact match - and filter\", <nl> + filter: &filters.LocalFilter{ <nl> + Root: &filters.Clause{ <nl> + Operator: filters.OperatorAnd, <nl> + Operands: []filters.Clause{ <nl> + { <nl> + Operator: filters.OperatorEqual, <nl> + On: &filters.Path{ <nl> + Class: \"foo\", <nl> + Property: schema.PropertyName(propName), <nl> + }, <nl> + Value: &filters.Value{ <nl> + Value: 7, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + }, <nl> + { <nl> + Operator: filters.OperatorEqual, <nl> + On: &filters.Path{ <nl> + Class: \"foo\", <nl> + Property: schema.PropertyName(propName), <nl> + }, <nl> + Value: &filters.Value{ <nl> + Value: 14, <nl> + Type: schema.DataTypeInt, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + }, <nl> + expectedListBeforeUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(14) <nl> + return list <nl> + }, <nl> + expectedListAfterUpdate: func() helpers.AllowList { <nl> + list := helpers.AllowList{} <nl> + list.Insert(14) <nl> + return list <nl> + }, <nl> + }, <nl> + } <nl> + <nl> + for _, test := range tests { <nl> + t.Run(test.name, func(t *testing.T) { <nl> + rowCacher.reset() <nl> + <nl> + t.Run(\"cache should be empty\", func(t *testing.T) { <nl> + assert.Equal(t, 0, rowCacher.count) <nl> + }) <nl> + <nl> + t.Run(\"with cold cache\", func(t *testing.T) { <nl> + res, err := searcher.DocIDs(context.Background(), test.filter, <nl> + additional.Properties{}, \"\") <nl> + assert.Nil(t, err) <nl> + assert.Equal(t, test.expectedListBeforeUpdate(), res) <nl> + }) <nl> + <nl> + t.Run(\"cache should be filled now\", func(t *testing.T) { <nl> + assert.Equal(t, 1, rowCacher.count) <nl> + assert.Equal(t, test.expectedListBeforeUpdate(), <nl> + rowCacher.lastEntry.AllowList) <nl> + assert.Equal(t, 0, rowCacher.hitCount) <nl> + }) <nl> + <nl> + t.Run(\"with warm cache\", func(t *testing.T) { <nl> + res, err := searcher.DocIDs(context.Background(), test.filter, <nl> + additional.Properties{}, \"\") <nl> + assert.Nil(t, err) <nl> + assert.Equal(t, test.expectedListBeforeUpdate(), res) <nl> + }) <nl> + <nl> + t.Run(\"cache should have received a hit\", func(t *testing.T) { <nl> + assert.Equal(t, 1, rowCacher.hitCount) <nl> + }) <nl> + <nl> + t.Run(\"alter the state to invalidate the cache\", func(t *testing.T) { <nl> + value, _ := LexicographicallySortableInt64(7) <nl> + idsBinary := idsToBinaryList([]uint64{21}) <nl> + hash := make([]byte, 16) <nl> + _, err := rand.Read(hash) <nl> + require.Nil(t, err) <nl> + require.Nil(t, bucket.SetAdd([]byte(value), idsBinary)) <nl> + require.Nil(t, bHashes.Put([]byte(value), hash)) <nl> + }) <nl> + <nl> + t.Run(\"with a stale cache\", func(t *testing.T) { <nl> + res, err := searcher.DocIDs(context.Background(), test.filter, <nl> + additional.Properties{}, \"\") <nl> + assert.Nil(t, err) <nl> + assert.Equal(t, test.expectedListAfterUpdate(), res) <nl> + }) <nl> + <nl> + t.Run(\"cache should have not have received another hit\", func(t *testing.T) { <nl> + assert.Equal(t, 1, rowCacher.hitCount) <nl> + }) <nl> + <nl> + t.Run(\"with the cache being fresh again now\", func(t *testing.T) { <nl> + res, err := searcher.DocIDs(context.Background(), test.filter, <nl> + additional.Properties{}, \"\") <nl> + assert.Nil(t, err) <nl> + assert.Equal(t, test.expectedListAfterUpdate(), res) <nl> + }) <nl> + <nl> + t.Run(\"cache should have received another hit\", func(t *testing.T) { <nl> + assert.Equal(t, 2, rowCacher.hitCount) <nl> + }) <nl> + <nl> + t.Run(\"restore inverted index, so we can run test suite again\", <nl> + func(t *testing.T) { <nl> + idsList := idsToBinaryList([]uint64{21}) <nl> + value, _ := LexicographicallySortableInt64(7) <nl> + require.Nil(t, bucket.SetDeleteSingle(value, idsList[0])) <nl> + rowCacher.reset() <nl> + }) <nl> + }) <nl> + } <nl> +} <nl> + <nl> func idsToBinaryList(ids []uint64) [][]byte { <nl> - out := make([][]byte, len(ids)*8) <nl> + out := make([][]byte, len(ids)) <nl> for i, id := range ids { <nl> out[i] = make([]byte, 8) <nl> binary.LittleEndian.PutUint64(out[i], id) <nl> ", "msg": "validate cached int filters"}
{"diff_id": 4018, "repo": "semi-technologies/weaviate", "sha": "f26f67aa313bb8bbc3ae29a433adcd86bfb53db6", "time": "06.01.2022 20:01:35", "diff": "mmm a / modules/qna-transformers/additional/answer/answer_test.go <nl> ppp b / modules/qna-transformers/additional/answer/answer_test.go <nl>@@ -453,8 +453,10 @@ func TestAdditionalAnswerProvider(t *testing.T) { <nl> assert.Equal(t, true, answerAdditional.HasAnswer) <nl> answer, answerOK = in[1].AdditionalProperties[\"answer\"] <nl> assert.False(t, answerOK) <nl> + assert.Nil(t, answer) <nl> answer, answerOK = in[2].AdditionalProperties[\"answer\"] <nl> assert.False(t, answerOK) <nl> + assert.Nil(t, answer) <nl> }) <nl> } <nl> ", "msg": "fixed tests for qna reranking feature"}
{"diff_id": 4023, "repo": "semi-technologies/weaviate", "sha": "fe6be937fbb59cc9a9187e54eb345f2591da4174", "time": "04.02.2022 11:50:05", "diff": "mmm a / adapters/repos/db/lsmkv/bucket.go <nl> ppp b / adapters/repos/db/lsmkv/bucket.go <nl>@@ -353,20 +353,24 @@ func (b *Bucket) setNewActiveMemtable() error { <nl> } <nl> func (b *Bucket) Count() int { <nl> + b.flushLock.RLock() <nl> + defer b.flushLock.RUnlock() <nl> + <nl> if b.strategy != StrategyReplace { <nl> panic(\"Count() called on strategy other than 'replace'\") <nl> } <nl> - memtableCount := b.memtableNetCount() <nl> - fmt.Printf(\"memtable count is %d\\n\", memtableCount) <nl> + memtableCount := b.memtableNetCount(b.active.countStats()) <nl> + if b.flushing != nil { <nl> + memtableCount += b.memtableNetCount(b.flushing.countStats()) <nl> + } <nl> diskCount := b.disk.count() <nl> return memtableCount + diskCount <nl> } <nl> -func (b *Bucket) memtableNetCount() int { <nl> +func (b *Bucket) memtableNetCount(stats *countStats) int { <nl> netCount := 0 <nl> - stats := b.active.countStats() <nl> // TODO: this uses regular get, given that this may be called quite commonly, <nl> // we might consider building a pure Exists(), which skips reading the value <nl> ", "msg": "make sure to include flushing memtable if flush cycle is running"}
{"diff_id": 4027, "repo": "semi-technologies/weaviate", "sha": "2473c95dc9b19ebd5d386660d03c7f95d1193139", "time": "21.02.2022 15:34:41", "diff": "mmm a / adapters/repos/db/inverted/prop_length_tracker_test.go <nl> ppp b / adapters/repos/db/inverted/prop_length_tracker_test.go <nl>@@ -8,79 +8,52 @@ import ( <nl> ) <nl> func Test_PropertyLengthTracker(t *testing.T) { <nl> - t.Run(\"single prop, mixed values\", func(t *testing.T) { <nl> - tracker := NewPropertyLengthTracker(\"\") <nl> - values := []float32{ <nl> - 2, 2, 3, 100, 100, 500, 7, <nl> - } <nl> - <nl> - actualMean := float32(0) <nl> - for _, v := range values { <nl> - tracker.TrackProperty(\"my-very-first-prop\", v) <nl> - actualMean += v <nl> - } <nl> - actualMean = actualMean / float32(len(values)) <nl> - <nl> - res, err := tracker.PropertyMean(\"my-very-first-prop\") <nl> - require.Nil(t, err) <nl> - <nl> - assert.InEpsilon(t, actualMean, res, 0.1) <nl> - }) <nl> - <nl> - t.Run(\"single prop, high values\", func(t *testing.T) { <nl> - tracker := NewPropertyLengthTracker(\"\") <nl> - values := []float32{ <nl> - 1000, 1200, 1000, 1300, 800, 2000, 2050, 2070, 900, <nl> - } <nl> - <nl> - actualMean := float32(0) <nl> - for _, v := range values { <nl> - tracker.TrackProperty(\"my-very-first-prop\", v) <nl> - actualMean += v <nl> - } <nl> - actualMean = actualMean / float32(len(values)) <nl> - <nl> - res, err := tracker.PropertyMean(\"my-very-first-prop\") <nl> - require.Nil(t, err) <nl> - <nl> - assert.InEpsilon(t, actualMean, res, 0.1) <nl> - }) <nl> - <nl> - t.Run(\"single prop, very high values\", func(t *testing.T) { <nl> - tracker := NewPropertyLengthTracker(\"\") <nl> - values := []float32{ <nl> + t.Run(\"single prop\", func(t *testing.T) { <nl> + type test struct { <nl> + values []float32 <nl> + name string <nl> + } <nl> + <nl> + tests := []test{ <nl> + { <nl> + values: []float32{2, 2, 3, 100, 100, 500, 7}, <nl> + name: \"mixed values\", <nl> + }, { <nl> + values: []float32{ <nl> + 1000, 1200, 1000, 1300, 800, 2000, 2050, <nl> + 2070, 900, <nl> + }, <nl> + name: \"high values\", <nl> + }, { <nl> + values: []float32{ <nl> 60000, 50000, 65000, <nl> - } <nl> + }, <nl> + name: \"very high values\", <nl> + }, { <nl> + values: []float32{ <nl> + 1, 2, 4, 3, 4, 2, 1, 5, 6, 7, 8, 2, 7, 2, 3, 5, <nl> + 6, 3, 5, 9, 3, 4, 8, <nl> + }, <nl> + name: \"very low values\", <nl> + }, <nl> + } <nl> + <nl> + for _, test := range tests { <nl> + t.Run(test.name, func(t *testing.T) { <nl> + tracker := NewPropertyLengthTracker(\"\") <nl> actualMean := float32(0) <nl> - for _, v := range values { <nl> + for _, v := range test.values { <nl> tracker.TrackProperty(\"my-very-first-prop\", v) <nl> actualMean += v <nl> } <nl> - actualMean = actualMean / float32(len(values)) <nl> + actualMean = actualMean / float32(len(test.values)) <nl> res, err := tracker.PropertyMean(\"my-very-first-prop\") <nl> require.Nil(t, err) <nl> assert.InEpsilon(t, actualMean, res, 0.1) <nl> }) <nl> - <nl> - t.Run(\"single prop, very low values\", func(t *testing.T) { <nl> - tracker := NewPropertyLengthTracker(\"\") <nl> - values := []float32{ <nl> - 1, 2, 4, 3, 4, 2, 1, 5, 6, 7, 8, 2, 7, 2, 3, 5, 6, 3, 5, 9, 3, 4, 8, <nl> } <nl> - <nl> - actualMean := float32(0) <nl> - for _, v := range values { <nl> - tracker.TrackProperty(\"my-very-first-prop\", v) <nl> - actualMean += v <nl> - } <nl> - actualMean = actualMean / float32(len(values)) <nl> - <nl> - res, err := tracker.PropertyMean(\"my-very-first-prop\") <nl> - require.Nil(t, err) <nl> - <nl> - assert.InEpsilon(t, actualMean, res, 0.1) <nl> }) <nl> } <nl> ", "msg": "refactor test for prop length tracker"}
{"diff_id": 4029, "repo": "semi-technologies/weaviate", "sha": "2e486864a339bd9fd9bc2bd32948322f47580690", "time": "25.02.2022 17:41:39", "diff": "mmm a / adapters/repos/db/inverted/bm25_searcher.go <nl> ppp b / adapters/repos/db/inverted/bm25_searcher.go <nl>@@ -116,17 +116,6 @@ func (b *BM25Searcher) retrieveScoreAndSortForSingleTerm(ctx context.Context, <nl> } <nl> fmt.Printf(\"term %q: score ids took %s\\n\", term, time.Since(before)) <nl> - before = time.Now() <nl> - // TODO: this runtime sorting is only because the storage is not implemented <nl> - // in an always sorted manner. Once we have that implemented, we can skip <nl> - // this expensive runtime-sort <nl> - sort.Slice(ids.docIDs, func(a, b int) bool { <nl> - return ids.docIDs[a].id < ids.docIDs[b].id <nl> - }) <nl> - <nl> - // TODO: structured logging <nl> - fmt.Printf(\"term %q: sorting by doc ids took %s\\n\", term, time.Since(before)) <nl> - <nl> return ids, nil <nl> } <nl> ", "msg": "remove temporary doc id sorting from bm25\nnow that the Map type in the lsmkv supports sorted keys and the doc ids\nare encoded is big endian, any time we retrieve doc ids, we can be\ncertain that they are already sorted."}
{"diff_id": 4039, "repo": "semi-technologies/weaviate", "sha": "3fa0a9776d079f897db5144f418b536cf5eb8581", "time": "10.03.2022 18:08:37", "diff": "mmm a / None <nl> ppp b / adapters/repos/db/lsmkv/binary_search_tree_test.go <nl>+package lsmkv <nl> + <nl> +import ( <nl> + \"math/rand\" <nl> + \"testing\" <nl> + \"time\" <nl> + <nl> + \"github.com/stretchr/testify/require\" <nl> +) <nl> + <nl> +// This test asserts that the *binarySearchTree.insert <nl> +// method properly calculates the net additions of a <nl> +// new node into the tree <nl> +func TestInsertNetAdditions_Replace(t *testing.T) { <nl> + rand.Seed(time.Now().UnixNano()) <nl> + <nl> + t.Run(\"single node entry\", func(t *testing.T) { <nl> + tree := &binarySearchTree{} <nl> + <nl> + key := make([]byte, 8) <nl> + val := make([]byte, 8) <nl> + <nl> + rand.Read(key) <nl> + rand.Read(val) <nl> + <nl> + n := tree.insert(key, val, nil) <nl> + require.Equal(t, len(key)+len(val), n) <nl> + }) <nl> + <nl> + t.Run(\"multiple unique node entries\", func(t *testing.T) { <nl> + tree := &binarySearchTree{} <nl> + <nl> + amount := 100 <nl> + size := 8 <nl> + <nl> + var n int <nl> + for i := 0; i < amount; i++ { <nl> + key := make([]byte, size) <nl> + val := make([]byte, size) <nl> + <nl> + rand.Read(key) <nl> + rand.Read(val) <nl> + <nl> + n += tree.insert(key, val, nil) <nl> + } <nl> + <nl> + require.Equal(t, amount*size*2, n) <nl> + }) <nl> + <nl> + t.Run(\"multiple non-unique node entries\", func(t *testing.T) { <nl> + tree := &binarySearchTree{} <nl> + <nl> + var ( <nl> + amount = 100 <nl> + keySize = 100 <nl> + origValSize = 100 <nl> + newValSize = origValSize * 100 <nl> + keys = make([][]byte, amount) <nl> + vals = make([][]byte, amount) <nl> + <nl> + netAdditions int <nl> + ) <nl> + <nl> + // write the keys and original values <nl> + for i := range keys { <nl> + key := make([]byte, keySize) <nl> + rand.Read(key) <nl> + <nl> + val := make([]byte, origValSize) <nl> + rand.Read(val) <nl> + <nl> + keys[i], vals[i] = key, val <nl> + } <nl> + <nl> + // make initial inserts <nl> + for i := range keys { <nl> + netAdditions += tree.insert(keys[i], vals[i], nil) <nl> + } <nl> + <nl> + // change the values of the existing keys <nl> + // with new values of different length <nl> + for i := 0; i < amount; i++ { <nl> + val := make([]byte, newValSize) <nl> + rand.Read(val) <nl> + <nl> + vals[i] = val <nl> + } <nl> + <nl> + for i := 0; i < amount; i++ { <nl> + netAdditions += tree.insert(keys[i], vals[i], nil) <nl> + } <nl> + <nl> + // Formulas for calculating the total net additions after <nl> + // updating the keys with differently sized values <nl> + expectedFirstNetAdd := amount * (keySize + origValSize) <nl> + expectedSecondNetAdd := (amount * (keySize + newValSize)) - (amount * keySize) - (amount * origValSize) <nl> + expectedNetAdditions := expectedFirstNetAdd + expectedSecondNetAdd <nl> + <nl> + require.Equal(t, expectedNetAdditions, netAdditions) <nl> + }) <nl> +} <nl> ", "msg": "add BST tests for Replace strategy"}
{"diff_id": 4041, "repo": "semi-technologies/weaviate", "sha": "fb9b142e080a1513fa30efb7ccc628cf896778ab", "time": "11.03.2022 10:28:35", "diff": "mmm a / adapters/repos/db/lsmkv/bucket_threshold_test.go <nl> ppp b / adapters/repos/db/lsmkv/bucket_threshold_test.go <nl>@@ -164,9 +164,9 @@ func TestMemtableThreshold_Replace(t *testing.T) { <nl> // give the bucket time to fill up beyond threshold <nl> time.Sleep(time.Millisecond) <nl> - if !isSizeWithinTolerance(t, bucket.active.size, memtableThreshold, tolerance) { <nl> + if !isSizeWithinTolerance(t, bucket.active.Size(), memtableThreshold, tolerance) { <nl> t.Fatalf(\"memtable size (%d) was allowed to increase beyond threshold (%d) with tolerance of (%f)%%\", <nl> - bucket.active.size, memtableThreshold, tolerance) <nl> + bucket.active.Size(), memtableThreshold, tolerance) <nl> } else { <nl> done <- true <nl> } <nl> ", "msg": "use .Size() instead of .size in test to prevent a race condition"}
{"diff_id": 4076, "repo": "semi-technologies/weaviate", "sha": "71b3918e7b2b2b603cdcd7a30e63983f4f26c887", "time": "05.06.2022 20:35:51", "diff": "mmm a / adapters/repos/db/shard.go <nl> ppp b / adapters/repos/db/shard.go <nl>@@ -104,10 +104,13 @@ func NewShard(ctx context.Context, promMetrics *monitoring.PrometheusMetrics, <nl> case \"l2-squared\": <nl> distProv = distancer.NewL2SquaredProvider() <nl> // here I can add new cases for manhattan distance, minowski distance etc <nl> + case \"manhattan\": <nl> + // set distProv as distancer.NewManhattanProvide() <nl> + distProv = distancer.NewManhattanProvider() <nl> default: <nl> return nil, errors.Errorf(\"unrecognized distance metric %q,\"+ <nl> \"choose one of [\\\"cosine\\\", \\\"l2-squared\\\"]\", hnswUserConfig.Distance) <nl> - } <nl> + } // add manhanntan to this default as well <nl> vi, err := hnsw.New(hnsw.Config{ <nl> Logger: index.logger, <nl> ", "msg": "Add code to include manhattan distance"}
{"diff_id": 4079, "repo": "semi-technologies/weaviate", "sha": "cceff0082b44fcbd3ce98d93a897c9695355e5ca", "time": "16.06.2022 16:30:44", "diff": "mmm a / adapters/repos/db/crud.go <nl> ppp b / adapters/repos/db/crud.go <nl>@@ -167,7 +167,7 @@ func (d *DB) Exists(ctx context.Context, class string, id strfmt.UUID) (bool, er <nl> } <nl> index := d.GetIndex(schema.ClassName(class)) <nl> if index == nil { <nl> - return false, fmt.Errorf(\"index not found for class %s\", class) <nl> + return false, nil <nl> } <nl> return index.exists(ctx, id) <nl> } <nl> ", "msg": "do not return an error if index is not found while checking for object existence"}
{"diff_id": 4089, "repo": "semi-technologies/weaviate", "sha": "97cb3d8e14558f112e2456144bacfa1c58d21c76", "time": "27.06.2022 18:16:14", "diff": "mmm a / adapters/repos/db/lsmkv/bucket_threshold_test.go <nl> ppp b / adapters/repos/db/lsmkv/bucket_threshold_test.go <nl>@@ -222,6 +222,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> require.Nil(t, err) <nl> t.Run(\"assert no segments exist initially\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 0, len(bucket.disk.segments)) <nl> }) <nl> @@ -230,6 +233,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert no segments exist even after passing the idle threshold\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 0, len(bucket.disk.segments)) <nl> }) <nl> @@ -261,6 +267,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert no segments exist initially\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 0, len(bucket.disk.segments)) <nl> }) <nl> @@ -269,6 +278,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert that a flush has occurred (and one segment exists)\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 1, len(bucket.disk.segments)) <nl> }) <nl> @@ -300,6 +312,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert no segments exist initially\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 0, len(bucket.disk.segments)) <nl> }) <nl> @@ -317,6 +332,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert that no flushing has occurred\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 0, len(bucket.disk.segments)) <nl> }) <nl> @@ -325,6 +343,9 @@ func TestMemtableFlushesIfIdle(t *testing.T) { <nl> }) <nl> t.Run(\"assert that a flush has occurred (and one segment exists)\", func(t *testing.T) { <nl> + bucket.disk.maintenanceLock.RLock() <nl> + defer bucket.disk.maintenanceLock.RUnlock() <nl> + <nl> assert.Equal(t, 1, len(bucket.disk.segments)) <nl> }) <nl> ", "msg": "fix potential data race in integration tests"}
{"diff_id": 4094, "repo": "semi-technologies/weaviate", "sha": "a93613e7bd09c0e2850785866645c3090cb8e171", "time": "02.07.2022 10:49:00", "diff": "mmm a / adapters/repos/db/lsmkv/recover_from_wal_integration_test.go <nl> ppp b / adapters/repos/db/lsmkv/recover_from_wal_integration_test.go <nl>@@ -16,6 +16,7 @@ package lsmkv <nl> import ( <nl> \"bytes\" <nl> + \"context\" <nl> \"fmt\" <nl> \"io\" <nl> \"math/rand\" <nl> @@ -42,7 +43,7 @@ func TestReplaceStrategy_RecoverFromWAL(t *testing.T) { <nl> fmt.Println(err) <nl> }() <nl> - t.Run(\"without previous state\", func(t *testing.T) { <nl> + t.Run(\"without some previous state\", func(t *testing.T) { <nl> b, err := NewBucket(testCtx(), dirNameOriginal, nullLogger(), nil, <nl> WithStrategy(StrategyReplace)) <nl> require.Nil(t, err) <nl> @@ -50,6 +51,38 @@ func TestReplaceStrategy_RecoverFromWAL(t *testing.T) { <nl> // so big it effectively never triggers as part of this test <nl> b.SetMemtableThreshold(1e9) <nl> + t.Run(\"set one key that will be flushed orderly\", func(t *testing.T) { <nl> + // the motivation behind flushing this initial segment is to check that <nl> + // deletion as part of the recvoery also works correclty. If we would <nl> + // just delete something that was created as part of the same memtable, <nl> + // the tests would still pass, even with removing the logic that recovers <nl> + // tombstones. <nl> + // <nl> + // To make sure they fail in this case, this prior state was introduced. <nl> + // An entry with key \"key-2\" is introduced in a previous segment, so if <nl> + // the deletion fails as part of the recovery this key would still be <nl> + // present later on. With the deletion working correctly it will be gone. <nl> + // <nl> + // You can test this by commenting the \"p.memtable.setTombstone()\" line <nl> + // in p.doReplace(). This will fail the tests suite, but prior to this <nl> + // adddition it would have passed. <nl> + key2 := []byte(\"key-2\") <nl> + orig2 := []byte(\"delete me later - you should never find me again\") <nl> + <nl> + err = b.Put(key2, orig2) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> + t.Run(\"shutdown (orderly) bucket to create first segment\", func(t *testing.T) { <nl> + b.Shutdown(context.Background()) <nl> + <nl> + // then recreate bucket <nl> + var err error <nl> + b, err = NewBucket(testCtx(), dirNameOriginal, nullLogger(), nil, <nl> + WithStrategy(StrategyReplace)) <nl> + require.Nil(t, err) <nl> + }) <nl> + <nl> t.Run(\"set original values\", func(t *testing.T) { <nl> key1 := []byte(\"key-1\") <nl> key2 := []byte(\"key-2\") <nl> @@ -100,6 +133,7 @@ func TestReplaceStrategy_RecoverFromWAL(t *testing.T) { <nl> }) <nl> t.Run(\"copy state into recovery folder and destroy original\", func(t *testing.T) { <nl> + t.Run(\"copy over wals\", func(t *testing.T) { <nl> cmd := exec.Command(\"/bin/bash\", \"-c\", fmt.Sprintf(\"cp -r %s/*.wal %s\", <nl> dirNameOriginal, dirNameRecovered)) <nl> var out bytes.Buffer <nl> @@ -109,6 +143,19 @@ func TestReplaceStrategy_RecoverFromWAL(t *testing.T) { <nl> fmt.Println(out.String()) <nl> t.Fatal(err) <nl> } <nl> + }) <nl> + <nl> + t.Run(\"copy over segments\", func(t *testing.T) { <nl> + cmd := exec.Command(\"/bin/bash\", \"-c\", fmt.Sprintf(\"cp -r %s/*.db %s\", <nl> + dirNameOriginal, dirNameRecovered)) <nl> + var out bytes.Buffer <nl> + cmd.Stderr = &out <nl> + err := cmd.Run() <nl> + if err != nil { <nl> + fmt.Println(out.String()) <nl> + t.Fatal(err) <nl> + } <nl> + }) <nl> b = nil <nl> require.Nil(t, os.RemoveAll(dirNameOriginal)) <nl> }) <nl> ", "msg": "make wal recovery tests more robusts for deletes"}
{"diff_id": 4119, "repo": "semi-technologies/weaviate", "sha": "668ee1dd445d68a7c7dc0c065f6a6416daddcf90", "time": "25.07.2022 08:30:22", "diff": "mmm a / adapters/repos/db/lsmkv/red_black_tree_test.go <nl> ppp b / adapters/repos/db/lsmkv/red_black_tree_test.go <nl>package lsmkv <nl> import ( <nl> + \"fmt\" <nl> \"math\" <nl> + \"math/rand\" <nl> \"testing\" <nl> \"github.com/stretchr/testify/require\" <nl> @@ -107,8 +109,8 @@ func TestRbTrees(t *testing.T) { <nl> iByte := []byte{uint8(key)} <nl> tree.insert(iByte, iByte, nil) <nl> require.Empty(t, tree.root.parent) <nl> - require.True(t, ValidateRBTree(t, tree)) <nl> } <nl> + ValidateRBTree(t, tree) <nl> flattenTree := tree.flattenInOrder() <nl> require.Equal(t, len(tt.keys), len(flattenTree)) // no entries got lost <nl> @@ -132,7 +134,7 @@ func TestRbTrees(t *testing.T) { <nl> } <nl> // add keys as a) normal keys b) tombstone keys and c) half tombstone, half normal. <nl> -// The resulting trees must have the same order and colors <nl> +// The resulting (reblalanced) trees must have the same order and colors <nl> var tombstoneTests = []struct { <nl> name string <nl> keys []uint <nl> @@ -141,7 +143,8 @@ var tombstoneTests = []struct { <nl> {\"Rotate right around root\", []uint{61, 30, 10}}, <nl> {\"Multiple rotations along the tree and colour changes\", []uint{166, 92, 33, 133, 227, 236, 71, 183, 18, 139, 245, 161}}, <nl> {\"Ordered nodes increasing\", []uint{1, 2, 3, 4, 5, 6, 7, 8}}, <nl> - {\"Ordered nodes decreasing\", []uint{8, 7, 6, 5, 4, 3, 2, 1}}} <nl> + {\"Ordered nodes decreasing\", []uint{8, 7, 6, 5, 4, 3, 2, 1}}, <nl> +} <nl> func TestTombstones(t *testing.T) { <nl> for _, tt := range tombstoneTests { <nl> @@ -159,9 +162,9 @@ func TestTombstones(t *testing.T) { <nl> treeHalfHalf.setTombstone(iByte, nil) <nl> } <nl> } <nl> - require.True(t, ValidateRBTree(t, treeNormal)) <nl> - require.True(t, ValidateRBTree(t, treeTombstone)) <nl> - require.True(t, ValidateRBTree(t, treeHalfHalf)) <nl> + ValidateRBTree(t, treeNormal) <nl> + ValidateRBTree(t, treeTombstone) <nl> + ValidateRBTree(t, treeHalfHalf) <nl> treeNormalFlatten := treeNormal.flattenInOrder() <nl> treeTombstoneFlatten := treeTombstone.flattenInOrder() <nl> @@ -180,6 +183,37 @@ func TestTombstones(t *testing.T) { <nl> } <nl> } <nl> +type void struct{} <nl> + <nl> +var member void <nl> + <nl> +func TestRandomTrees(t *testing.T) { <nl> + setSeed(t) <nl> + tree := &binarySearchTree{} <nl> + amount := rand.Intn(100000) <nl> + keySize := rand.Intn(100) <nl> + uniqueKeys := make(map[string]void) <nl> + for i := 0; i < amount; i++ { <nl> + key := make([]byte, keySize) <nl> + rand.Read(key) <nl> + uniqueKeys[fmt.Sprint(key)] = member <nl> + if rand.Intn(5) == 1 { // add 20% of all entries as tombstone <nl> + tree.setTombstone(key, nil) <nl> + } else { <nl> + tree.insert(key, key, nil) <nl> + } <nl> + } <nl> + <nl> + // all added keys are still part of the tree <nl> + treeFlattened := tree.flattenInOrder() <nl> + require.Equal(t, len(uniqueKeys), len(treeFlattened)) <nl> + for _, entry := range treeFlattened { <nl> + _, ok := uniqueKeys[fmt.Sprint(entry.key)] <nl> + require.True(t, ok) <nl> + } <nl> + ValidateRBTree(t, tree) <nl> +} <nl> + <nl> func getIndexInSlice(reorderedKeys []uint, key []byte) int { <nl> for i, v := range reorderedKeys { <nl> if v == uint(key[0]) { <nl> @@ -189,20 +223,30 @@ func getIndexInSlice(reorderedKeys []uint, key []byte) int { <nl> return -1 <nl> } <nl> -// Checks if a tree could be an RB tree <nl> -func ValidateRBTree(t *testing.T, tree *binarySearchTree) bool { <nl> - if tree.root.colourIsred { <nl> - return false <nl> - } <nl> +// Checks if a tree is a RB tree <nl> +// <nl> +// There are several properties that valid RB trees follow: <nl> +// 1) The root node is always black <nl> +// 2) The max depth of a tree is 2* Log2(N+1), where N is the number of nodes <nl> +// 3) Every path from root to leave has the same number of _black_ nodes <nl> +// 4) Red nodes only have black (or nil) children <nl> +// <nl> +// In addition this also validates some general tree properties: <nl> +// - root has no parent <nl> +// - if node A is a child of B, B must be the parent of A) <nl> +func ValidateRBTree(t *testing.T, tree *binarySearchTree) { <nl> + require.False(t, tree.root.colourIsred) <nl> + require.True(t, tree.root.parent == nil) <nl> - treeDepth, nodeCount := MaxTreeDepthAndNodeCount(t, tree.root) <nl> + treeDepth, nodeCount, _ := WalkTree(t, tree.root) <nl> maxDepth := 2 * math.Log2(float64(nodeCount)+1) <nl> - return treeDepth <= int(maxDepth) <nl> + require.True(t, treeDepth <= int(maxDepth)) <nl> } <nl> -func MaxTreeDepthAndNodeCount(t *testing.T, node *binarySearchNode) (int, int) { <nl> +// Walks through the tree and counts the depth, number of nodes and number of black nodes <nl> +func WalkTree(t *testing.T, node *binarySearchNode) (int, int, int) { <nl> if node == nil { <nl> - return 0, 0 <nl> + return 0, 0, 0 <nl> } <nl> // validate parent/child connections <nl> @@ -213,17 +257,29 @@ func MaxTreeDepthAndNodeCount(t *testing.T, node *binarySearchNode) (int, int) { <nl> require.Equal(t, node.left.parent, node) <nl> } <nl> + // red nodes need black (or nil) children <nl> + if node.colourIsred { <nl> + require.True(t, node.left == nil || !node.left.colourIsred) <nl> + require.True(t, node.right == nil || !node.right.colourIsred) <nl> + } <nl> + <nl> + blackNode := int(1) <nl> + if node.colourIsred { <nl> + blackNode = 0 <nl> + } <nl> + <nl> if node.right == nil && node.left == nil { <nl> - return 1, 1 <nl> + return 1, 1, blackNode <nl> } <nl> - depthRight, nodeCountRight := MaxTreeDepthAndNodeCount(t, node.right) <nl> - depthLeft, nodeCountLeft := MaxTreeDepthAndNodeCount(t, node.left) <nl> + depthRight, nodeCountRight, blackNodesDepthRight := WalkTree(t, node.right) <nl> + depthLeft, nodeCountLeft, blackNodesDepthLeft := WalkTree(t, node.left) <nl> + require.Equal(t, blackNodesDepthRight, blackNodesDepthLeft) <nl> nodeCount := nodeCountLeft + nodeCountRight + 1 <nl> if depthRight > depthLeft { <nl> - return depthRight + 1, nodeCount <nl> + return depthRight + 1, nodeCount, blackNodesDepthRight + blackNode <nl> } else { <nl> - return depthLeft + 1, nodeCount <nl> + return depthLeft + 1, nodeCount, blackNodesDepthRight + blackNode <nl> } <nl> } <nl> ", "msg": "Add randomized red-black tree test and extend validation of trees"}
{"diff_id": 4126, "repo": "semi-technologies/weaviate", "sha": "49e00e23cdf9b05e7a77e493b2f9a8ba04ac532f", "time": "02.08.2022 21:55:19", "diff": "mmm a / adapters/repos/db/inverted/objects_test.go <nl> ppp b / adapters/repos/db/inverted/objects_test.go <nl>@@ -313,6 +313,7 @@ func TestAnalyzeObject(t *testing.T) { <nl> }) <nl> t.Run(\"with array properties\", func(t *testing.T) { <nl> + //nolint:gofumpt // the following block is formatted differently with go1.18 vs go1.19 <nl> schema := map[string]interface{}{ <nl> \"descriptions\": []interface{}{\"I am great!\", \"I am also great!\"}, <nl> \"emails\": []interface{}{\"john@doe.com\", \"john2@doe.com\"}, <nl> ", "msg": "temporarily ignore linting of code block that differs\nbetween 1.18 and 1.19\nThis can be removed when golangci-lint can run with go 1.19"}
{"diff_id": 4153, "repo": "semi-technologies/weaviate", "sha": "eb78d39cec5b653e9043c1f1da4e1bfa65c25675", "time": "28.08.2022 01:06:10", "diff": "mmm a / test/docker/compose.go <nl> ppp b / test/docker/compose.go <nl>@@ -28,6 +28,8 @@ const ( <nl> envTestText2vecContextionaryImage = \"TEST_TEXT2VEC_CONTEXTIONARY_IMAGE\" <nl> // envTestQnATransformersImage adds ability to pass a custom image to module tests <nl> envTestQnATransformersImage = \"TEST_QNA_TRANSFORMERS_IMAGE\" <nl> + // envTestSUMTransformersImage adds ability to pass a custom image to module tests <nl> + envTestSUMTransformersImage = \"TEST_SUM_TRANSFORMERS_IMAGE\" <nl> ) <nl> type Compose struct { <nl> @@ -39,6 +41,7 @@ type Compose struct { <nl> withContextionary bool <nl> withQnATransformers bool <nl> withWeaviate bool <nl> + withSUMTransformers bool <nl> } <nl> func New() *Compose { <nl> @@ -75,6 +78,12 @@ func (d *Compose) WithQnATransformers() *Compose { <nl> return d <nl> } <nl> +func (d *Compose) WithSUMTransformers() *Compose { <nl> + d.withSUMTransformers = true <nl> + d.enableModules = append(d.enableModules, SUMTransformers) <nl> + return d <nl> +} <nl> + <nl> func (d *Compose) WithWeaviate() *Compose { <nl> d.withWeaviate = true <nl> return d <nl> @@ -140,6 +149,17 @@ func (d *Compose) Start(ctx context.Context) (*DockerCompose, error) { <nl> } <nl> containers = append(containers, container) <nl> } <nl> + if d.withSUMTransformers { <nl> + image := os.Getenv(envTestSUMTransformersImage) <nl> + container, err := startSUMTransformers(ctx, networkName, image) <nl> + if err != nil { <nl> + return nil, errors.Wrapf(err, \"start %s\", SUMTransformers) <nl> + } <nl> + for k, v := range container.envSettings { <nl> + envSettings[k] = v <nl> + } <nl> + containers = append(containers, container) <nl> + } <nl> if d.withWeaviate { <nl> image := os.Getenv(envTestWeaviateImage) <nl> container, err := startWeaviate(ctx, d.enableModules, d.defaultVectorizerModule, <nl> ", "msg": "module test added for sum-transformer"}
{"diff_id": 4195, "repo": "semi-technologies/weaviate", "sha": "d26a93ce92030d4b7f43ce0a5a862034a5238843", "time": "05.10.2022 11:37:53", "diff": "mmm a / adapters/repos/db/vector/hnsw/vector_cache.go <nl> ppp b / adapters/repos/db/vector/hnsw/vector_cache.go <nl>@@ -242,6 +242,14 @@ func (c *shardedLockCache) obtainAllLocks() { <nl> // dimensionality <nl> func (c *shardedLockCache) dimensions(id uint64) int { <nl> c.shardedLocks[id%shardFactor].RLock() <nl> + if int(id) >= len(c.cache) { <nl> + // if an object has no vector it could have a higher doc id than the size <nl> + // of the vector index, in this case we can assume it has no dimensions <nl> + // (since it had no vector), but cannot actually check, as this would lead <nl> + // to an OOM error <nl> + c.shardedLocks[id%shardFactor].RUnlock() <nl> + return 0 <nl> + } <nl> vec := c.cache[id] <nl> c.shardedLocks[id%shardFactor].RUnlock() <nl> ", "msg": "handle out-of-range in temporary dimension logic"}
{"diff_id": 4199, "repo": "semi-technologies/weaviate", "sha": "b3fb5a4eb3315d16178914662caf3aa952d516ae", "time": "06.10.2022 22:07:21", "diff": "mmm a / modules/backup-s3/s3/s3.go <nl> ppp b / modules/backup-s3/s3/s3.go <nl>@@ -28,8 +28,6 @@ import ( <nl> ) <nl> const ( <nl> - AWS_ROLE_ARN = \"AWS_ROLE_ARN\" <nl> - AWS_WEB_IDENTITY_TOKEN_FILE = \"AWS_WEB_IDENTITY_TOKEN_FILE\" <nl> AWS_REGION = \"AWS_REGION\" <nl> AWS_DEFAULT_REGION = \"AWS_DEFAULT_REGION\" <nl> ) <nl> @@ -46,10 +44,12 @@ func New(config Config, logger logrus.FieldLogger, dataPath string) (*s3, error) <nl> if len(region) == 0 { <nl> region = os.Getenv(AWS_DEFAULT_REGION) <nl> } <nl> - creds := credentials.NewEnvAWS() <nl> - if len(os.Getenv(AWS_WEB_IDENTITY_TOKEN_FILE)) > 0 && len(os.Getenv(AWS_ROLE_ARN)) > 0 { <nl> - creds = credentials.NewIAM(\"\") <nl> + <nl> + creds := credentials.NewIAM(\"\") <nl> + if _, err := creds.Get(); err != nil { <nl> + creds = credentials.NewEnvAWS() <nl> } <nl> + <nl> client, err := minio.New(config.Endpoint(), &minio.Options{ <nl> Creds: creds, <nl> Region: region, <nl> ", "msg": "Add support for AWS IAM based authorization"}
{"diff_id": 4203, "repo": "semi-technologies/weaviate", "sha": "a77e18882885b2eb23d4ad26004f3edc1b6f666e", "time": "10.10.2022 12:21:34", "diff": "mmm a / adapters/repos/db/batch_integration_test.go <nl> ppp b / adapters/repos/db/batch_integration_test.go <nl>@@ -55,9 +55,8 @@ func TestBatchPutObjectsWithDimensions(t *testing.T) { <nl> require.Nil(t, err) <nl> defer repo.Shutdown(context.Background()) <nl> - migrator := NewMigrator(repo, logger) <nl> - t.Run(\"creating the thing class\", testAddBatchObjectClass(repo, migrator, schemaGetter)) <nl> + simpleInsertObjects(t, repo, \"ThingForBatching\", 201) <nl> dimBefore := GetDimensionsFromRepo(repo, \"ThingForBatching\") <nl> require.Equal(t, 0, dimBefore, \"Dimensions are empty before import\") <nl> @@ -302,12 +301,12 @@ func TestBatchDeleteObjects_JourneyWithDimensions(t *testing.T) { <nl> dimBefore := GetDimensionsFromRepo(repo, \"ThingForBatching\") <nl> require.Equal(t, 0, dimBefore, \"Dimensions are empty before import\") <nl> - t.Run(\"batch import things\", testBatchImportObjects(repo)) <nl> + simpleInsertObjects(t, repo, \"ThingForBatching\", 103) <nl> dimAfter := GetDimensionsFromRepo(repo, \"ThingForBatching\") <nl> require.Equal(t, 309, dimAfter, \"Dimensions are present before delete\") <nl> - t.Run(\"batch delete journey things\", testBatchDeleteObjectsJourney(repo, queryMaximumResults)) <nl> + simpleDeleteObjects(t, repo, \"ThingForBatching\", 103) <nl> dimFinal := GetDimensionsFromRepo(repo, \"ThingForBatching\") <nl> require.Equal(t, 0, dimFinal, \"Dimensions have been deleted\") <nl> ", "msg": "Fix more dimension tests"}
{"diff_id": 4206, "repo": "semi-technologies/weaviate", "sha": "0a04f218d618cf2d0a0ce2594f29cea4a55382d8", "time": "10.10.2022 13:55:46", "diff": "mmm a / adapters/repos/db/shard_dimension_tracking_test.go <nl> ppp b / adapters/repos/db/shard_dimension_tracking_test.go <nl>@@ -18,7 +18,6 @@ import ( <nl> \"context\" <nl> \"fmt\" <nl> \"math/rand\" <nl> - \"os\" <nl> \"testing\" <nl> \"time\" <nl> @@ -37,11 +36,7 @@ func Benchmark_Migration(b *testing.B) { <nl> for i := 0; i < b.N; i++ { <nl> rand.Seed(time.Now().UnixNano()) <nl> - dir := b.TempDir() <nl> - <nl> - defer os.RemoveAll(dir) <nl> - <nl> - dirName := os.TempDir() <nl> + dirName := b.TempDir() <nl> shardState := singleShardState() <nl> logger := logrus.New() <nl> ", "msg": "Better dirname"}
{"diff_id": 4216, "repo": "semi-technologies/weaviate", "sha": "60bcb3ce54678bb5e4ab9f5ef6149c778a6c4c0b", "time": "08.11.2022 11:28:47", "diff": "mmm a / usecases/schema/startup_cluster_sync.go <nl> ppp b / usecases/schema/startup_cluster_sync.go <nl>@@ -33,7 +33,11 @@ func (m *Manager) startupClusterSync(ctx context.Context, <nl> return m.startupHandleSingleNode(ctx, nodes) <nl> } <nl> - return nil <nl> + if isEmpty(localSchema) { <nl> + return m.startupJoinCluster(ctx, localSchema) <nl> + } <nl> + <nl> + return m.validateSchemaCorruption(ctx, localSchema) <nl> } <nl> // startupHandleSingleNode deals with the case where there is only a single <nl> @@ -62,6 +66,42 @@ func (m *Manager) startupHandleSingleNode(ctx context.Context, <nl> return nil <nl> } <nl> +// startupJoinCluster migrates the schema for a new node. The assumption is <nl> +// that other nodes have schema state and we need to migrate this schema to the <nl> +// local node transactionally. In other words, this startup process can not <nl> +// occur concurrently with a user-initiated schema update. One of those must <nl> +// fail. <nl> +// <nl> +// There is one edge case: The cluster could consist of multiple nodes which <nl> +// are empty. In this case, no migration is required. <nl> +func (m *Manager) startupJoinCluster(ctx context.Context, <nl> + localSchema *State, <nl> +) error { <nl> + return nil <nl> +} <nl> + <nl> +// validateSchemaCorruption makes sure that - given that all nodes in the <nl> +// cluster have a schema - they are in sync. If not the cluster is considered <nl> +// broken and needs to be repaired manually <nl> +func (m *Manager) validateSchemaCorruption(ctx context.Context, <nl> + localSchema *State, <nl> +) error { <nl> + // TODO <nl> + return nil <nl> +} <nl> + <nl> func logrusStartupSyncFields() logrus.Fields { <nl> return logrus.Fields{\"action\": \"startup_cluster_schema_sync\"} <nl> } <nl> + <nl> +func isEmpty(schema *State) bool { <nl> + if schema.ObjectSchema == nil { <nl> + return true <nl> + } <nl> + <nl> + if len(schema.ObjectSchema.Classes) == 0 { <nl> + return true <nl> + } <nl> + <nl> + return false <nl> +} <nl> ", "msg": "add empty shells for schema migration processes"}
{"diff_id": 4226, "repo": "semi-technologies/weaviate", "sha": "daf014a2608b46ff31b5a53823050a41a77531c7", "time": "09.11.2022 17:29:22", "diff": "mmm a / adapters/clients/cluster_schema.go <nl> ppp b / adapters/clients/cluster_schema.go <nl>@@ -15,11 +15,11 @@ import ( <nl> \"bytes\" <nl> \"context\" <nl> \"encoding/json\" <nl> + \"fmt\" <nl> \"io\" <nl> \"net/http\" <nl> \"net/url\" <nl> - \"github.com/pkg/errors\" <nl> \"github.com/semi-technologies/weaviate/usecases/cluster\" <nl> ) <nl> @@ -46,33 +46,54 @@ func (c *ClusterSchema) OpenTransaction(ctx context.Context, host string, <nl> jsonBytes, err := json.Marshal(pl) <nl> if err != nil { <nl> - return errors.Wrap(err, \"marshal transaction payload\") <nl> + return fmt.Errorf(\"marshal transaction payload: %w\", err) <nl> } <nl> req, err := http.NewRequestWithContext(ctx, method, url.String(), <nl> bytes.NewReader(jsonBytes)) <nl> if err != nil { <nl> - return errors.Wrap(err, \"open http request\") <nl> + return fmt.Errorf(\"open http request: %w\", err) <nl> } <nl> req.Header.Set(\"content-type\", \"application/json\") <nl> res, err := c.client.Do(req) <nl> if err != nil { <nl> - return errors.Wrap(err, \"send http request\") <nl> + return fmt.Errorf(\"send http request: %w\", err) <nl> } <nl> defer res.Body.Close() <nl> + body, _ := io.ReadAll(res.Body) <nl> if res.StatusCode != http.StatusCreated { <nl> if res.StatusCode == http.StatusConflict { <nl> return cluster.ErrConcurrentTransaction <nl> } <nl> - body, _ := io.ReadAll(res.Body) <nl> - return errors.Errorf(\"unexpected status code %d (%s)\", res.StatusCode, <nl> + return fmt.Errorf(\"unexpected status code %d (%s)\", res.StatusCode, <nl> body) <nl> } <nl> + // optional for backward-compatibility before v1.17 where only <nl> + // write-transcactions where supported. They had no return value other than <nl> + // the status code. With the introduction of read-transactions it is now <nl> + // possible to return the requsted value <nl> + if len(body) == 0 { <nl> + return nil <nl> + } <nl> + <nl> + var txRes txPayload <nl> + err = json.Unmarshal(body, &txRes) <nl> + if err != nil { <nl> + return fmt.Errorf(\"unexpected error unmarshalling tx response: %w\", err) <nl> + } <nl> + <nl> + if tx.ID != txRes.ID { <nl> + return fmt.Errorf(\"unexpected mismatch between outgoing and incoming tx ids:\"+ <nl> + \"%s vs %s\", tx.ID, txRes.ID) <nl> + } <nl> + <nl> + tx.Payload = txRes.Payload <nl> + <nl> return nil <nl> } <nl> @@ -85,17 +106,17 @@ func (c *ClusterSchema) AbortTransaction(ctx context.Context, host string, <nl> req, err := http.NewRequestWithContext(ctx, method, url.String(), nil) <nl> if err != nil { <nl> - return errors.Wrap(err, \"open http request\") <nl> + return fmt.Errorf(\"open http request: %w\", err) <nl> } <nl> res, err := c.client.Do(req) <nl> if err != nil { <nl> - return errors.Wrap(err, \"send http request\") <nl> + return fmt.Errorf(\"send http request: %w\", err) <nl> } <nl> defer res.Body.Close() <nl> if res.StatusCode != http.StatusNoContent { <nl> - return errors.Errorf(\"unexpected status code %d\", res.StatusCode) <nl> + return fmt.Errorf(\"unexpected status code %d\", res.StatusCode) <nl> } <nl> return nil <nl> @@ -110,17 +131,17 @@ func (c *ClusterSchema) CommitTransaction(ctx context.Context, host string, <nl> req, err := http.NewRequestWithContext(ctx, method, url.String(), nil) <nl> if err != nil { <nl> - return errors.Wrap(err, \"open http request\") <nl> + return fmt.Errorf(\"open http request: %w\", err) <nl> } <nl> res, err := c.client.Do(req) <nl> if err != nil { <nl> - return errors.Wrap(err, \"send http request\") <nl> + return fmt.Errorf(\"send http request: %w\", err) <nl> } <nl> defer res.Body.Close() <nl> if res.StatusCode != http.StatusNoContent { <nl> - return errors.Errorf(\"unexpected status code %d\", res.StatusCode) <nl> + return fmt.Errorf(\"unexpected status code %d\", res.StatusCode) <nl> } <nl> return nil <nl> ", "msg": "[ci skip] update schema tx client for bi-directionality\nalso add missing tests"}
{"diff_id": 4227, "repo": "semi-technologies/weaviate", "sha": "3ad6bdabcc0d343221ef99161670e03abc58f54f", "time": "09.11.2022 18:19:14", "diff": "mmm a / usecases/cluster/state.go <nl> ppp b / usecases/cluster/state.go <nl>@@ -36,8 +36,6 @@ func Init(userConfig Config, logger logrus.FieldLogger) (*State, error) { <nl> cfg := memberlist.DefaultLocalConfig() <nl> cfg.LogOutput = newLogParser(logger) <nl> - cfg.AdvertiseAddr = \"127.0.0.1\" <nl> - <nl> if userConfig.Hostname != \"\" { <nl> cfg.Name = userConfig.Hostname <nl> } <nl> ", "msg": "Remove temp hack to get Weaviate running without internet access"}
{"diff_id": 4236, "repo": "semi-technologies/weaviate", "sha": "cdf7c02d12d84fe5d85ea14282e29161618fc5ec", "time": "27.11.2022 16:41:32", "diff": "mmm a / adapters/repos/db/lsmkv/segment_bloom_filters.go <nl> ppp b / adapters/repos/db/lsmkv/segment_bloom_filters.go <nl>@@ -82,7 +82,7 @@ func (ind *segment) storeBloomFilterOnDisk() error { <nl> _, err := ind.bloomFilter.WriteTo(buf) <nl> if err != nil { <nl> - return fmt.Errorf(\"write bloom filter to disk: %w\", err) <nl> + return fmt.Errorf(\"write bloom filter: %w\", err) <nl> } <nl> data := buf.Bytes() <nl> @@ -97,6 +97,10 @@ func (ind *segment) storeBloomFilterOnDisk() error { <nl> return fmt.Errorf(\"write checkusm to file: %w\", err) <nl> } <nl> + if _, err := f.Write(data); err != nil { <nl> + return fmt.Errorf(\"write bloom filter to disk: %w\", err) <nl> + } <nl> + <nl> if err := f.Close(); err != nil { <nl> return fmt.Errorf(\"close bloom filter file: %w\", err) <nl> } <nl> @@ -144,7 +148,17 @@ func (ind *segment) initSecondaryBloomFilter(pos int) error { <nl> } <nl> if ok { <nl> - return ind.loadBloomFilterSecondaryFromDisk(pos) <nl> + err = ind.loadBloomFilterSecondaryFromDisk(pos) <nl> + if err == nil { <nl> + return nil <nl> + } <nl> + <nl> + if err != ErrInvalidChecksum { <nl> + // not a recoverable error <nl> + return err <nl> + } <nl> + <nl> + // now continue re-calculating <nl> } <nl> keys, err := ind.secondaryIndices[pos].AllKeys() <nl> @@ -171,13 +185,25 @@ func (ind *segment) initSecondaryBloomFilter(pos int) error { <nl> } <nl> func (ind *segment) storeBloomFilterSecondaryOnDisk(pos int) error { <nl> + buf := new(bytes.Buffer) <nl> + _, err := ind.secondaryBloomFilters[pos].WriteTo(buf) <nl> + if err != nil { <nl> + return fmt.Errorf(\"write bloom filter: %w\", err) <nl> + } <nl> + <nl> + data := buf.Bytes() <nl> + chksm := crc32.ChecksumIEEE(data) <nl> + <nl> f, err := os.Create(ind.bloomFilterSecondaryPath(pos)) <nl> if err != nil { <nl> return fmt.Errorf(\"open file for writing: %w\", err) <nl> } <nl> - _, err = ind.secondaryBloomFilters[pos].WriteTo(f) <nl> - if err != nil { <nl> + if err := binary.Write(f, binary.LittleEndian, chksm); err != nil { <nl> + return fmt.Errorf(\"write checkusm to file: %w\", err) <nl> + } <nl> + <nl> + if _, err := f.Write(data); err != nil { <nl> return fmt.Errorf(\"write bloom filter to disk: %w\", err) <nl> } <nl> @@ -194,8 +220,19 @@ func (ind *segment) loadBloomFilterSecondaryFromDisk(pos int) error { <nl> return fmt.Errorf(\"open file for reading: %w\", err) <nl> } <nl> + buf := new(bytes.Buffer) <nl> + if _, err := buf.ReadFrom(f); err != nil { <nl> + return fmt.Errorf(\"read bloom filter data from file: %w\", err) <nl> + } <nl> + data := buf.Bytes() <nl> + chcksm := binary.LittleEndian.Uint32(data[:4]) <nl> + actual := crc32.ChecksumIEEE(data[4:]) <nl> + if chcksm != actual { <nl> + return ErrInvalidChecksum <nl> + } <nl> + <nl> ind.secondaryBloomFilters[pos] = new(bloom.BloomFilter) <nl> - _, err = ind.secondaryBloomFilters[pos].ReadFrom(f) <nl> + _, err = ind.secondaryBloomFilters[pos].ReadFrom(bytes.NewReader(data[4:])) <nl> if err != nil { <nl> return fmt.Errorf(\"read bloom filter from disk: %w\", err) <nl> } <nl> ", "msg": "add checksums to secondary bloom filters, add tests"}
{"diff_id": 4240, "repo": "semi-technologies/weaviate", "sha": "bd5f2c1e559141d443db5f782bdc0d31ad490f21", "time": "05.12.2022 11:07:50", "diff": "mmm a / usecases/schema/startup_cluster_sync_test.go <nl> ppp b / usecases/schema/startup_cluster_sync_test.go <nl>@@ -13,6 +13,7 @@ package schema <nl> import ( <nl> \"encoding/json\" <nl> + \"fmt\" <nl> \"testing\" <nl> \"github.com/semi-technologies/weaviate/entities/models\" <nl> @@ -150,6 +151,49 @@ func TestStartupSync(t *testing.T) { <nl> localSchema := sm.GetSchemaSkipAuth() <nl> assert.Equal(t, \"GutenTag\", localSchema.FindClassByName(\"GutenTag\").Class) <nl> }) <nl> + <nl> + t.Run(\"new node joining, other nodes include an outdated version\", func(t *testing.T) { <nl> + clusterState := &fakeClusterState{ <nl> + hosts: []string{\"node1\", \"node2\"}, <nl> + } <nl> + <nl> + txClient := &fakeTxClient{ <nl> + openErr: fmt.Errorf(\"unrecognized schema transaction type\"), <nl> + } <nl> + <nl> + sm, err := newManagerWithClusterAndTx(t, clusterState, txClient, nil) <nl> + require.Nil(t, err) // no error, sync was skipped <nl> + <nl> + schema := sm.GetSchemaSkipAuth() <nl> + assert.Len(t, schema.Objects.Classes, 0, \"schema is still empty\") <nl> + }) <nl> + <nl> + t.Run(\"node with data (re-)joining, but other nodes are too old\", func(t *testing.T) { <nl> + // we expect that sync would be skipped beacause the other nodes can't take <nl> + // part in the sync <nl> + clusterState := &fakeClusterState{ <nl> + hosts: []string{\"node1\", \"node2\"}, <nl> + } <nl> + <nl> + txClient := &fakeTxClient{ <nl> + openErr: fmt.Errorf(\"unrecognized schema transaction type\"), <nl> + } <nl> + <nl> + sm, err := newManagerWithClusterAndTx(t, clusterState, txClient, &State{ <nl> + ObjectSchema: &models.Schema{ <nl> + Classes: []*models.Class{ <nl> + { <nl> + Class: \"Hola\", <nl> + VectorIndexType: \"hnsw\", <nl> + }, <nl> + }, <nl> + }, <nl> + }) <nl> + require.Nil(t, err) // startup sync was skipped, no error <nl> + schema := sm.GetSchemaSkipAuth() <nl> + require.Len(t, schema.Objects.Classes, 1, \"schema is still the local schema\") <nl> + assert.Equal(t, \"Hola\", schema.Objects.Classes[0].Class) <nl> + }) <nl> } <nl> func TestStartupSyncUnhappyPaths(t *testing.T) { <nl> ", "msg": "test cases where cluster sync is impossible due to version conflict"}
{"diff_id": 4253, "repo": "semi-technologies/weaviate", "sha": "40a2d361b90ce5e3e559e257cb00ddb237a3ebf1", "time": "16.12.2022 09:43:07", "diff": "mmm a / adapters/clients/replication.go <nl> ppp b / adapters/clients/replication.go <nl>@@ -44,18 +44,18 @@ func (c *replicationClient) PutObject(ctx context.Context, host, index, <nl> shard, requestID string, obj *storobj.Object, <nl> ) (replica.SimpleResponse, error) { <nl> var resp replica.SimpleResponse <nl> - payload, err := clusterapi.IndicesPayloads.SingleObject.Marshal(obj) <nl> + body, err := clusterapi.IndicesPayloads.SingleObject.Marshal(obj) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"encode request: %w\", err) <nl> } <nl> - req, err := newHttpReplicaRequest(ctx, http.MethodPost, host, index, shard, requestID, \"\", bytes.NewReader(payload)) <nl> + req, err := newHttpReplicaRequest(ctx, http.MethodPost, host, index, shard, requestID, \"\", nil) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> clusterapi.IndicesPayloads.SingleObject.SetContentTypeHeaderReq(req) <nl> - err = c.do(c.timeoutUnit*10, req, &resp) <nl> + err = c.do(c.timeoutUnit*10, req, body, &resp) <nl> return resp, err <nl> } <nl> @@ -68,7 +68,7 @@ func (c *replicationClient) DeleteObject(ctx context.Context, host, index, <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> - err = c.do(c.timeoutUnit*2, req, &resp) <nl> + err = c.do(c.timeoutUnit*2, req, nil, &resp) <nl> return resp, err <nl> } <nl> @@ -80,13 +80,13 @@ func (c *replicationClient) PutObjects(ctx context.Context, host, index, <nl> if err != nil { <nl> return resp, fmt.Errorf(\"encode request: %w\", err) <nl> } <nl> - req, err := newHttpReplicaRequest(ctx, http.MethodPost, host, index, shard, requestID, \"\", bytes.NewReader(body)) <nl> + req, err := newHttpReplicaRequest(ctx, http.MethodPost, host, index, shard, requestID, \"\", nil) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> clusterapi.IndicesPayloads.ObjectList.SetContentTypeHeaderReq(req) <nl> - err = c.do(c.timeoutUnit*10, req, &resp) <nl> + err = c.do(c.timeoutUnit*10, req, body, &resp) <nl> return resp, err <nl> } <nl> @@ -100,13 +100,13 @@ func (c *replicationClient) MergeObject(ctx context.Context, host, index, shard, <nl> } <nl> req, err := newHttpReplicaRequest(ctx, http.MethodPatch, host, index, shard, <nl> - requestID, doc.ID.String(), bytes.NewReader(body)) <nl> + requestID, doc.ID.String(), nil) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> clusterapi.IndicesPayloads.MergeDoc.SetContentTypeHeaderReq(req) <nl> - err = c.do(c.timeoutUnit*2, req, &resp) <nl> + err = c.do(c.timeoutUnit*2, req, body, &resp) <nl> return resp, err <nl> } <nl> @@ -119,14 +119,13 @@ func (c *replicationClient) AddReferences(ctx context.Context, host, index, <nl> return resp, fmt.Errorf(\"encode request: %w\", err) <nl> } <nl> req, err := newHttpReplicaRequest(ctx, http.MethodPost, host, index, shard, <nl> - requestID, \"references\", <nl> - bytes.NewReader(body)) <nl> + requestID, \"references\", nil) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> clusterapi.IndicesPayloads.ReferenceList.SetContentTypeHeaderReq(req) <nl> - err = c.do(c.timeoutUnit*10, req, &resp) <nl> + err = c.do(c.timeoutUnit*10, req, body, &resp) <nl> return resp, err <nl> } <nl> @@ -137,13 +136,13 @@ func (c *replicationClient) DeleteObjects(ctx context.Context, host, index, shar <nl> if err != nil { <nl> return resp, fmt.Errorf(\"encode request: %w\", err) <nl> } <nl> - req, err := newHttpReplicaRequest(ctx, http.MethodDelete, host, index, shard, requestID, \"\", bytes.NewReader(body)) <nl> + req, err := newHttpReplicaRequest(ctx, http.MethodDelete, host, index, shard, requestID, \"\", nil) <nl> if err != nil { <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> clusterapi.IndicesPayloads.BatchDeleteParams.SetContentTypeHeaderReq(req) <nl> - err = c.do(c.timeoutUnit*10, req, &resp) <nl> + err = c.do(c.timeoutUnit*10, req, body, &resp) <nl> return resp, err <nl> } <nl> @@ -154,7 +153,7 @@ func (c *replicationClient) Commit(ctx context.Context, host, index, shard strin <nl> return fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> - return c.do(c.timeoutUnit*64, req, resp) <nl> + return c.do(c.timeoutUnit*64, req, nil, resp) <nl> } <nl> func (c *replicationClient) Abort(ctx context.Context, host, index, shard, requestID string) ( <nl> @@ -165,7 +164,7 @@ func (c *replicationClient) Abort(ctx context.Context, host, index, shard, reque <nl> return resp, fmt.Errorf(\"create http request: %w\", err) <nl> } <nl> - err = c.do(c.timeoutUnit, req, &resp) <nl> + err = c.do(c.timeoutUnit, req, nil, &resp) <nl> return resp, err <nl> } <nl> @@ -191,16 +190,9 @@ func newHttpReplicaCMD(host, cmd, index, shard, requestId string, body io.Reader <nl> return http.NewRequest(http.MethodPost, url.String(), body) <nl> } <nl> -func (c *replicationClient) do(timeout time.Duration, req *http.Request, resp interface{}) (err error) { <nl> +func (c *replicationClient) do(timeout time.Duration, req *http.Request, body []byte, resp interface{}) (err error) { <nl> ctx, cancel := context.WithTimeout(req.Context(), timeout) <nl> defer cancel() <nl> - <nl> - // we have to save the req body contents (if present) <nl> - // prior to the first request. otherwise, the first <nl> - // request reads and closes the request body, and all <nl> - // future retries fail <nl> - body := readRequestBody(req) <nl> - <nl> try := func(ctx context.Context) (bool, error) { <nl> req.Body = io.NopCloser(bytes.NewReader(body)) <nl> res, err := c.client.Do(req) <nl> @@ -209,11 +201,9 @@ func (c *replicationClient) do(timeout time.Duration, req *http.Request, resp in <nl> } <nl> defer res.Body.Close() <nl> - if res.StatusCode == http.StatusServiceUnavailable { <nl> - return true, fmt.Errorf(\"host %q is unavailable\", req.Host) <nl> - } <nl> if code := res.StatusCode; code != http.StatusOK { <nl> - shouldRetry := code == http.StatusInternalServerError || code == http.StatusTooManyRequests <nl> + shouldRetry := code == http.StatusInternalServerError || <nl> + code == http.StatusTooManyRequests || code == http.StatusServiceUnavailable <nl> return shouldRetry, fmt.Errorf(\"status code: %v\", code) <nl> } <nl> if err := json.NewDecoder(res.Body).Decode(resp); err != nil { <nl> @@ -229,12 +219,3 @@ func (c *replicationClient) do(timeout time.Duration, req *http.Request, resp in <nl> func backOff(d time.Duration) time.Duration { <nl> return time.Duration(float64(d.Nanoseconds()*2) * (0.5 + rand.Float64())) <nl> } <nl> - <nl> -func readRequestBody(req *http.Request) []byte { <nl> - var body []byte <nl> - if req.Body != nil { <nl> - body, _ = io.ReadAll(req.Body) <nl> - req.Body.Close() <nl> - } <nl> - return body <nl> -} <nl> ", "msg": "Replication: no need to copy request body to make request retryable"}
{"diff_id": 4260, "repo": "semi-technologies/weaviate", "sha": "e927411585a5ad4df7ad5d1924a1b7d890e3c44a", "time": "20.12.2022 21:11:32", "diff": "mmm a / usecases/schema/get.go <nl> ppp b / usecases/schema/get.go <nl>@@ -27,8 +27,10 @@ func (m *Manager) GetSchema(principal *models.Principal) (schema.Schema, error) <nl> return schema.Schema{}, err <nl> } <nl> + m.Lock() <nl> + defer m.Unlock() <nl> return schema.Schema{ <nl> - Objects: m.state.ObjectSchema, <nl> + Objects: m.state.ObjectSchema.Deepcopy(), <nl> }, nil <nl> } <nl> ", "msg": "Add locking for getting schema with public call"}
{"diff_id": 4262, "repo": "semi-technologies/weaviate", "sha": "beb5b8c2e595a4e79c0370f4b2d4cafe7e81528a", "time": "21.12.2022 10:32:53", "diff": "mmm a / usecases/schema/get.go <nl> ppp b / usecases/schema/get.go <nl>@@ -54,6 +54,8 @@ func (m *Manager) getSchema() schema.Schema { <nl> } <nl> func (m *Manager) IndexedInverted(className, propertyName string) bool { <nl> + m.Lock() <nl> + defer m.Unlock() <nl> class := m.getClassByName(className) <nl> if class == nil { <nl> return false <nl> ", "msg": "Add lock to public schemanager function"}
{"diff_id": 4267, "repo": "semi-technologies/weaviate", "sha": "1ab1f7191c2095ddaa157155a8db8beb4aa3e3ed", "time": "19.01.2023 10:29:21", "diff": "mmm a / adapters/handlers/rest/middlewares.go <nl> ppp b / adapters/handlers/rest/middlewares.go <nl>@@ -96,7 +96,7 @@ func makeSetupGlobalMiddleware(appState *state.State) func(http.Handler) http.Ha <nl> handler = makeAddMonitoring(appState.Metrics)(handler) <nl> } <nl> handler = addPreflight(handler) <nl> - handler = addLiveAndReadyness(handler) <nl> + handler = addLiveAndReadyness(appState, handler) <nl> handler = addHandleRoot(handler) <nl> handler = makeAddModuleHandlers(appState.Modules)(handler) <nl> handler = addInjectHeadersIntoContext(handler) <nl> @@ -171,7 +171,7 @@ func addInjectHeadersIntoContext(next http.Handler) http.Handler { <nl> }) <nl> } <nl> -func addLiveAndReadyness(next http.Handler) http.Handler { <nl> +func addLiveAndReadyness(state *state.State, next http.Handler) http.Handler { <nl> return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { <nl> if r.URL.String() == \"/v1/.well-known/live\" { <nl> w.WriteHeader(http.StatusOK) <nl> @@ -179,7 +179,11 @@ func addLiveAndReadyness(next http.Handler) http.Handler { <nl> } <nl> if r.URL.String() == \"/v1/.well-known/ready\" { <nl> - w.WriteHeader(http.StatusOK) <nl> + code := http.StatusServiceUnavailable <nl> + if state.DB.StartupComplete() && state.Cluster.ClusterHealthScore() == 0 { <nl> + code = http.StatusOK <nl> + } <nl> + w.WriteHeader(code) <nl> return <nl> } <nl> ", "msg": "DB: change readiness prob endpoint to return success only when DB completes startup and internal member-list is healthy\nThis will prevent k8s from sending traffic to the pod while DB is not ready"}
{"diff_id": 4284, "repo": "vmware-tanzu/sonobuoy", "sha": "4850ed0426e750789d4932743f8b68196bd73322", "time": "03.01.2018 16:56:50", "diff": "mmm a / pkg/results/reader.go <nl> ppp b / pkg/results/reader.go <nl>@@ -5,6 +5,7 @@ import ( <nl> \"bytes\" <nl> \"compress/gzip\" <nl> \"encoding/json\" <nl> + \"encoding/xml\" <nl> \"errors\" <nl> \"io\" <nl> \"os\" <nl> @@ -155,8 +156,20 @@ func ExtractIntoStruct(predicate func(string) bool, path string, info os.FileInf <nl> if !ok { <nl> return errors.New(\"info.Sys() is not a reader\") <nl> } <nl> + // TODO(chuckha) there must be a better way <nl> + if strings.HasSuffix(path, \"xml\") { <nl> + decoder := xml.NewDecoder(reader) <nl> + err := decoder.Decode(object) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + return nil <nl> + } <nl> + <nl> + // If it's not xml it's probably json <nl> decoder := json.NewDecoder(reader) <nl> err := decoder.Decode(object) <nl> + <nl> if err != nil { <nl> return err <nl> } <nl> ", "msg": "Detect type to decode"}
{"diff_id": 4292, "repo": "vmware-tanzu/sonobuoy", "sha": "0052f5c67857f04f826fed03f641949813537817", "time": "27.02.2018 16:26:18", "diff": "mmm a / pkg/discovery/discovery.go <nl> ppp b / pkg/discovery/discovery.go <nl>@@ -61,6 +61,10 @@ func Run(kubeClient kubernetes.Interface, cfg *config.Config) (errCount int) { <nl> logrus.AddHook(hook) <nl> + // Unset all hooks as we exit the Run function <nl> + defer func() { <nl> + logrus.StandardLogger().Hooks = make(logrus.LevelHooks) <nl> + }() <nl> // closure used to collect and report errors. <nl> trackErrorsFor := func(action string) func(error) { <nl> return func(err error) { <nl> ", "msg": "Remove logrus hooks at the end of the run.\nSonobuoy has captured everything interesting and has created the results.\nNo more logs should be written to disk at this point."}
{"diff_id": 4299, "repo": "vmware-tanzu/sonobuoy", "sha": "25d62bf7469c0a683e4f88c41bf9b2da29e036de", "time": "08.03.2018 15:16:33", "diff": "mmm a / pkg/templates/manifest.go <nl> ppp b / pkg/templates/manifest.go <nl>@@ -38,7 +38,7 @@ kind: ClusterRoleBinding <nl> metadata: <nl> labels: <nl> component: sonobuoy <nl> - name: sonobuoy-serviceaccount <nl> + name: sonobuoy-serviceaccount-{{.Namespace}} <nl> roleRef: <nl> apiGroup: rbac.authorization.k8s.io <nl> kind: ClusterRole <nl> ", "msg": "Namespace serviceaccount\nFixes The issue was that serviceaccounts would not be created by new runs,\nbut would still point to a single namespace."}
{"diff_id": 4310, "repo": "vmware-tanzu/sonobuoy", "sha": "2d365052f89c92c29de5d54626563c5eb5834060", "time": "26.03.2018 11:54:37", "diff": "mmm a / None <nl> ppp b / cmd/sonobuoy/app/gen_plugin.go <nl>+/* <nl> +Copyright 2018 Heptio Inc. <nl> + <nl> +Licensed under the Apache License, Version 2.0 (the \"License\"); <nl> +you may not use this file except in compliance with the License. <nl> +You may obtain a copy of the License at <nl> + <nl> + http://www.apache.org/licenses/LICENSE-2.0 <nl> + <nl> +Unless required by applicable law or agreed to in writing, software <nl> +distributed under the License is distributed on an \"AS IS\" BASIS, <nl> +WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. <nl> +See the License for the specific language governing permissions and <nl> +limitations under the License. <nl> +*/ <nl> + <nl> +package app <nl> + <nl> +import ( <nl> + \"crypto/ecdsa\" <nl> + \"crypto/elliptic\" <nl> + \"crypto/rand\" <nl> + \"crypto/tls\" <nl> + \"crypto/x509\" <nl> + \"fmt\" <nl> + \"math/big\" <nl> + \"os\" <nl> + <nl> + \"github.com/heptio/sonobuoy/pkg/config\" <nl> + \"github.com/heptio/sonobuoy/pkg/errlog\" <nl> + \"github.com/heptio/sonobuoy/pkg/plugin\" <nl> + \"github.com/heptio/sonobuoy/pkg/plugin/loader\" <nl> + \"github.com/pkg/errors\" <nl> + \"github.com/spf13/cobra\" <nl> +) <nl> + <nl> +const ( <nl> + placeholderHostname = \"<hostname>\" <nl> +) <nl> + <nl> +// GenPluginConfig are the input options for running <nl> +type GenPluginConfig struct { <nl> + Paths []string <nl> + PluginName string <nl> +} <nl> + <nl> +var genPluginOpts GenPluginConfig <nl> + <nl> +func init() { <nl> + cmd := &cobra.Command{ <nl> + Use: \"plugin\", <nl> + Short: \"Generates the manifest Sonobuoy uses to run a worker for the given plugin\", <nl> + Run: genPluginManifest, <nl> + Hidden: true, <nl> + Args: cobra.ExactArgs(1), <nl> + } <nl> + <nl> + GenCommand.PersistentFlags().StringArrayVarP( <nl> + &genPluginOpts.Paths, \"paths\", \"p\", []string{\".\", \"./examples/plugins.d/\"}, <nl> + \"the paths to search for the plugins in. Defaults to . and ./plugins.d/\", <nl> + ) <nl> + // TODO: Other options? <nl> + GenCommand.AddCommand(cmd) <nl> +} <nl> + <nl> +func genPluginManifest(cmd *cobra.Command, args []string) { <nl> + genPluginOpts.PluginName = args[0] <nl> + code := 0 <nl> + manifest, err := generatePluginManifest(genPluginOpts) <nl> + if err == nil { <nl> + fmt.Printf(\"%s\\n\", manifest) <nl> + } else { <nl> + errlog.LogError(errors.Wrap(err, \"error attempting to generate sonobuoy manifest\")) <nl> + code = 1 <nl> + } <nl> + os.Exit(code) <nl> +} <nl> + <nl> +func generatePluginManifest(cfg GenPluginConfig) ([]byte, error) { <nl> + plugins, err := loader.LoadAllPlugins( <nl> + config.DefaultNamespace, <nl> + config.DefaultImage, <nl> + \"Always\", <nl> + cfg.Paths, <nl> + []plugin.Selection{{Name: cfg.PluginName}}, <nl> + ) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + <nl> + if len(plugins) != 1 { <nl> + return nil, fmt.Errorf(\"expected 1 plugin, got %v\", len(plugins)) <nl> + } <nl> + <nl> + cert, err := genCert() <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + <nl> + return plugins[0].FillTemplate(placeholderHostname, cert) <nl> +} <nl> + <nl> +func genCert() (*tls.Certificate, error) { <nl> + privKey, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader) <nl> + if err != nil { <nl> + return nil, errors.Wrap(err, \"couldn't generate private key\") <nl> + } <nl> + tmpl := &x509.Certificate{ <nl> + SerialNumber: big.NewInt(0), <nl> + } <nl> + certDER, err := x509.CreateCertificate(rand.Reader, tmpl, tmpl, &privKey.PublicKey, privKey) <nl> + if err != nil { <nl> + return nil, errors.Wrap(err, \"couldn't create certificate\") <nl> + } <nl> + <nl> + return &tls.Certificate{ <nl> + Certificate: [][]byte{certDER}, <nl> + PrivateKey: privKey, <nl> + }, nil <nl> +} <nl> ", "msg": "Re-add `sonobuoy gen plugin`.\nSuper helpful for debugging plugin generation."}
{"diff_id": 4324, "repo": "vmware-tanzu/sonobuoy", "sha": "152531c02b0a28ca0600944b1f2f9088f4d3085f", "time": "22.02.2019 20:36:43", "diff": "mmm a / pkg/plugin/driver/daemonset/daemonset.go <nl> ppp b / pkg/plugin/driver/daemonset/daemonset.go <nl>@@ -102,7 +102,7 @@ func (p *Plugin) Run(kubeclient kubernetes.Interface, hostname string, cert *tls <nl> return errors.Wrap(err, \"couldn't fill template\") <nl> } <nl> if err := kuberuntime.DecodeInto(scheme.Codecs.UniversalDecoder(), b, &daemonSet); err != nil { <nl> - return errors.Wrapf(err, \"could not decode the executed template into a daemonset. Plugin name: \", p.GetName()) <nl> + return errors.Wrapf(err, \"could not decode the executed template into a daemonset. Plugin name: %v\", p.GetName()) <nl> } <nl> secret, err := p.MakeTLSSecret(cert) <nl> ", "msg": "Fix go vet error in daemonset.go"}
{"diff_id": 4331, "repo": "vmware-tanzu/sonobuoy", "sha": "d34e9cf1558766473e312526e887c0559915af0d", "time": "22.05.2019 15:48:16", "diff": "mmm a / pkg/discovery/queries.go <nl> ppp b / pkg/discovery/queries.go <nl>@@ -137,6 +137,11 @@ func QueryResources( <nl> ns *string, <nl> cfg *config.Config) error { <nl> + // Early exit; avoid forming query or creating output directories. <nl> + if len(resources) == 0 { <nl> + return nil <nl> + } <nl> + <nl> if ns != nil { <nl> logrus.Infof(\"Running ns query (%v)\", *ns) <nl> } else { <nl> ", "msg": "Don't create resource directories if not querying for resources\nOrdering issue in the logic caused the paths to be created even\nif the queries were noops. This adds an empty-check before\ncreating the directories at all.\nRelated to"}
{"diff_id": 4332, "repo": "vmware-tanzu/sonobuoy", "sha": "98d7d32e7311c7222a395142d13b2f8f1f8f5cab", "time": "04.06.2019 16:15:52", "diff": "mmm a / pkg/plugin/aggregation/aggregator.go <nl> ppp b / pkg/plugin/aggregation/aggregator.go <nl>@@ -53,11 +53,33 @@ type Aggregator struct { <nl> // resultEvents is a channel that is written to when results are seen <nl> // by the server, so we can block until we're done. <nl> resultEvents chan *plugin.Result <nl> + <nl> // resultsMutex prevents race conditions if two identical results <nl> // come in at the same time. <nl> resultsMutex sync.Mutex <nl> } <nl> +// httpError is an internal error type which allows us to unify result processing <nl> +// across http and non-http flows. <nl> +type httpError struct { <nl> + err error <nl> + code int <nl> +} <nl> + <nl> +// HttpCode returns the http code associated with the error or an InternalServerError <nl> +// if none is set. <nl> +func (e *httpError) HttpCode() int { <nl> + if e.code != 0 { <nl> + return e.code <nl> + } <nl> + return http.StatusInternalServerError <nl> +} <nl> + <nl> +// Error describes the error. <nl> +func (e *httpError) Error() string { <nl> + return e.err.Error() <nl> +} <nl> + <nl> // NewAggregator constructs a new Aggregator object to write the given result <nl> // set out to the given output directory. <nl> func NewAggregator(outputDir string, expected []plugin.ExpectedResult) *Aggregator { <nl> @@ -110,12 +132,11 @@ func (a *Aggregator) isResultDuplicate(result *plugin.Result) bool { <nl> return ok <nl> } <nl> -// HandleHTTPResult is called every time the HTTP server gets a well-formed <nl> -// request with results. This method is responsible for returning with things <nl> -// like a 409 conflict if a node has checked in twice (or a 403 forbidden if a <nl> -// node isn't expected), as well as actually calling handleResult to write the <nl> -// results to OutputDir. <nl> -func (a *Aggregator) HandleHTTPResult(result *plugin.Result, w http.ResponseWriter) { <nl> +// processResult is the centralized location for result processing. It is thread-safe <nl> +// and checks for whether or not the result should be excluded due to be either <nl> +// unexpected or a duplicate. Errors returned via this method will be of the type <nl> +// *httpError so that HTTP servers can respond appropriately to clients. <nl> +func (a *Aggregator) processResult(result *plugin.Result) error { <nl> a.resultsMutex.Lock() <nl> defer a.resultsMutex.Unlock() <nl> @@ -123,34 +144,54 @@ func (a *Aggregator) HandleHTTPResult(result *plugin.Result, w http.ResponseWrit <nl> // Make sure we were expecting this result <nl> if !a.isResultExpected(result) { <nl> - http.Error( <nl> - w, <nl> - fmt.Sprintf(\"Result %v unexpected\", resultID), <nl> - http.StatusForbidden, <nl> - ) <nl> - return <nl> + return &httpError{ <nl> + err: fmt.Errorf(\"result %v unexpected\", resultID), <nl> + code: http.StatusForbidden, <nl> + } <nl> } <nl> // Don't allow duplicates <nl> if a.isResultDuplicate(result) { <nl> - logrus.Warningf(\"Got a duplicate result %v\", resultID) <nl> - http.Error( <nl> - w, <nl> - fmt.Sprintf(\"Result %v already received\", resultID), <nl> - http.StatusConflict, <nl> - ) <nl> - return <nl> + return &httpError{ <nl> + err: fmt.Errorf(\"result %v already received\", resultID), <nl> + code: http.StatusConflict, <nl> + } <nl> } <nl> if err := a.handleResult(result); err != nil { <nl> - errMsg := fmt.Sprintf(\"Error handling result %v: %v\", resultID, err) <nl> - logrus.Info(errMsg) <nl> + return &httpError{ <nl> + err: fmt.Errorf(\"error handling result %v: %v\", resultID, err), <nl> + code: http.StatusInternalServerError, <nl> + } <nl> + } <nl> + <nl> + return nil <nl> +} <nl> + <nl> +// HandleHTTPResult is called every time the HTTP server gets a well-formed <nl> +// request with results. This method is responsible for returning with things <nl> +// like a 409 conflict if a node has checked in twice (or a 403 forbidden if a <nl> +// node isn't expected), as well as actually calling handleResult to write the <nl> +// results to OutputDir. <nl> +func (a *Aggregator) HandleHTTPResult(result *plugin.Result, w http.ResponseWriter) { <nl> + err := a.processResult(result) <nl> + if err != nil { <nl> + switch t := err.(type) { <nl> + case *httpError: <nl> + logrus.Errorf(\"Result processing error (%v): %v\", t.HttpCode(), t.Error()) <nl> http.Error( <nl> w, <nl> - errMsg, <nl> + t.Error(), <nl> + t.HttpCode(), <nl> + ) <nl> + default: <nl> + logrus.Errorf(\"Result processing error (%v): %v\", http.StatusInternalServerError, t.Error()) <nl> + http.Error( <nl> + w, <nl> + err.Error(), <nl> http.StatusInternalServerError, <nl> ) <nl> - return <nl> + } <nl> } <nl> } <nl> @@ -166,26 +207,16 @@ func (a *Aggregator) IngestResults(resultsCh <-chan *plugin.Result) { <nl> if !more { <nl> break <nl> } <nl> - // Don't consume results we're not expecting, unless they're <nl> - // errors (see below.) <nl> - if !a.isResultExpected(result) { <nl> - logrus.Warningf(\"Result unexpected: %v\", result) <nl> - continue <nl> - } <nl> - func() { <nl> - a.resultsMutex.Lock() <nl> - defer a.resultsMutex.Unlock() <nl> - <nl> - // Don't consume results we've already seen <nl> - if a.isResultDuplicate(result) { <nl> - logrus.Warningf(\"Duplicate result: %v\", result) <nl> - return <nl> + err := a.processResult(result) <nl> + if err != nil { <nl> + switch t := err.(type) { <nl> + case *httpError: <nl> + logrus.Errorf(\"Result processing error (%v): %v\", t.HttpCode(), t.Error()) <nl> + default: <nl> + logrus.Errorf(\"Result processing error (%v): %v\", http.StatusInternalServerError, t.Error()) <nl> + } <nl> } <nl> - <nl> - a.handleResult(result) <nl> - }() <nl> - <nl> } <nl> } <nl> ", "msg": "Unify http/non-http result handling\nWe have two flows for this logic: one for the\nHTTP server and one for the channel monitoring\nresults.\nThis change places all the logic into a central\nmethod so that it can be modified/tested more\nconfidently/succinctly."}
{"diff_id": 4333, "repo": "vmware-tanzu/sonobuoy", "sha": "fd7722dff6401a311b83feefd6ec5661c479e71a", "time": "27.06.2019 19:27:16", "diff": "mmm a / cmd/sonobuoy/app/images.go <nl> ppp b / cmd/sonobuoy/app/images.go <nl>@@ -59,6 +59,7 @@ func NewCmdImages() *cobra.Command { <nl> Run: pullImages, <nl> Args: cobra.ExactArgs(0), <nl> } <nl> + AddE2ERegistryConfigFlag(&imagesflags.e2eRegistryConfig, pullCmd.Flags()) <nl> AddKubeconfigFlag(&imagesflags.kubeconfig, pullCmd.Flags()) <nl> AddPluginFlag(&imagesflags.plugin, pullCmd.Flags()) <nl> @@ -166,7 +167,7 @@ func pullImages(cmd *cobra.Command, args []string) { <nl> os.Exit(1) <nl> } <nl> - upstreamImages, err := image.GetImages(defaultE2ERegistries, version) <nl> + upstreamImages, err := image.GetImages(imagesflags.e2eRegistryConfig, version) <nl> if err != nil { <nl> errlog.LogError(errors.Wrap(err, \"couldn't init upstream registry list\")) <nl> os.Exit(1) <nl> ", "msg": "Add e2e-repo-config flag to images pull command"}
{"diff_id": 4337, "repo": "vmware-tanzu/sonobuoy", "sha": "30365d102a698bc6ad1f470cf39946fe938de71f", "time": "18.12.2019 19:18:30", "diff": "mmm a / pkg/plugin/aggregation/status.go <nl> ppp b / pkg/plugin/aggregation/status.go <nl>@@ -108,7 +108,7 @@ func (s *Status) updateStatus() error { <nl> // is returned. <nl> func GetStatus(client kubernetes.Interface, namespace string) (*Status, error) { <nl> if _, err := client.CoreV1().Namespaces().Get(namespace, metav1.GetOptions{}); err != nil { <nl> - return nil, errors.Wrap(err, \"sonobuoy namespace does not exist\") <nl> + return nil, errors.Wrapf(err, \"failed to get namespace %v\", namespace) <nl> } <nl> // Determine sonobuoy pod name <nl> ", "msg": "Improve error message from GetStatus\nThe GetStatus function assumed that any error when getting a namespace\nwas due to that namespace not existing. Instead state that it was an\nerror when getting the namespace."}
{"diff_id": 4339, "repo": "vmware-tanzu/sonobuoy", "sha": "5fd899104420ad64fb5f4db2dea0838fdd718b40", "time": "21.01.2020 16:40:41", "diff": "mmm a / pkg/config/config.go <nl> ppp b / pkg/config/config.go <nl>@@ -63,7 +63,7 @@ const ( <nl> DefaultDNSNamespace = \"kube-system\" <nl> // DefaultSystemdLogsImage is the URL for the docker image used by the systemd-logs plugin <nl> - DefaultSystemdLogsImage = \"gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest\" <nl> + DefaultSystemdLogsImage = \"sonobuoy/systemd-logs:v0.3\" <nl> ) <nl> var ( <nl> ", "msg": "Update to use latest systemd-logs image\nThe image is now published to the sonobuoy repo on dockerhub.\nIn addition, the plugin code is now at github.com/vmware-tanzu/sonobuoy-plugins"}
{"diff_id": 4345, "repo": "vmware-tanzu/sonobuoy", "sha": "30592b486b523f9cfbb94ba3d721cedcbf489665", "time": "22.03.2021 12:03:27", "diff": "mmm a / pkg/buildinfo/version.go <nl> ppp b / pkg/buildinfo/version.go <nl>@@ -20,7 +20,7 @@ limitations under the License. <nl> package buildinfo <nl> // Version is the current version of Sonobuoy, set by the go linker's -X flag at build time <nl> -var Version = \"v0.20.0\" <nl> +var Version = \"v0.50.0\" <nl> // GitSHA is the actual commit that is being built, set by the go linker's -X flag at build time. <nl> var GitSHA string <nl> ", "msg": "Bump sonobuoy version\nThe executable on github is correctly reporting v0.50.0 but not\nmaster. So developer builds are incorrectly showing v0.20.0."}
{"diff_id": 4347, "repo": "vmware-tanzu/sonobuoy", "sha": "c738d7cc439f877fb6cfd3229b6fc2f0edf2af05", "time": "28.06.2021 07:35:31", "diff": "mmm a / test/integration/sonobuoy_integration_test.go <nl> ppp b / test/integration/sonobuoy_integration_test.go <nl>@@ -110,6 +110,7 @@ func cleanup(t *testing.T, namespace string) { <nl> } <nl> func TestUseNamespaceFromManifest(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -141,6 +142,7 @@ func TestUseNamespaceFromManifest(t *testing.T) { <nl> // TestSimpleRun runs a simple plugin to check that it runs successfully <nl> func TestSimpleRun(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -152,6 +154,7 @@ func TestSimpleRun(t *testing.T) { <nl> } <nl> func TestRetrieveAndExtract(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -195,6 +198,7 @@ func TestRetrieveAndExtract(t *testing.T) { <nl> // TestQuick runs a real \"--mode quick\" check against the cluster to ensure that it passes. <nl> func TestQuick(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -212,6 +216,7 @@ func TestQuick(t *testing.T) { <nl> } <nl> func TestConfigmaps(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -324,6 +329,7 @@ func saveToArtifacts(t *testing.T, p string) (newPath string) { <nl> // TestSonobuoyVersion checks that all fields in the output from `version` are non-empty <nl> func TestSonobuoyVersion(t *testing.T) { <nl> + t.Parallel() <nl> stdout := mustRunSonobuoyCommand(t, \"version\") <nl> lines := strings.Split(stdout.String(), \"\\n\") <nl> @@ -338,6 +344,7 @@ func TestSonobuoyVersion(t *testing.T) { <nl> } <nl> func TestManualResultsJob(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -362,6 +369,7 @@ func TestManualResultsJob(t *testing.T) { <nl> } <nl> func TestManualResultsDaemonSet(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> @@ -391,6 +399,7 @@ func TestManualResultsDaemonSet(t *testing.T) { <nl> } <nl> func TestManualResultsWithNestedDetails(t *testing.T) { <nl> + t.Parallel() <nl> ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout) <nl> defer cancel() <nl> ", "msg": "Run integration tests in parallel\nMost of the tests in the integration suite can\nbe run in parallel to save time. This is due to the\nfact that they all run in their own namespaces\nanyways. If they conflict somehow we'll have to\ndebug that as well.\nFixes"}
{"diff_id": 4354, "repo": "vmware-tanzu/sonobuoy", "sha": "91f9c2af539c77370b3b2a218bfadfc22f8e7b8e", "time": "11.02.2022 22:42:32", "diff": "mmm a / cmd/sonobuoy/app/version.go <nl> ppp b / cmd/sonobuoy/app/version.go <nl>@@ -18,6 +18,7 @@ package app <nl> import ( <nl> \"fmt\" <nl> + \"runtime\" <nl> \"github.com/pkg/errors\" <nl> \"github.com/spf13/cobra\" <nl> @@ -57,6 +58,8 @@ func runVersion(versionflags *versionFlags) func(cmd *cobra.Command, args []stri <nl> fmt.Printf(\"MinimumKubeVersion: %s\\n\", buildinfo.MinimumKubeVersion) <nl> fmt.Printf(\"MaximumKubeVersion: %s\\n\", buildinfo.MaximumKubeVersion) <nl> fmt.Printf(\"GitSHA: %s\\n\", buildinfo.GitSHA) <nl> + fmt.Printf(\"GoVersion: %s\\n\", runtime.Version()) <nl> + fmt.Printf(\"Platform: %s/%s\\n\", runtime.GOOS, runtime.GOARCH) <nl> // Get Kubernetes version, this is last so that the regular version information <nl> // will be shown even if the API server cannot be contacted and throws an error <nl> ", "msg": "Improve version subcommand\nadds Go version and platform information to the output"}
{"diff_id": 4359, "repo": "vmware-tanzu/sonobuoy", "sha": "8690c6cb2971589a8ef319fe2de242f14092a37a", "time": "09.05.2022 11:43:48", "diff": "mmm a / test/integration/sonobuoy_integration_test.go <nl> ppp b / test/integration/sonobuoy_integration_test.go <nl>@@ -422,6 +422,7 @@ func checkTarballPluginForErrors(t *testing.T, tarball, plugin string, failCount <nl> } <nl> func saveToArtifacts(t *testing.T, p string) (newPath string) { <nl> + p = strings.TrimSpace(p) <nl> artifactsDir := os.Getenv(\"ARTIFACTS_DIR\") <nl> if artifactsDir == \"\" { <nl> t.Logf(\"Skipping saving artifact %v since ARTIFACTS_DIR is unset.\", p) <nl> @@ -439,7 +440,7 @@ func saveToArtifacts(t *testing.T, p string) (newPath string) { <nl> var stdout, stderr bytes.Buffer <nl> // Shell out to `mv` instead of using os.Rename(); the latter caused a problem due to files being on different devices. <nl> - cmd := exec.CommandContext(context.Background(), bash, \"-c\", fmt.Sprintf(\"mv -r %v %v\", origFile, artifactFile)) <nl> + cmd := exec.CommandContext(context.Background(), bash, \"-c\", fmt.Sprintf(\"mv %v %v\", origFile, artifactFile)) <nl> cmd.Stdout = &stdout <nl> cmd.Stderr = &stderr <nl> ", "msg": "Remove -r option from mv command\nError with this option causing artifacts not to be moved for CI\nFixes"}
{"diff_id": 4375, "repo": "karimra/gnmic", "sha": "f38773646656dbb7a96a48a37da1d800126fdfcd", "time": "09.03.2020 16:38:17", "diff": "mmm a / cmd/get.go <nl> ppp b / cmd/get.go <nl>@@ -65,9 +65,6 @@ var getCmd = &cobra.Command{ <nl> Encoding: gnmi.Encoding(encodingVal), <nl> } <nl> model := viper.GetString(\"get-model\") <nl> - if model != \"\" { <nl> - req.UseModels = append(req.UseModels, &gnmi.ModelData{Name: model}) <nl> - } <nl> prefix := viper.GetString(\"get-prefix\") <nl> if prefix != \"\" { <nl> gnmiPrefix, err := xpath.ToGNMIPath(prefix) <nl> @@ -86,7 +83,7 @@ var getCmd = &cobra.Command{ <nl> } <nl> dataType := viper.GetString(\"get-type\") <nl> if dataType != \"\" { <nl> - dti, ok := gnmi.GetRequest_DataType_value[dataType] <nl> + dti, ok := gnmi.GetRequest_DataType_value[strings.ToUpper(dataType)] <nl> if !ok { <nl> return fmt.Errorf(\"unknown data type %s\", dataType) <nl> } <nl> @@ -119,8 +116,35 @@ var getCmd = &cobra.Command{ <nl> ctx, cancel := context.WithCancel(context.Background()) <nl> defer cancel() <nl> ctx = metadata.AppendToOutgoingContext(ctx, \"username\", username, \"password\", password) <nl> - <nl> - response, err := client.Get(ctx, req) <nl> + xreq := req <nl> + if model != \"\" { <nl> + capResp, err := client.Capabilities(ctx, &gnmi.CapabilityRequest{}) <nl> + if err != nil { <nl> + log.Printf(\"%v\", err) <nl> + return <nl> + } <nl> + var found bool <nl> + for _, m := range capResp.SupportedModels { <nl> + if m.Name == model { <nl> + if debug { <nl> + log.Printf(\"target %s: found model: %v\\n\", address, m) <nl> + } <nl> + xreq.UseModels = append(xreq.UseModels, <nl> + &gnmi.ModelData{ <nl> + Name: model, <nl> + Organization: m.Organization, <nl> + Version: m.Version, <nl> + }) <nl> + found = true <nl> + break <nl> + } <nl> + } <nl> + if !found { <nl> + log.Printf(\"model '%s' not supported by target %s\", model, address) <nl> + return <nl> + } <nl> + } <nl> + response, err := client.Get(ctx, xreq) <nl> if err != nil { <nl> log.Printf(\"error sending get request: %v\", err) <nl> return <nl> ", "msg": "use capabilities to get the model details"}
{"diff_id": 4379, "repo": "karimra/gnmic", "sha": "10bade5f772a7347a57ba48bb1ee929d582f0252", "time": "24.04.2020 10:02:00", "diff": "mmm a / cmd/set.go <nl> ppp b / cmd/set.go <nl>@@ -159,13 +159,11 @@ var setCmd = &cobra.Command{ <nl> } <nl> } else { <nl> var vType string <nl> - if inlineUpdates { <nl> if len(updateTypes) > i { <nl> vType = updateTypes[i] <nl> } else { <nl> vType = \"json\" <nl> } <nl> - } <nl> switch vType { <nl> case \"json\": <nl> buff := new(bytes.Buffer) <nl> @@ -263,13 +261,11 @@ var setCmd = &cobra.Command{ <nl> } <nl> } else { <nl> var vType string <nl> - if inlineReplaces { <nl> if len(replaceTypes) > i { <nl> vType = replaceTypes[i] <nl> } else { <nl> vType = \"json\" <nl> } <nl> - } <nl> switch vType { <nl> case \"json\": <nl> buff := new(bytes.Buffer) <nl> ", "msg": "make json a default type for set update/replace"}
{"diff_id": 4391, "repo": "karimra/gnmic", "sha": "3458e51fb6db93dee444d46b202212e55e53710e", "time": "22.05.2020 08:44:17", "diff": "mmm a / cmd/subscribe.go <nl> ppp b / cmd/subscribe.go <nl>@@ -132,7 +132,7 @@ var subscribeCmd = &cobra.Command{ <nl> } <nl> switch resp := subscribeRsp.Response.(type) { <nl> case *gnmi.SubscribeResponse_Update: <nl> - printSubscribeResponse(nil, subscribeRsp) <nl> + printSubscribeResponse(map[string]interface{}{\"source\": address}, subscribeRsp) <nl> case *gnmi.SubscribeResponse_SyncResponse: <nl> logger.Printf(\"received sync response=%+v from %s\\n\", resp.SyncResponse, address) <nl> if subscReq.GetSubscribe().Mode == gnmi.SubscriptionList_ONCE { <nl> @@ -162,7 +162,7 @@ var subscribeCmd = &cobra.Command{ <nl> } <nl> switch resp := subscribeRsp.Response.(type) { <nl> case *gnmi.SubscribeResponse_Update: <nl> - printSubscribeResponse(nil, subscribeRsp) <nl> + printSubscribeResponse(map[string]interface{}{\"source\": address}, subscribeRsp) <nl> case *gnmi.SubscribeResponse_SyncResponse: <nl> fmt.Printf(\"%ssync response: %+v\\n\", printPrefix, resp.SyncResponse) <nl> } <nl> ", "msg": "add source field to subscribe output"}
{"diff_id": 4399, "repo": "karimra/gnmic", "sha": "a9a8599a6ed602a8f8d9a9cf1d589f7e1d9eac1b", "time": "10.06.2020 13:38:24", "diff": "mmm a / cmd/capabilities.go <nl> ppp b / cmd/capabilities.go <nl>@@ -21,10 +21,10 @@ import ( <nl> \"strings\" <nl> \"sync\" <nl> - \"github.com/gogo/protobuf/proto\" <nl> \"github.com/openconfig/gnmi/proto/gnmi\" <nl> \"github.com/spf13/viper\" <nl> \"google.golang.org/grpc/metadata\" <nl> + \"google.golang.org/protobuf/encoding/prototext\" <nl> \"github.com/spf13/cobra\" <nl> ) <nl> @@ -93,6 +93,7 @@ func reqCapability(ctx context.Context, address, username, password string, wg * <nl> nctx = metadata.AppendToOutgoingContext(nctx, \"username\", username, \"password\", password) <nl> req := &gnmi.CapabilityRequest{} <nl> + logger.Printf(\"sending gNMI CapabilityRequest: gnmi_ext.Extension='%v' to %s\", req.Extension, address) <nl> response, err := client.Capabilities(nctx, req) <nl> if err != nil { <nl> logger.Printf(\"error sending capabilities request: %v\", err) <nl> @@ -111,8 +112,7 @@ func printCapResponse(r *gnmi.CapabilityResponse, address string) { <nl> printPrefix = fmt.Sprintf(\"[%s] \", address) <nl> } <nl> if viper.GetString(\"format\") == \"textproto\" { <nl> - rsp := proto.MarshalTextString(r) <nl> - fmt.Println(indent(printPrefix, rsp)) <nl> + fmt.Printf(\"%s\\n\", indent(printPrefix, prototext.Format(r))) <nl> return <nl> } <nl> fmt.Printf(\"%sgNMI version: %s\\n\", printPrefix, r.GNMIVersion) <nl> ", "msg": "fixed prototext output for capability req"}
{"diff_id": 4403, "repo": "karimra/gnmic", "sha": "4219bc7b58323629982d150b73f6e7eff0475c5c", "time": "11.06.2020 13:59:39", "diff": "mmm a / cmd/set.go <nl> ppp b / cmd/set.go <nl>@@ -400,7 +400,7 @@ func setRequest(ctx context.Context, req *gnmi.SetRequest, address, username, pa <nl> } <nl> lock.Lock() <nl> defer lock.Unlock() <nl> - logger.Printf(\"sending gNMI SetRequest: '%s' to %s\", prototext.MarshalOptions{Multiline: false}.Format(req), address) <nl> + logger.Printf(\"sending gNMI SetRequest: prefix='%v', delete='%v', replace='%v', update='%v', extension='%v' to %s\", req.Prefix, req.Delete, req.Replace, req.Update, req.Extension, address) <nl> printSetRequest(printPrefix, req) <nl> response, err := client.Set(nctx, req) <nl> ", "msg": "reverted back to log using fields instead of proto marshaller"}
{"diff_id": 4410, "repo": "karimra/gnmic", "sha": "e3fd997f14188ce212a9bb8d6cd5fef816ffb381", "time": "12.06.2020 21:12:37", "diff": "mmm a / cmd/root.go <nl> ppp b / cmd/root.go <nl>@@ -357,7 +357,7 @@ func setupCloseHandler(cancelFn context.CancelFunc) { <nl> signal.Notify(c, os.Interrupt, syscall.SIGTERM, syscall.SIGINT, syscall.SIGKILL) <nl> go func() { <nl> sig := <-c <nl> - fmt.Printf(\"received signal '%s'. terminating...\\n\", sig.String()) <nl> + fmt.Printf(\"\\nreceived signal '%s'. terminating...\\n\", sig.String()) <nl> cancelFn() <nl> os.Exit(0) <nl> }() <nl> ", "msg": "added newlined for terminating message"}
{"diff_id": 4422, "repo": "karimra/gnmic", "sha": "1e30ae60c5fde2ad014416dff840cde2cd251a82", "time": "17.06.2020 17:41:02", "diff": "mmm a / cmd/subscribe.go <nl> ppp b / cmd/subscribe.go <nl>package cmd <nl> import ( <nl> + \"bytes\" <nl> \"context\" <nl> \"encoding/json\" <nl> \"errors\" <nl> @@ -24,6 +25,8 @@ import ( <nl> \"time\" <nl> \"github.com/google/gnxi/utils/xpath\" <nl> + \"github.com/karimra/gnmiClient/outputs\" <nl> + _ \"github.com/karimra/gnmiClient/outputs/all\" <nl> \"github.com/manifoldco/promptui\" <nl> \"github.com/openconfig/gnmi/proto/gnmi\" <nl> \"github.com/spf13/cobra\" <nl> @@ -73,10 +76,17 @@ var subscribeCmd = &cobra.Command{ <nl> polledSubsChan[targets[i].Address] = make(chan struct{}) <nl> } <nl> } <nl> + outputs, err := getOutputs() <nl> + if err != nil { <nl> + return err <nl> + } <nl> + for _, o := range outputs { <nl> + go o.Start() <nl> + } <nl> wg := new(sync.WaitGroup) <nl> wg.Add(len(targets)) <nl> for _, target := range targets { <nl> - go subRequest(ctx, subscReq, target, wg, polledSubsChan, waitChan) <nl> + go subRequest(ctx, subscReq, target, wg, polledSubsChan, waitChan, outputs) <nl> } <nl> if subscReq.GetSubscribe().Mode == gnmi.SubscriptionList_POLL { <nl> addresses := make([]string, len(targets)) <nl> @@ -111,11 +121,20 @@ var subscribeCmd = &cobra.Command{ <nl> waitChan <- struct{}{} <nl> } <nl> wg.Wait() <nl> + for _, o := range outputs { <nl> + o.Close() <nl> + } <nl> return nil <nl> }, <nl> } <nl> -func subRequest(ctx context.Context, req *gnmi.SubscribeRequest, target *target, wg *sync.WaitGroup, polledSubsChan map[string]chan struct{}, waitChan chan struct{}) { <nl> +func subRequest(ctx context.Context, <nl> + req *gnmi.SubscribeRequest, <nl> + target *target, <nl> + wg *sync.WaitGroup, <nl> + polledSubsChan map[string]chan struct{}, <nl> + waitChan chan struct{}, <nl> + outputs []outputs.Output) { <nl> defer wg.Done() <nl> conn, err := createGrpcConn(target.Address) <nl> if err != nil { <nl> @@ -182,13 +201,22 @@ func subRequest(ctx context.Context, req *gnmi.SubscribeRequest, target *target, <nl> } <nl> switch resp := subscribeRsp.Response.(type) { <nl> case *gnmi.SubscribeResponse_Update: <nl> - lock.Lock() <nl> b, err := formatSubscribeResponse(map[string]interface{}{\"source\": target.Address}, subscribeRsp) <nl> if err != nil { <nl> logger.Printf(\"failed to format subscribe response: %v\", err) <nl> return <nl> } <nl> - fmt.Println(string(b)) <nl> + for _, o := range outputs { <nl> + go o.Write(b) <nl> + } <nl> + buff := new(bytes.Buffer) <nl> + err = json.Indent(buff, b, \"\", \" \") <nl> + if err != nil { <nl> + logger.Printf(\"failed to indent msg: err=%v, msg=%s\", err, string(b)) <nl> + return <nl> + } <nl> + lock.Lock() <nl> + fmt.Println(buff.String()) <nl> lock.Unlock() <nl> case *gnmi.SubscribeResponse_SyncResponse: <nl> logger.Printf(\"received sync response=%+v from %s\\n\", resp.SyncResponse, target.Address) <nl> @@ -223,9 +251,16 @@ func subRequest(ctx context.Context, req *gnmi.SubscribeRequest, target *target, <nl> b, err := formatSubscribeResponse(map[string]interface{}{\"source\": target.Address}, subscribeRsp) <nl> if err != nil { <nl> logger.Printf(\"failed to format subscribe response: %v\", err) <nl> + waitChan <- struct{}{} <nl> + continue <nl> + } <nl> + buff := new(bytes.Buffer) <nl> + err = json.Indent(buff, b, \"\", \" \") <nl> + if err != nil { <nl> + logger.Printf(\"failed to indent msg: err=%v : msg=%s\", err, string(b)) <nl> return <nl> } <nl> - fmt.Println(string(b)) <nl> + fmt.Println(buff.String()) <nl> case *gnmi.SubscribeResponse_SyncResponse: <nl> fmt.Printf(\"sync response from %s: %+v\\n\", target.Address, resp.SyncResponse) <nl> } <nl> @@ -387,3 +422,39 @@ func formatSubscribeResponse(meta map[string]interface{}, subResp *gnmi.Subscrib <nl> } <nl> return nil, nil <nl> } <nl> + <nl> +func getOutputs() ([]outputs.Output, error) { <nl> + outDef := viper.GetStringMap(\"outputs\") <nl> + fmt.Println(outDef) <nl> + outputDestinations := make([]outputs.Output, 0) <nl> + for n, d := range outDef { <nl> + initalizer, ok := outputs.Outputs[n] <nl> + if !ok { <nl> + logger.Printf(\"unknown output type '%s'\", n) <nl> + continue <nl> + } <nl> + dl := convert(d) <nl> + switch dl.(type) { <nl> + case []interface{}: <nl> + outs := d.([]interface{}) <nl> + for _, ou := range outs { <nl> + fmt.Printf(\"ou: %T\\n\", ou) <nl> + switch ou.(type) { <nl> + case map[string]interface{}: <nl> + o := initalizer() <nl> + err := o.Initialize(ou.(map[string]interface{})) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + outputDestinations = append(outputDestinations, o) <nl> + default: <nl> + logger.Printf(\"unknown configuration format: %T : %v\", d, d) <nl> + } <nl> + } <nl> + default: <nl> + logger.Printf(\"unknown configuration format: %T : %v\", d, d) <nl> + return nil, fmt.Errorf(\"unknown configuration format: %T : %v\", d, d) <nl> + } <nl> + } <nl> + return outputDestinations, nil <nl> +} <nl> ", "msg": "add outputs options to subscribe cmd"}
{"diff_id": 4439, "repo": "karimra/gnmic", "sha": "8bdb5ead756ef58f259d6ea823b191131dc2dc95", "time": "20.06.2020 21:03:59", "diff": "mmm a / cmd/root.go <nl> ppp b / cmd/root.go <nl>@@ -79,7 +79,7 @@ var rootCmd = &cobra.Command{ <nl> logger = log.New(f, \"\", log.LstdFlags|log.Lmicroseconds) <nl> logger.SetFlags(log.LstdFlags | log.Lmicroseconds) <nl> if viper.GetBool(\"debug\") { <nl> - grpclog.SetLogger(logger) <nl> + grpclog.SetLogger(logger) //lint:ignore SA1019 see https://github.com/karimra/gnmiClient/issues/59 <nl> } <nl> }, <nl> PersistentPostRun: func(cmd *cobra.Command, args []string) { <nl> ", "msg": "disable staticcheck linter check for SetLoggerV2"}
{"diff_id": 4451, "repo": "karimra/gnmic", "sha": "967a253d4088feb2b0a198674f4b2962b02058c1", "time": "26.06.2020 11:36:51", "diff": "mmm a / outputs/nats_output/natsOutput.go <nl> ppp b / outputs/nats_output/natsOutput.go <nl>@@ -2,6 +2,7 @@ package nats_output <nl> import ( <nl> \"context\" <nl> + \"fmt\" <nl> \"log\" <nl> \"net\" <nl> \"os\" <nl> @@ -44,7 +45,7 @@ type NatsOutput struct { <nl> type Config struct { <nl> Name string <nl> Address string <nl> - Subject string <nl> + SubjectPrefix string <nl> Username string <nl> Password string <nl> ConnectTimeout time.Duration <nl> @@ -78,10 +79,14 @@ func (n *NatsOutput) Init(cfg map[string]interface{}, logger *log.Logger) error <nl> } <nl> // Write // <nl> -func (n *NatsOutput) Write(b []byte) { <nl> - err := n.conn.Publish(n.Cfg.Subject, b) <nl> +func (n *NatsOutput) Write(b []byte, meta outputs.Meta) { <nl> + subject := n.Cfg.SubjectPrefix <nl> + if s, ok := meta[\"source\"]; ok { <nl> + subject += fmt.Sprintf(\".%s\", s) <nl> + } <nl> + err := n.conn.Publish(subject, b) <nl> if err != nil { <nl> - log.Printf(\"failed to write to nats subject '%s': %v\", n.Cfg.Subject, err) <nl> + log.Printf(\"failed to write to nats subject '%s': %v\", subject, err) <nl> return <nl> } <nl> // n.logger.Printf(\"wrote %d bytes to nats_subject=%s\", len(b), n.Cfg.Subject) <nl> ", "msg": "use subject prefix for nats output"}
{"diff_id": 4463, "repo": "karimra/gnmic", "sha": "5b0d3d11fde0fc13244a55c6afc1496efaad0c47", "time": "01.07.2020 22:30:52", "diff": "mmm a / cmd/root.go <nl> ppp b / cmd/root.go <nl>@@ -533,10 +533,10 @@ func numTargets() int { <nl> return 0 <nl> } <nl> -func createTargets() ([]*collector.Target, error) { <nl> +func createTargets() (map[string]*collector.TargetConfig, error) { <nl> var err error <nl> addresses := viper.GetStringSlice(\"address\") <nl> - targets := make([]*collector.Target, 0, len(addresses)) <nl> + targets := make(map[string]*collector.TargetConfig) <nl> defGrpcPort := viper.GetString(\"port\") <nl> defUsername := viper.GetString(\"username\") <nl> defPassword := viper.GetString(\"password\") <nl> @@ -565,15 +565,8 @@ func createTargets() ([]*collector.Target, error) { <nl> } <nl> tc.Address = addr <nl> setTargetConfigDefaults(tc) <nl> - t, err := collector.NewTarget(tc) <nl> - if err != nil { <nl> - return nil, err <nl> - } <nl> - targets = append(targets, t) <nl> + targets[tc.Name] = tc <nl> } <nl> - sort.Slice(targets, func(i, j int) bool { <nl> - return targets[i].Config.Address < targets[j].Config.Address <nl> - }) <nl> return targets, nil <nl> } <nl> // case targets is defined in config file <nl> @@ -589,11 +582,9 @@ func createTargets() ([]*collector.Target, error) { <nl> default: <nl> return nil, fmt.Errorf(\"unexpected targets format, got: %T\", targetsInt) <nl> } <nl> - ltm := len(targetsMap) <nl> - if ltm == 0 { <nl> + if len(targetsMap) == 0 { <nl> return nil, fmt.Errorf(\"no targets found\") <nl> } <nl> - targets = make([]*collector.Target, 0, ltm) <nl> for addr, t := range targetsMap { <nl> tc := new(collector.TargetConfig) <nl> _, _, err := net.SplitHostPort(addr) <nl> @@ -620,11 +611,8 @@ func createTargets() ([]*collector.Target, error) { <nl> if viper.GetBool(\"debug\") { <nl> logger.Printf(\"read target config: %s\", tc) <nl> } <nl> - t, err := collector.NewTarget(tc) <nl> - if err != nil { <nl> - return nil, fmt.Errorf(\"failed to create target: %v\", err) <nl> - } <nl> - targets = append(targets, t) <nl> + <nl> + targets[tc.Name] = tc <nl> } <nl> return targets, nil <nl> } <nl> ", "msg": "change targets to map rather than list"}
{"diff_id": 4477, "repo": "karimra/gnmic", "sha": "9d1f04625b69f5e904e23716be9827d3e23f0a48", "time": "07.07.2020 10:02:48", "diff": "mmm a / collector/collector.go <nl> ppp b / collector/collector.go <nl>@@ -144,7 +144,7 @@ func (c *Collector) Subscribe(tName string) error { <nl> if err != nil { <nl> return err <nl> } <nl> - c.Logger.Printf(\"sending gnmi SubscribeRequest: subscribe='%+v', mode='%+v', encoding='%+v', to %s\", <nl> + c.Logger.Printf(\"sending gNMI SubscribeRequest: subscribe='%+v', mode='%+v', encoding='%+v', to %s\", <nl> req, req.GetSubscribe().GetMode(), req.GetSubscribe().GetEncoding(), t.Config.Name) <nl> go t.Subscribe(c.ctx, req, sc.Name) <nl> } <nl> ", "msg": "set gnmi->gNMI to make it consistent with other log msgs"}
{"diff_id": 4488, "repo": "karimra/gnmic", "sha": "ac5e682d68ac964005baa475e7dc7d73d76dffc2", "time": "08.07.2020 13:38:52", "diff": "mmm a / cmd/set.go <nl> ppp b / cmd/set.go <nl>@@ -405,6 +405,10 @@ func createSetRequest() (*gnmi.SetRequest, error) { <nl> Val: value, <nl> }) <nl> } <nl> + if (len(req.Delete) == 0) && (len(req.Update) == 0) && (len(req.Replace) == 0) { <nl> + return nil, errors.New(\"no data to populate Set Request with\") <nl> + } <nl> + <nl> return req, nil <nl> } <nl> ", "msg": "fix set command should stop if the --update-file references to non existent file"}
{"diff_id": 4505, "repo": "karimra/gnmic", "sha": "5b52fb4ded21e27ffac6e14b27632fa6ade3cf78", "time": "11.07.2020 18:22:50", "diff": "mmm a / cmd/set.go <nl> ppp b / cmd/set.go <nl>@@ -265,10 +265,9 @@ func init() { <nl> } <nl> func createSetRequest() (*gnmi.SetRequest, error) { <nl> - prefix := viper.GetString(\"set-prefix\") <nl> - gnmiPrefix, err := xpath.ToGNMIPath(prefix) <nl> + gnmiPrefix, err := collector.CreatePrefix(viper.GetString(\"set-prefix\"), viper.GetString(\"set-target\")) <nl> if err != nil { <nl> - return nil, err <nl> + return nil, fmt.Errorf(\"prefix parse error: %v\", err) <nl> } <nl> deletes := viper.GetStringSlice(\"set-delete\") <nl> updates := viper.GetStringSlice(\"set-update\") <nl> ", "msg": "add prefix with target to createSetRequest func"}
{"diff_id": 4511, "repo": "karimra/gnmic", "sha": "55ecae23cf1e56f19bcbc3251037d8a5b042e8bd", "time": "15.07.2020 15:37:47", "diff": "mmm a / cmd/subscribe.go <nl> ppp b / cmd/subscribe.go <nl>@@ -321,6 +321,7 @@ func getSubscriptions() (map[string]*collector.SubscriptionConfig, error) { <nl> paths := viper.GetStringSlice(\"subscribe-path\") <nl> hi := viper.GetDuration(\"subscribe-heartbeat-interval\") <nl> si := viper.GetDuration(\"subscribe-sample-interval\") <nl> + qos := viper.GetUint32(\"qos\") <nl> if len(paths) > 0 { <nl> sub := new(collector.SubscriptionConfig) <nl> sub.Name = \"default\" <nl> @@ -329,7 +330,7 @@ func getSubscriptions() (map[string]*collector.SubscriptionConfig, error) { <nl> sub.Target = viper.GetString(\"subscribe-target\") <nl> sub.Mode = viper.GetString(\"subscribe-mode\") <nl> sub.Encoding = viper.GetString(\"encoding\") <nl> - sub.Qos = viper.GetUint32(\"qos\") <nl> + sub.Qos = &qos <nl> sub.StreamMode = viper.GetString(\"subscribe-stream-mode\") <nl> sub.HeartbeatInterval = &hi <nl> sub.SampleInterval = &si <nl> @@ -358,12 +359,26 @@ func getSubscriptions() (map[string]*collector.SubscriptionConfig, error) { <nl> return nil, err <nl> } <nl> sub.Name = sn <nl> - if sub.SampleInterval == nil { // inherit global \"subscribe-sample-interval\" option if its not set <nl> + <nl> + // inherit global \"subscribe-*\" option if it's not set <nl> + if sub.SampleInterval == nil { <nl> sub.SampleInterval = &si <nl> } <nl> - if sub.HeartbeatInterval == nil { // inherit global \"subscribe-heartbeat-interval\" option if its not set <nl> + if sub.HeartbeatInterval == nil { <nl> sub.HeartbeatInterval = &hi <nl> } <nl> + if sub.Encoding == \"\" { <nl> + sub.Encoding = viper.GetString(\"encoding\") <nl> + } <nl> + if sub.Mode == \"\" { <nl> + sub.Mode = viper.GetString(\"subscribe-mode\") <nl> + } <nl> + if strings.ToUpper(sub.Mode) == \"STREAM\" && sub.StreamMode == \"\" { <nl> + sub.StreamMode = viper.GetString(\"subscribe-stream-mode\") <nl> + } <nl> + if sub.Qos == nil { <nl> + sub.Qos = &qos <nl> + } <nl> subscriptions[sn] = sub <nl> } <nl> return subscriptions, nil <nl> ", "msg": "add subscribe global values to subscriptions defs"}
{"diff_id": 4550, "repo": "karimra/gnmic", "sha": "477bfb0620999d4aecdf13aed27896c1819def8e", "time": "23.07.2020 23:05:45", "diff": "mmm a / cmd/subscribe.go <nl> ppp b / cmd/subscribe.go <nl>@@ -217,9 +217,7 @@ func getOutputs() (map[string][]outputs.Output, error) { <nl> \"file-type\": \"stdout\", <nl> \"format\": viper.GetString(\"format\"), <nl> } <nl> - stdoutFile := make([]interface{}, 1) <nl> - stdoutFile[0] = stdoutConfig <nl> - outDef[\"stdout\"] = stdoutFile <nl> + outDef[\"stdout\"] = []interface{}{stdoutConfig} <nl> } <nl> outputDestinations := make(map[string][]outputs.Output) <nl> for name, d := range outDef { <nl> @@ -231,6 +229,10 @@ func getOutputs() (map[string][]outputs.Output, error) { <nl> case map[string]interface{}: <nl> if outType, ok := ou[\"type\"]; ok { <nl> if initalizer, ok := outputs.Outputs[outType.(string)]; ok { <nl> + format, ok := ou[\"format\"] <nl> + if !ok || (ok && format == \"\") { <nl> + ou[\"format\"] = viper.GetString(\"format\") <nl> + } <nl> o := initalizer() <nl> err := o.Init(ou, logger) <nl> if err != nil { <nl> ", "msg": "get format fromviper if not set or is an empty string"}
{"diff_id": 4552, "repo": "karimra/gnmic", "sha": "4cfdd724dbfca8dbd67fc25b1ac0fde0f5a568ae", "time": "24.07.2020 19:11:43", "diff": "mmm a / cmd/get.go <nl> ppp b / cmd/get.go <nl>@@ -27,6 +27,7 @@ import ( <nl> \"github.com/openconfig/gnmi/proto/gnmi\" <nl> \"github.com/spf13/cobra\" <nl> \"github.com/spf13/viper\" <nl> + \"google.golang.org/protobuf/encoding/protojson\" <nl> \"google.golang.org/protobuf/encoding/prototext\" <nl> ) <nl> @@ -102,10 +103,22 @@ func printGetResponse(address string, response *gnmi.GetResponse) { <nl> if numTargets() > 1 && !viper.GetBool(\"no-prefix\") { <nl> printPrefix = fmt.Sprintf(\"[%s] \", address) <nl> } <nl> - if viper.GetString(\"format\") == \"prototext\" { <nl> - fmt.Printf(\"%s\\n\", indent(printPrefix, prototext.Format(response))) <nl> + switch viper.GetString(\"format\") { <nl> + case \"protojson\": <nl> + b, err := protojson.MarshalOptions{Multiline: true, Indent: \" \"}.Marshal(response) <nl> + if err != nil { <nl> + logger.Printf(\"error marshaling protojson msg: %v\", err) <nl> + if !viper.GetBool(\"log\") { <nl> + fmt.Printf(\"error marshaling protojson msg: %v\\n\", err) <nl> + } <nl> return <nl> } <nl> + fmt.Printf(\"%s\\n\", indent(printPrefix, string(b))) <nl> + return <nl> + case \"prototext\": <nl> + fmt.Printf(\"%s\\n\", indent(printPrefix, prototext.Format(response))) <nl> + return <nl> + case \"json\": <nl> for _, notif := range response.Notification { <nl> msg := new(msg) <nl> msg.Source = address <nl> @@ -136,6 +149,10 @@ func printGetResponse(address string, response *gnmi.GetResponse) { <nl> } <nl> fmt.Printf(\"%s%s\\n\", printPrefix, string(dMsg)) <nl> } <nl> + case \"event\": <nl> + fmt.Println(\"event format is not implemented for get command\") <nl> + return <nl> + } <nl> fmt.Println() <nl> } <nl> ", "msg": "add format options to get response printing"}
{"diff_id": 4578, "repo": "karimra/gnmic", "sha": "f62148eaa66693bb7d48c0f5a133fe261e3071d4", "time": "31.08.2020 19:10:15", "diff": "mmm a / collector/msg.go <nl> ppp b / collector/msg.go <nl>@@ -159,7 +159,7 @@ func getValue(updValue *gnmi.TypedValue) (interface{}, error) { <nl> case *gnmi.TypedValue_AnyVal: <nl> value = updValue.GetAnyVal() <nl> } <nl> - if value == nil { <nl> + if value == nil && len(jsondata) != 0 { <nl> err := json.Unmarshal(jsondata, &value) <nl> if err != nil { <nl> return nil, err <nl> ", "msg": "check for byte slice length before unmarshaling"}
{"diff_id": 4606, "repo": "karimra/gnmic", "sha": "a51807f35c5879acfd506bd65f8f1d85f11a71f5", "time": "29.09.2020 12:46:20", "diff": "mmm a / outputs/influxdb_output/influxdb_output.go <nl> ppp b / outputs/influxdb_output/influxdb_output.go <nl>@@ -194,6 +194,7 @@ func (i *InfluxDBOutput) health(ctx context.Context) { <nl> } <nl> func (i *InfluxDBOutput) worker(ctx context.Context, idx int) { <nl> + for { <nl> select { <nl> case <-ctx.Done(): <nl> i.logger.Printf(\"worker-%d terminating...\", idx) <nl> @@ -202,3 +203,4 @@ func (i *InfluxDBOutput) worker(ctx context.Context, idx int) { <nl> i.writer.WritePoint(influxdb2.NewPoint(ev.Name, ev.Tags, ev.Values, time.Unix(0, ev.Timestamp))) <nl> } <nl> } <nl> +} <nl> ", "msg": "fix influxdb output worker func"}
{"diff_id": 4623, "repo": "karimra/gnmic", "sha": "876cf70e81bf52aa09da86c9acb3c3eddfd9211f", "time": "12.10.2020 10:24:45", "diff": "mmm a / cmd/prompt.go <nl> ppp b / cmd/prompt.go <nl>@@ -3,6 +3,7 @@ package cmd <nl> import ( <nl> \"fmt\" <nl> \"io/ioutil\" <nl> + \"log\" <nl> \"os\" <nl> \"strings\" <nl> @@ -69,11 +70,17 @@ var promptModeCmd = &cobra.Command{ <nl> promptHistory = make([]string, 0, 256) <nl> home, err := homedir.Dir() <nl> if err != nil { <nl> - return err <nl> + if viper.GetBool(\"debug\") { <nl> + log.Printf(\"failed to get home directory: %v\", err) <nl> + } <nl> + return nil <nl> } <nl> content, err := ioutil.ReadFile(home + \"/.gnmic.history\") <nl> if err != nil { <nl> - return err <nl> + if viper.GetBool(\"debug\") { <nl> + log.Printf(\"failed to read history file: %v\", err) <nl> + } <nl> + return nil <nl> } <nl> history := strings.Split(string(content), \"\\n\") <nl> for i := range history { <nl> @@ -81,7 +88,7 @@ var promptModeCmd = &cobra.Command{ <nl> promptHistory = append(promptHistory, history[i]) <nl> } <nl> } <nl> - return err <nl> + return nil <nl> }, <nl> PostRun: func(cmd *cobra.Command, args []string) { <nl> cmd.ResetFlags() <nl> ", "msg": "prompt cmd does not fail if the history file cannot be read"}
{"diff_id": 4625, "repo": "karimra/gnmic", "sha": "1b884237c322f6dbea0e48f004709eef86cc72d7", "time": "13.10.2020 07:58:06", "diff": "mmm a / cmd/prompt.go <nl> ppp b / cmd/prompt.go <nl>@@ -142,6 +142,7 @@ func initPromptFlags(cmd *cobra.Command) { <nl> cmd.Flags().String(\"prefix-color\", \"yellow\", \"terminal prefix color\") <nl> cmd.Flags().String(\"suggestions-bg-color\", \"black\", \"suggestion box background color\") <nl> cmd.Flags().String(\"description-bg-color\", \"yellow\", \"description box background color\") <nl> + cmd.Flags().Bool(\"suggest-all-flags\", false, \"suggest local as well as inherited flags of subcommands\") <nl> viper.BindPFlag(\"prompt-file\", cmd.LocalFlags().Lookup(\"file\")) <nl> viper.BindPFlag(\"prompt-exclude\", cmd.LocalFlags().Lookup(\"exclude\")) <nl> viper.BindPFlag(\"prompt-dir\", cmd.LocalFlags().Lookup(\"dir\")) <nl> @@ -149,6 +150,7 @@ func initPromptFlags(cmd *cobra.Command) { <nl> viper.BindPFlag(\"prompt-prefix-color\", cmd.LocalFlags().Lookup(\"prefix-color\")) <nl> viper.BindPFlag(\"prompt-suggestions-bg-color\", cmd.LocalFlags().Lookup(\"suggestions-bg-color\")) <nl> viper.BindPFlag(\"prompt-description-bg-color\", cmd.LocalFlags().Lookup(\"description-bg-color\")) <nl> + viper.BindPFlag(\"prompt-suggest-all-flags\", cmd.LocalFlags().Lookup(\"suggest-all-flags\")) <nl> } <nl> func findMatchedXPATH(entry *yang.Entry, word string, cursor int) []goprompt.Suggest { <nl> @@ -419,13 +421,9 @@ func (co cmdPrompt) Run() { <nl> } <nl> func findSuggestions(co cmdPrompt, doc goprompt.Document) []goprompt.Suggest { <nl> - showLocalFlags := false <nl> command := co.RootCmd <nl> args := strings.Fields(doc.CurrentLine()) <nl> if found, _, err := command.Find(args); err == nil { <nl> - if command != found { <nl> - showLocalFlags = true <nl> - } <nl> command = found <nl> } <nl> @@ -455,33 +453,18 @@ func findSuggestions(co cmdPrompt, doc goprompt.Document) []goprompt.Suggest { <nl> } <nl> } <nl> } <nl> - if showLocalFlags { <nl> - // load local flags of the command <nl> - addFlags := func(flag *pflag.Flag) { <nl> - if flag.Hidden { <nl> - return <nl> - } <nl> - suggestions = append(suggestions, goprompt.Suggest{Text: \"--\" + flag.Name, Description: flag.Usage}) <nl> - } <nl> - command.LocalFlags().VisitAll(addFlags) <nl> - // command.InheritedFlags().VisitAll(addFlags) <nl> - } else { <nl> - <nl> - // persistent flags are shown if run. <nl> addFlags := func(flag *pflag.Flag) { <nl> if flag.Hidden { <nl> return <nl> } <nl> - // if strings.HasPrefix(doc.GetWordBeforeCursor(), \"--\") { <nl> - // suggestions = append(suggestions, goprompt.Suggest{Text: \"--\" + flag.Name, Description: flag.Usage}) <nl> - // } else if strings.HasPrefix(doc.GetWordBeforeCursor(), \"-\") && flag.Shorthand != \"\" { <nl> - // suggestions = append(suggestions, goprompt.Suggest{Text: \"-\" + flag.Shorthand, Description: flag.Usage}) <nl> - // } <nl> - if strings.HasPrefix(doc.GetWordBeforeCursor(), \"-\") { <nl> suggestions = append(suggestions, goprompt.Suggest{Text: \"--\" + flag.Name, Description: flag.Usage}) <nl> } <nl> - } <nl> + // load local flags <nl> command.LocalFlags().VisitAll(addFlags) <nl> + if viper.GetBool(\"prompt-suggest-all-flags\") { <nl> + // load inherited flags <nl> + command.InheritedFlags().VisitAll(addFlags) <nl> } <nl> + <nl> return goprompt.FilterHasPrefix(suggestions, doc.GetWordBeforeCursor(), true) <nl> } <nl> ", "msg": "added --suggest-all-flags flag to prompt command to enable suggesting all flags under subcommands"}
{"diff_id": 4662, "repo": "karimra/gnmic", "sha": "7e28d1e583a597187df789480a051fe238cd93d1", "time": "20.10.2020 20:57:50", "diff": "mmm a / cmd/prompt.go <nl> ppp b / cmd/prompt.go <nl>@@ -145,6 +145,7 @@ func initPromptFlags(cmd *cobra.Command) { <nl> cmd.Flags().String(\"suggestions-bg-color\", \"dark_blue\", \"suggestion box background color\") <nl> cmd.Flags().String(\"description-bg-color\", \"dark_gray\", \"description box background color\") <nl> cmd.Flags().Bool(\"suggest-all-flags\", false, \"suggest local as well as inherited flags of subcommands\") <nl> + cmd.Flags().Bool(\"description-with-prefix\", false, \"show YANG module prefix in XPATH suggestion description\") <nl> viper.BindPFlag(\"prompt-file\", cmd.LocalFlags().Lookup(\"file\")) <nl> viper.BindPFlag(\"prompt-exclude\", cmd.LocalFlags().Lookup(\"exclude\")) <nl> viper.BindPFlag(\"prompt-dir\", cmd.LocalFlags().Lookup(\"dir\")) <nl> @@ -153,6 +154,7 @@ func initPromptFlags(cmd *cobra.Command) { <nl> viper.BindPFlag(\"prompt-suggestions-bg-color\", cmd.LocalFlags().Lookup(\"suggestions-bg-color\")) <nl> viper.BindPFlag(\"prompt-description-bg-color\", cmd.LocalFlags().Lookup(\"description-bg-color\")) <nl> viper.BindPFlag(\"prompt-suggest-all-flags\", cmd.LocalFlags().Lookup(\"suggest-all-flags\")) <nl> + viper.BindPFlag(\"prompt-description-with-prefix\", cmd.LocalFlags().Lookup(\"description-with-prefix\")) <nl> } <nl> func findMatchedXPATH(entry *yang.Entry, word string, cursor int) []goprompt.Suggest { <nl> @@ -231,6 +233,12 @@ func buildXPATHDescription(entry *yang.Entry) string { <nl> } <nl> sb.WriteString(getPermissions(entry)) <nl> sb.WriteString(\" \") <nl> + if viper.GetBool(\"prompt-description-with-prefix\") { <nl> + if entry.Prefix != nil { <nl> + sb.WriteString(entry.Prefix.Name) <nl> + sb.WriteString(\": \") <nl> + } <nl> + } <nl> sb.WriteString(entry.Description) <nl> return sb.String() <nl> } <nl> ", "msg": "add yang module prefix to xpath description"}
{"diff_id": 4691, "repo": "karimra/gnmic", "sha": "c029b4e64ffe18826d86fa1a521953edcf3c2af1", "time": "10.11.2020 22:18:52", "diff": "mmm a / cmd/subscribe.go <nl> ppp b / cmd/subscribe.go <nl>@@ -314,7 +314,7 @@ func getSubscriptions() (map[string]*collector.SubscriptionConfig, error) { <nl> } <nl> if len(paths) > 0 { <nl> sub := new(collector.SubscriptionConfig) <nl> - sub.Name = \"default\" <nl> + sub.Name = fmt.Sprintf(\"default-%d\", time.Now().Unix()) <nl> sub.Paths = paths <nl> sub.Prefix = viper.GetString(\"subscribe-prefix\") <nl> sub.Target = viper.GetString(\"subscribe-target\") <nl> ", "msg": "change default subscription name to include a timestamp"}
{"diff_id": 4706, "repo": "karimra/gnmic", "sha": "57e2c1da6065c2bec46fc04f24cc1d608fcafbc0", "time": "26.11.2020 00:31:01", "diff": "mmm a / outputs/tcp_output/tcp_output.go <nl> ppp b / outputs/tcp_output/tcp_output.go <nl>@@ -17,7 +17,7 @@ import ( <nl> const ( <nl> defaultRetryTimer = 2 * time.Second <nl> - numWorkers = 1 <nl> + defaultNumWorkers = 1 <nl> ) <nl> func init() { <nl> @@ -45,6 +45,7 @@ type Config struct { <nl> Format string `mapstructure:\"format,omitempty\"` <nl> KeepAlive time.Duration `mapstructure:\"keep-alive,omitempty\"` <nl> RetryInterval time.Duration `mapstructure:\"retry-interval,omitempty\"` <nl> + NumWorkers int `mapstructure:\"num-workers,omitempty\"` <nl> } <nl> func (t *TCPOutput) Init(ctx context.Context, cfg map[string]interface{}, logger *log.Logger) error { <nl> @@ -70,6 +71,9 @@ func (t *TCPOutput) Init(ctx context.Context, cfg map[string]interface{}, logger <nl> if t.Cfg.RetryInterval == 0 { <nl> t.Cfg.RetryInterval = defaultRetryTimer <nl> } <nl> + if t.Cfg.NumWorkers < 1 { <nl> + t.Cfg.NumWorkers = defaultNumWorkers <nl> + } <nl> t.mo = &collector.MarshalOptions{Format: t.Cfg.Format} <nl> go func() { <nl> <-ctx.Done() <nl> @@ -77,8 +81,8 @@ func (t *TCPOutput) Init(ctx context.Context, cfg map[string]interface{}, logger <nl> }() <nl> ctx, t.cancelFn = context.WithCancel(ctx) <nl> - for i := 0; i < numWorkers; i++ { <nl> - go t.start(ctx) <nl> + for i := 0; i < t.Cfg.NumWorkers; i++ { <nl> + go t.start(ctx, i) <nl> } <nl> return nil <nl> } <nl> @@ -113,17 +117,18 @@ func (t *TCPOutput) String() string { <nl> } <nl> return string(b) <nl> } <nl> -func (t *TCPOutput) start(ctx context.Context) { <nl> +func (t *TCPOutput) start(ctx context.Context, idx int) { <nl> + workerLogPrefix := fmt.Sprintf(\"worker-%d\", idx) <nl> START: <nl> tcpAddr, err := net.ResolveTCPAddr(\"tcp\", t.Cfg.Address) <nl> if err != nil { <nl> - t.logger.Printf(\"failed to resolve address: %v\", err) <nl> + t.logger.Printf(\"%s failed to resolve address: %v\", workerLogPrefix, err) <nl> time.Sleep(t.Cfg.RetryInterval) <nl> goto START <nl> } <nl> conn, err := net.DialTCP(\"tcp\", nil, tcpAddr) <nl> if err != nil { <nl> - t.logger.Printf(\"failed to dial TCP: %v\", err) <nl> + t.logger.Printf(\"%s failed to dial TCP: %v\", workerLogPrefix, err) <nl> time.Sleep(t.Cfg.RetryInterval) <nl> goto START <nl> } <nl> @@ -132,7 +137,6 @@ START: <nl> conn.SetKeepAlive(true) <nl> conn.SetKeepAlivePeriod(t.Cfg.KeepAlive) <nl> } <nl> - <nl> defer t.Close() <nl> for { <nl> select { <nl> @@ -144,7 +148,7 @@ START: <nl> } <nl> _, err = conn.Write(b) <nl> if err != nil { <nl> - t.logger.Printf(\"failed sending tcp bytes: %v\", err) <nl> + t.logger.Printf(\"%s failed sending tcp bytes: %v\", workerLogPrefix, err) <nl> conn.Close() <nl> time.Sleep(t.Cfg.RetryInterval) <nl> goto START <nl> ", "msg": "add configurable number of workers to tcp output"}
{"diff_id": 4709, "repo": "karimra/gnmic", "sha": "2fb1e9ac54e38eb81fe65fa920e231842540410d", "time": "26.11.2020 08:56:32", "diff": "mmm a / cmd/prompt.go <nl> ppp b / cmd/prompt.go <nl>@@ -151,8 +151,6 @@ var promptModeCmd = &cobra.Command{ <nl> } <nl> } <nl> // <nl> - ctx, cancel := context.WithCancel(context.Background()) <nl> - defer cancel() <nl> debug := viper.GetBool(\"debug\") <nl> targetsConfig, err := createTargets() <nl> if err != nil { <nl> @@ -187,7 +185,7 @@ var promptModeCmd = &cobra.Command{ <nl> coll = collector.NewCollector(cfg, targetsConfig, <nl> collector.WithDialOptions(createCollectorDialOpts()), <nl> collector.WithSubscriptions(subscriptionsConfig), <nl> - collector.WithOutputs(ctx, outs, logger), <nl> + collector.WithOutputs(context.Background(), outs, logger), <nl> collector.WithLogger(logger), <nl> ) <nl> } <nl> ", "msg": "use background ctx to initialize coll in prompt cmd"}
{"diff_id": 4716, "repo": "karimra/gnmic", "sha": "ea709122c564b2c33a98d9f82023f5dece701ffa", "time": "27.11.2020 01:47:28", "diff": "mmm a / cmd/prompt.go <nl> ppp b / cmd/prompt.go <nl>@@ -1204,16 +1204,18 @@ func findSuggestions(co cmdPrompt, doc goprompt.Document) []goprompt.Suggest { <nl> func resolveGlobs(globs []string) ([]string, error) { <nl> results := make([]string, 0, len(globs)) <nl> for _, pattern := range globs { <nl> - if strings.ContainsAny(pattern, `*?[`) { <nl> + for _, p := range strings.Split(pattern, \",\") { <nl> + if strings.ContainsAny(p, `*?[`) { <nl> // is a glob pattern <nl> - matches, err := filepath.Glob(pattern) <nl> + matches, err := filepath.Glob(p) <nl> if err != nil { <nl> return nil, err <nl> } <nl> results = append(results, matches...) <nl> } else { <nl> // is not a glob pattern ( file or dir ) <nl> - results = append(results, pattern) <nl> + results = append(results, p) <nl> + } <nl> } <nl> } <nl> return results, nil <nl> ", "msg": "split read paths by comma in case they were specified in the config file"}
{"diff_id": 4727, "repo": "karimra/gnmic", "sha": "b75f570508c46bac8930333a54c115bd4855d7f6", "time": "30.11.2020 07:33:21", "diff": "mmm a / collector/collector.go <nl> ppp b / collector/collector.go <nl>@@ -199,12 +199,14 @@ func (c *Collector) InitOutput(ctx context.Context, name string) { <nl> c.logger.Printf(\"starting output type %s\", outType) <nl> if initializer, ok := outputs.Outputs[outType.(string)]; ok { <nl> out := initializer() <nl> + if c.reg != nil { <nl> for _, m := range out.Metrics() { <nl> err := c.reg.Register(m) <nl> if err != nil { <nl> c.logger.Printf(\"failed to register output '%s' metric : %v\", name, err) <nl> } <nl> } <nl> + } <nl> go out.Init(ctx, cfg, c.logger) <nl> c.Outputs[name] = out <nl> } <nl> ", "msg": "register metrics only if the prometheus Registry is initialized"}
{"diff_id": 4746, "repo": "karimra/gnmic", "sha": "97eef6dc6c538e9d270a8e55f36e6d37e254bc85", "time": "20.01.2021 11:29:52", "diff": "mmm a / inputs/kafka_input/kafka_input.go <nl> ppp b / inputs/kafka_input/kafka_input.go <nl>@@ -23,6 +23,8 @@ const ( <nl> defaultTopic = \"telemetry\" <nl> defaultNumWorkers = 1 <nl> defaultSessionTimeout = 10 * time.Second <nl> + defaultHeartbeatInterval = 3 * time.Second <nl> + defaultRecoveryWaitTime = 2 * time.Second <nl> defaultAddress = \"localhost:9092\" <nl> defaultGroupID = \"gnmic-consumers\" <nl> ) <nl> @@ -54,8 +56,8 @@ type Config struct { <nl> Address string `mapstructure:\"address,omitempty\"` <nl> Topic string `mapstructure:\"topic,omitempty\"` <nl> GroupID string `mapstructure:\"group-id,omitempty\"` <nl> - MaxRetry int `mapstructure:\"max-retry,omitempty\"` <nl> - Timeout time.Duration `mapstructure:\"timeout,omitempty\"` <nl> + SessionTimeout time.Duration `mapstructure:\"session-timeout,omitempty\"` <nl> + HeartbeatInterval time.Duration `mapstructure:\"heartbeat-interval,omitempty\"` <nl> RecoveryWaitTime time.Duration `mapstructure:\"recovery-wait-time,omitempty\"` <nl> Version string `mapstructure:\"version,omitempty\"` <nl> Format string `mapstructure:\"format,omitempty\"` <nl> @@ -88,10 +90,11 @@ func (k *KafkaInput) Start(ctx context.Context, cfg map[string]interface{}, opts <nl> func (k *KafkaInput) worker(ctx context.Context, idx int) { <nl> defer k.wg.Done() <nl> config := sarama.NewConfig() <nl> - config.Version = sarama.V0_11_0_1 <nl> + config.Version = k.Cfg.kafkaVersion <nl> config.ClientID = fmt.Sprintf(\"%s-%d\", k.Cfg.Name, idx) <nl> config.Consumer.Return.Errors = true <nl> - config.Consumer.Group.Session.Timeout = k.Cfg.Timeout <nl> + config.Consumer.Group.Session.Timeout = k.Cfg.SessionTimeout <nl> + config.Consumer.Group.Heartbeat.Interval = k.Cfg.HeartbeatInterval <nl> config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRange <nl> // TODO: finish kafka config <nl> @@ -232,12 +235,18 @@ func (k *KafkaInput) setDefaults() error { <nl> if k.Cfg.NumWorkers <= 0 { <nl> k.Cfg.NumWorkers = defaultNumWorkers <nl> } <nl> - if k.Cfg.Timeout <= 2*time.Millisecond { <nl> - k.Cfg.Timeout = defaultSessionTimeout <nl> + if k.Cfg.SessionTimeout <= 2*time.Millisecond { <nl> + k.Cfg.SessionTimeout = defaultSessionTimeout <nl> + } <nl> + if k.Cfg.HeartbeatInterval <= 1*time.Millisecond { <nl> + k.Cfg.HeartbeatInterval = defaultHeartbeatInterval <nl> } <nl> if k.Cfg.GroupID == \"\" { <nl> k.Cfg.GroupID = defaultGroupID <nl> } <nl> + if k.Cfg.RecoveryWaitTime <= 0 { <nl> + k.Cfg.RecoveryWaitTime = defaultRecoveryWaitTime <nl> + } <nl> return nil <nl> } <nl> ", "msg": "add session-timeout, heartbeat-interval and recovery-wait-time to kafka_input"}
{"diff_id": 4754, "repo": "karimra/gnmic", "sha": "07336148994643d60d11973fdd3d04515977b402", "time": "11.02.2021 22:02:59", "diff": "mmm a / config/config.go <nl> ppp b / config/config.go <nl>@@ -154,8 +154,8 @@ func (c *Config) Load(file string) error { <nl> if err != nil { <nl> return err <nl> } <nl> - c.FileConfig.AddConfigPath(home) <nl> c.FileConfig.AddConfigPath(\".\") <nl> + c.FileConfig.AddConfigPath(home) <nl> c.FileConfig.AddConfigPath(xdg.ConfigHome) <nl> c.FileConfig.AddConfigPath(xdg.ConfigHome + \"/gnmic\") <nl> c.FileConfig.SetConfigName(configName) <nl> @@ -174,7 +174,6 @@ func (c *Config) Load(file string) error { <nl> return nil <nl> } <nl> - <nl> func (c *Config) SetLogger() { <nl> if c.Globals.LogFile != \"\" { <nl> f, err := os.OpenFile(c.Globals.LogFile, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666) <nl> ", "msg": "changed order of config path search\ncheck the local (\".\") directory before the home directory.  this would\nallow for local override in a given project, etc."}
{"diff_id": 4759, "repo": "karimra/gnmic", "sha": "78ae9e899f52bd0f2d5c51d3a937bb61ea9d4ace", "time": "24.02.2021 11:31:13", "diff": "mmm a / app/app.go <nl> ppp b / app/app.go <nl>@@ -139,8 +139,10 @@ func (a *App) PreRun(_ *cobra.Command, args []string) error { <nl> func (a *App) PrintMsg(address string, msgName string, msg proto.Message) error { <nl> a.printLock.Lock() <nl> defer a.printLock.Unlock() <nl> + if a.Config.PrintRequest { <nl> fmt.Fprint(os.Stderr, msgName) <nl> fmt.Fprintln(os.Stderr, \"\") <nl> + } <nl> printPrefix := \"\" <nl> if len(a.Config.TargetsList()) > 1 && !a.Config.NoPrefix { <nl> printPrefix = fmt.Sprintf(\"[%s] \", address) <nl> ", "msg": "print the response msg name only if the request is printed"}
{"diff_id": 4796, "repo": "karimra/gnmic", "sha": "9db94641f8244b44e33dd7539175bf6416a04140", "time": "13.07.2021 22:46:44", "diff": "mmm a / outputs/kafka_output/kafka_output.go <nl> ppp b / outputs/kafka_output/kafka_output.go <nl>@@ -2,6 +2,8 @@ package kafka_output <nl> import ( <nl> \"context\" <nl> + \"crypto/tls\" <nl> + \"crypto/x509\" <nl> \"encoding/json\" <nl> \"errors\" <nl> \"fmt\" <nl> @@ -66,6 +68,7 @@ type Config struct { <nl> Topic string `mapstructure:\"topic,omitempty\"` <nl> Name string `mapstructure:\"name,omitempty\"` <nl> SASL *sasl `mapstructure:\"sasl,omitempty\"` <nl> + TLS *tlsConfig `mapstructure:\"tls,omitempty\"` <nl> MaxRetry int `mapstructure:\"max-retry,omitempty\"` <nl> Timeout time.Duration `mapstructure:\"timeout,omitempty\"` <nl> RecoveryWaitTime time.Duration `mapstructure:\"recovery-wait-time,omitempty\"` <nl> @@ -86,6 +89,13 @@ type sasl struct { <nl> TokenURL string `mapstructure:\"token-url,omitempty\"` <nl> } <nl> +type tlsConfig struct { <nl> + CaFile string `mapstructure:\"ca-file,omitempty\"` <nl> + KeyFile string `mapstructure:\"key-file,omitempty\"` <nl> + CertFile string `mapstructure:\"cert-file,omitempty\"` <nl> + SkipVerify bool `mapstructure:\"skip-verify,omitempty\"` <nl> +} <nl> + <nl> func (k *KafkaOutput) String() string { <nl> b, err := json.Marshal(k) <nl> if err != nil { <nl> @@ -160,7 +170,10 @@ func (k *KafkaOutput) Init(ctx context.Context, name string, cfg map[string]inte <nl> } <nl> } <nl> - config := k.createConfig() <nl> + config, err := k.createConfig() <nl> + if err != nil { <nl> + return err <nl> + } <nl> ctx, k.cancelFn = context.WithCancel(ctx) <nl> k.wg.Add(k.Cfg.NumWorkers) <nl> for i := 0; i < k.Cfg.NumWorkers; i++ { <nl> @@ -339,9 +352,10 @@ func (k *KafkaOutput) SetName(name string) { <nl> func (k *KafkaOutput) SetClusterName(name string) {} <nl> -func (k *KafkaOutput) createConfig() *sarama.Config { <nl> +func (k *KafkaOutput) createConfig() (*sarama.Config, error) { <nl> cfg := sarama.NewConfig() <nl> cfg.ClientID = k.Cfg.Name <nl> + // SASL_PLAINTEXT or SASL_SSL <nl> if k.Cfg.SASL != nil { <nl> cfg.Net.SASL.Enable = true <nl> cfg.Net.SASL.User = k.Cfg.SASL.User <nl> @@ -360,11 +374,36 @@ func (k *KafkaOutput) createConfig() *sarama.Config { <nl> cfg.Net.SASL.TokenProvider = oauthbearer.NewTokenProvider(cfg.Net.SASL.User, cfg.Net.SASL.Password, k.Cfg.SASL.TokenURL) <nl> } <nl> } <nl> - // <nl> + // SSL or SASL_SSL <nl> + if k.Cfg.TLS != nil { <nl> + cfg.Net.TLS.Enable = true <nl> + tlscfg := &tls.Config{ <nl> + InsecureSkipVerify: k.Cfg.TLS.SkipVerify, <nl> + } <nl> + if k.Cfg.TLS.CaFile != \"\" { <nl> + caCert, err := ioutil.ReadFile(k.Cfg.TLS.CaFile) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"failed to read tls.ca-file: %v\", err) <nl> + } <nl> + caCertPool := x509.NewCertPool() <nl> + caCertPool.AppendCertsFromPEM(caCert) <nl> + tlscfg.RootCAs = caCertPool <nl> + } <nl> + if k.Cfg.TLS.CertFile != \"\" && k.Cfg.TLS.KeyFile != \"\" { <nl> + certificate, err := tls.LoadX509KeyPair(k.Cfg.TLS.CertFile, k.Cfg.TLS.KeyFile) <nl> + if err != nil { <nl> + return nil, fmt.Errorf(\"failed to read the keyPair tls.cert-file and tls.key-file: %v\", err) <nl> + } <nl> + tlscfg.Certificates = []tls.Certificate{certificate} <nl> + tlscfg.BuildNameToCertificate() <nl> + } <nl> + cfg.Net.TLS.Config = tlscfg <nl> + } <nl> + <nl> cfg.Producer.Retry.Max = k.Cfg.MaxRetry <nl> cfg.Producer.RequiredAcks = sarama.WaitForAll <nl> cfg.Producer.Return.Successes = true <nl> cfg.Producer.Timeout = k.Cfg.Timeout <nl> - return cfg <nl> + return cfg, nil <nl> } <nl> ", "msg": "add TLS config to kafka output"}
{"diff_id": 4797, "repo": "karimra/gnmic", "sha": "cb388d6a272967d8f293e0cae35b1413eea47fb2", "time": "13.07.2021 23:13:11", "diff": "mmm a / collector/subscription.go <nl> ppp b / collector/subscription.go <nl>@@ -108,7 +108,7 @@ func (sc *SubscriptionConfig) CreateSubscribeRequest(target string) (*gnmi.Subsc <nl> subscriptions[i].SampleInterval = uint64(sc.SampleInterval.Nanoseconds()) <nl> } <nl> subscriptions[i].SuppressRedundant = sc.SuppressRedundant <nl> - if subscriptions[i].SuppressRedundant { <nl> + if subscriptions[i].SuppressRedundant && sc.HeartbeatInterval != nil { <nl> subscriptions[i].HeartbeatInterval = uint64(sc.HeartbeatInterval.Nanoseconds()) <nl> } <nl> } <nl> ", "msg": "fix subscribe request creation when suppress-redundant is set but not heartbeat-interval"}
{"diff_id": 4811, "repo": "karimra/gnmic", "sha": "091fe9db90ab8afa7d3ca9a49421e06513775b0d", "time": "18.08.2021 12:51:28", "diff": "mmm a / formatters/event_convert/event_convert.go <nl> ppp b / formatters/event_convert/event_convert.go <nl>@@ -224,7 +224,13 @@ func convertToUint(i interface{}) (uint, error) { <nl> func convertToFloat(i interface{}) (float64, error) { <nl> switch i := i.(type) { <nl> case []uint8: <nl> + if len(i) == 4 { <nl> ij := math.Float32frombits(binary.BigEndian.Uint32([]byte(i))) <nl> + } else if len(i) == 8 { <nl> + ij := math.Float64frombits(binary.BigEndian.Uint64([]byte(i))) <nl> + } else { <nl> + return 0, nil <nl> + } <nl> return float64(ij), nil <nl> case string: <nl> iv, err := strconv.ParseFloat(i, 64) <nl> ", "msg": "Add check len for Binary Float on 32 bits and 64 bits"}
{"diff_id": 4824, "repo": "karimra/gnmic", "sha": "5c944facfee3426ba809209c60b2ab36ecfa8928", "time": "17.10.2021 16:24:52", "diff": "mmm a / app/clustering.go <nl> ppp b / app/clustering.go <nl>@@ -19,7 +19,7 @@ import ( <nl> const ( <nl> defaultClusterName = \"default-cluster\" <nl> - retryTimer = 2 * time.Second <nl> + retryTimer = 10 * time.Second <nl> lockWaitTime = 100 * time.Millisecond <nl> apiServiceName = \"gnmic-api\" <nl> ) <nl> @@ -132,12 +132,14 @@ START: <nl> } <nl> ctx, cancel := context.WithCancel(a.ctx) <nl> defer cancel() <nl> + go func() { <nl> go a.watchMembers(ctx) <nl> a.Logger.Printf(\"leader waiting %s before dispatching targets\", a.Config.Clustering.LeaderWaitTimer) <nl> time.Sleep(a.Config.Clustering.LeaderWaitTimer) <nl> a.Logger.Printf(\"leader done waiting, starting loader and dispatching targets\") <nl> go a.startLoader(ctx) <nl> go a.dispatchTargets(ctx) <nl> + }() <nl> doneCh, errCh := a.locker.KeepLock(a.ctx, leaderKey) <nl> select { <nl> ", "msg": "run cluster leader wait inside a goroutine to avoid consul session expiry"}
{"diff_id": 4840, "repo": "karimra/gnmic", "sha": "034d7f8a6653ec50dd65b430cc98b8efba3a1817", "time": "15.01.2022 18:13:45", "diff": "mmm a / target/target.go <nl> ppp b / target/target.go <nl>@@ -3,6 +3,7 @@ package target <nl> import ( <nl> \"context\" <nl> \"fmt\" <nl> + \"os\" <nl> \"strings\" <nl> \"sync\" <nl> @@ -76,6 +77,17 @@ func (t *Target) CreateGNMIClient(ctx context.Context, opts ...grpc.DialOption) <nl> if err != nil { <nl> return err <nl> } <nl> + <nl> + // TODO: add flag to enable tls pre master key log dump <nl> + if true { <nl> + logPath := t.Config.Name + \".tlskey.log\" <nl> + w, err := os.Create(logPath) <nl> + if err != nil { <nl> + return err <nl> + } <nl> + tlsConfig.KeyLogWriter = w <nl> + } <nl> + <nl> tOpts = append(tOpts, grpc.WithTransportCredentials(credentials.NewTLS(tlsConfig))) <nl> if t.Config.Token != nil && *t.Config.Token != \"\" { <nl> tOpts = append(tOpts, <nl> ", "msg": "prototype for tls premaster key logging"}
{"diff_id": 4845, "repo": "karimra/gnmic", "sha": "7f31aec79174200ffd26aac8f42a8240f83be4a2", "time": "21.02.2022 14:49:55", "diff": "mmm a / target/target.go <nl> ppp b / target/target.go <nl>@@ -68,7 +68,7 @@ func (t *Target) CreateGNMIClient(ctx context.Context, opts ...grpc.DialOption) <nl> return err <nl> } <nl> opts = append(opts, tOpts...) <nl> - <nl> + opts = append(opts, grpc.WithBlock()) <nl> // create a gRPC connection <nl> addrs := strings.Split(t.Config.Address, \",\") <nl> numAddrs := len(addrs) <nl> ", "msg": "add grpc withBlock option to grpc Dial options"}
{"diff_id": 4851, "repo": "karimra/gnmic", "sha": "0d50b8934940aa7db1992f272d021f4127a5eb32", "time": "01.04.2022 17:27:25", "diff": "mmm a / cmd/listen.go <nl> ppp b / cmd/listen.go <nl>@@ -71,6 +71,16 @@ func newListenCmd() *cobra.Command { <nl> } <nl> gApp.Logger.Printf(\"loaded proto files\") <nl> } <nl> + // read config <nl> + actCfg, err := gApp.Config.GetActions() <nl> + if err != nil { <nl> + return fmt.Errorf(\"failed reading actions config: %v\", err) <nl> + } <nl> + procCfg, err := gApp.Config.GetEventProcessors() <nl> + if err != nil { <nl> + return fmt.Errorf(\"failed reading event processors config: %v\", err) <nl> + } <nl> + <nl> server.Outputs = make(map[string]outputs.Output) <nl> outCfgs, err := gApp.Config.GetOutputs() <nl> if err != nil { <nl> @@ -80,7 +90,12 @@ func newListenCmd() *cobra.Command { <nl> if outType, ok := outConf[\"type\"]; ok { <nl> if initializer, ok := outputs.Outputs[outType.(string)]; ok { <nl> out := initializer() <nl> - go out.Init(ctx, name, outConf, outputs.WithLogger(gApp.Logger)) <nl> + go out.Init(ctx, name, outConf, <nl> + outputs.WithLogger(gApp.Logger), <nl> + outputs.WithEventProcessors(procCfg, gApp.Logger, nil, actCfg), <nl> + outputs.WithName(gApp.Config.InstanceName), <nl> + outputs.WithClusterName(gApp.Config.ClusterName), <nl> + ) <nl> server.Outputs[name] = out <nl> } <nl> } <nl> ", "msg": "enable event processors when using the listen command"}
{"diff_id": 4856, "repo": "karimra/gnmic", "sha": "97993d2171a0bdb9dba17c9c2b0d6a87a971515a", "time": "06.06.2022 15:04:31", "diff": "mmm a / outputs/influxdb_output/influxdb_output.go <nl> ppp b / outputs/influxdb_output/influxdb_output.go <nl>@@ -36,7 +36,7 @@ const ( <nl> func init() { <nl> outputs.Register(\"influxdb\", func() outputs.Output { <nl> return &InfluxDBOutput{ <nl> - Cfg: &Config{GnmiCacheConfig: &cache.GnmiCacheConfig{}}, <nl> + Cfg: &Config{}, <nl> eventChan: make(chan *formatters.EventMsg), <nl> reset: make(chan struct{}), <nl> startSig: make(chan struct{}), <nl> @@ -151,10 +151,10 @@ func (i *InfluxDBOutput) Init(ctx context.Context, name string, cfg map[string]i <nl> i.Cfg.HealthCheckPeriod = defaultHealthCheckPeriod <nl> } <nl> if i.Cfg.GnmiCacheConfig != nil { <nl> - i.initCache() <nl> if i.Cfg.CacheFlushTimer == 0 { <nl> i.Cfg.CacheFlushTimer = defaultCacheFlushTimer <nl> } <nl> + i.initCache() <nl> } <nl> iopts := influxdb2.DefaultOptions(). <nl> ", "msg": "minor influx cache bugfixes\nEnsure cache config is nil at init\nSet the cache flush timer prior to cache init"}
{"diff_id": 4880, "repo": "openshift/machine-api-operator", "sha": "eab2b07b7783c76454b7aa2d5568d2588b2f4e21", "time": "26.02.2019 15:46:51", "diff": "mmm a / cmd/machine-api-operator/start.go <nl> ppp b / cmd/machine-api-operator/start.go <nl>package main <nl> import ( <nl> + \"context\" <nl> \"flag\" <nl> \"github.com/golang/glog\" <nl> @@ -46,26 +47,25 @@ func runStartCmd(cmd *cobra.Command, args []string) { <nl> glog.Fatalf(\"error creating clients: %v\", err) <nl> } <nl> stopCh := make(chan struct{}) <nl> - run := func(stop <-chan struct{}) { <nl> - ctx := CreateControllerContext(cb, stopCh, componentNamespace) <nl> - if err := startControllers(ctx); err != nil { <nl> - glog.Fatalf(\"error starting controllers: %v\", err) <nl> - } <nl> - <nl> - ctx.KubeNamespacedInformerFactory.Start(ctx.Stop) <nl> - close(ctx.InformersStarted) <nl> - <nl> - select {} <nl> - } <nl> - <nl> - leaderelection.RunOrDie(leaderelection.LeaderElectionConfig{ <nl> + leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{ <nl> Lock: CreateResourceLock(cb, componentNamespace, componentName), <nl> LeaseDuration: LeaseDuration, <nl> RenewDeadline: RenewDeadline, <nl> RetryPeriod: RetryPeriod, <nl> Callbacks: leaderelection.LeaderCallbacks{ <nl> - OnStartedLeading: run, <nl> + OnStartedLeading: func(ctx context.Context) { <nl> + ctrlCtx := CreateControllerContext(cb, stopCh, componentNamespace) <nl> + <nl> + if err := startControllers(ctrlCtx); err != nil { <nl> + glog.Fatalf(\"error starting controllers: %v\", err) <nl> + } <nl> + <nl> + ctrlCtx.KubeNamespacedInformerFactory.Start(ctrlCtx.Stop) <nl> + close(ctrlCtx.InformersStarted) <nl> + <nl> + select {} <nl> + }, <nl> OnStoppedLeading: func() { <nl> glog.Fatalf(\"Leader election lost\") <nl> }, <nl> ", "msg": "Update mao controller to properly use updated deps"}
{"diff_id": 4881, "repo": "openshift/machine-api-operator", "sha": "bfbea130b6718459732847214730e911986176b2", "time": "28.02.2019 11:10:30", "diff": "mmm a / cmd/nodelink-controller/main.go <nl> ppp b / cmd/nodelink-controller/main.go <nl>@@ -375,7 +375,6 @@ func (c *Controller) processNode(node *corev1.Node) error { <nl> } <nl> if matchingMachine == nil { <nl> - glog.Warning(\"No machine was found for node %q\", node.Name) <nl> return fmt.Errorf(\"no machine was found for node: %q\", node.Name) <nl> } <nl> ", "msg": "Remove printing of Warning message\nWe return an error and the error is printed. Additionally, the Warning\nmessage had little value as it did not indicate which node (by name)\nwas not found. This removes a lot of duplicated output from the logs."}
{"diff_id": 4887, "repo": "openshift/machine-api-operator", "sha": "0cda2176f13543733f71f8a3b98e87c2d3de5e4d", "time": "01.04.2019 10:30:21", "diff": "mmm a / pkg/operator/operator.go <nl> ppp b / pkg/operator/operator.go <nl>@@ -11,6 +11,7 @@ import ( <nl> osconfigv1 \"github.com/openshift/api/config/v1\" <nl> v1 \"k8s.io/api/core/v1\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> + \"k8s.io/apimachinery/pkg/runtime\" <nl> utilruntime \"k8s.io/apimachinery/pkg/util/runtime\" <nl> \"k8s.io/apimachinery/pkg/util/wait\" <nl> appsinformersv1 \"k8s.io/client-go/informers/apps/v1\" <nl> @@ -20,8 +21,6 @@ import ( <nl> \"k8s.io/client-go/tools/cache\" <nl> \"k8s.io/client-go/tools/record\" <nl> \"k8s.io/client-go/util/workqueue\" <nl> - <nl> - \"github.com/openshift/cluster-api/pkg/client/clientset_generated/clientset/scheme\" <nl> ) <nl> const ( <nl> @@ -77,13 +76,16 @@ func New( <nl> operandVersions = append(operandVersions, osconfigv1.OperandVersion{Name: \"operator\", Version: releaseVersion}) <nl> } <nl> + eventRecorderScheme := runtime.NewScheme() <nl> + osconfigv1.Install(eventRecorderScheme) <nl> + <nl> optr := &Operator{ <nl> namespace: namespace, <nl> name: name, <nl> imagesFile: imagesFile, <nl> kubeClient: kubeClient, <nl> osClient: osClient, <nl> - eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"machineapioperator\"}), <nl> + eventRecorder: eventBroadcaster.NewRecorder(eventRecorderScheme, v1.EventSource{Component: \"machineapioperator\"}), <nl> queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"machineapioperator\"), <nl> operandVersions: operandVersions, <nl> } <nl> ", "msg": "Register ClusterOperator to properly generate event for the kind"}
{"diff_id": 4890, "repo": "openshift/machine-api-operator", "sha": "e83e21ac8472e7a2d1c42373ae3adb3f4fac5357", "time": "15.05.2019 12:21:32", "diff": "mmm a / cmd/machine-healthcheck/main.go <nl> ppp b / cmd/machine-healthcheck/main.go <nl>@@ -4,6 +4,8 @@ import ( <nl> \"flag\" <nl> \"runtime\" <nl> + \"k8s.io/klog\" <nl> + <nl> \"github.com/golang/glog\" <nl> mapiv1 \"github.com/openshift/cluster-api/pkg/apis/machine/v1beta1\" <nl> \"github.com/openshift/machine-api-operator/pkg/apis/healthchecking/v1alpha1\" <nl> @@ -22,6 +24,7 @@ func printVersion() { <nl> } <nl> func main() { <nl> + watchNamespace := flag.String(\"namespace\", \"\", \"Namespace that the controller watches to reconcile machine-api objects. If unspecified, the controller watches for machine-api objects across all namespaces.\") <nl> flag.Parse() <nl> printVersion() <nl> @@ -31,8 +34,13 @@ func main() { <nl> glog.Fatal(err) <nl> } <nl> + opts := manager.Options{} <nl> + if *watchNamespace != \"\" { <nl> + opts.Namespace = *watchNamespace <nl> + klog.Infof(\"Watching machine-api objects only in namespace %q for reconciliation.\", opts.Namespace) <nl> + } <nl> // Create a new Cmd to provide shared dependencies and start components <nl> - mgr, err := manager.New(cfg, manager.Options{}) <nl> + mgr, err := manager.New(cfg, opts) <nl> if err != nil { <nl> glog.Fatal(err) <nl> } <nl> ", "msg": "Add namespace flag for machine healthcheck"}
{"diff_id": 4892, "repo": "openshift/machine-api-operator", "sha": "ab7a0c62e4ec32f4914323afb35207967aa5e9de", "time": "24.03.2019 16:32:05", "diff": "mmm a / None <nl> ppp b / pkg/util/machines/machines.go <nl>+package machines <nl> + <nl> +import ( <nl> + \"context\" <nl> + \"fmt\" <nl> + <nl> + \"github.com/golang/glog\" <nl> + mapiv1 \"github.com/openshift/cluster-api/pkg/apis/machine/v1beta1\" <nl> + healthcheckingv1alpha1 \"github.com/openshift/machine-api-operator/pkg/apis/healthchecking/v1alpha1\" <nl> + \"github.com/openshift/machine-api-operator/pkg/util/conditions\" <nl> + <nl> + v1 \"k8s.io/api/core/v1\" <nl> + metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> + \"k8s.io/apimachinery/pkg/labels\" <nl> + <nl> + \"sigs.k8s.io/controller-runtime/pkg/client\" <nl> +) <nl> + <nl> +// IsMachineHealthy returns true if the the machine is running and machine node is healthy <nl> +func IsMachineHealthy(c client.Client, machine *mapiv1.Machine) bool { <nl> + if machine.Status.NodeRef == nil { <nl> + glog.Infof(\"machine %s does not have NodeRef\", machine.Name) <nl> + return false <nl> + } <nl> + <nl> + node := &v1.Node{} <nl> + key := client.ObjectKey{Namespace: metav1.NamespaceNone, Name: machine.Status.NodeRef.Name} <nl> + err := c.Get(context.TODO(), key, node) <nl> + if err != nil { <nl> + glog.Errorf(\"failed to fetch node for machine %s\", machine.Name) <nl> + return false <nl> + } <nl> + <nl> + readyCond := conditions.GetNodeCondition(node, v1.NodeReady) <nl> + if readyCond == nil { <nl> + glog.Infof(\"node %s does have 'Ready' condition\", machine.Name) <nl> + return false <nl> + } <nl> + <nl> + if readyCond.Status != v1.ConditionTrue { <nl> + glog.Infof(\"node %s does have has 'Ready' condition with the status %s\", machine.Name, readyCond.Status) <nl> + return false <nl> + } <nl> + return true <nl> +} <nl> + <nl> +// GetMachineMachineDisruptionBudgets returns list of machine disruption budgets that suit for the machine <nl> +func GetMachineMachineDisruptionBudgets(c client.Client, machine *mapiv1.Machine) ([]*healthcheckingv1alpha1.MachineDisruptionBudget, error) { <nl> + if len(machine.Labels) == 0 { <nl> + return nil, fmt.Errorf(\"no MachineDisruptionBudgets found for machine %v because it has no labels\", machine.Name) <nl> + } <nl> + <nl> + list := &healthcheckingv1alpha1.MachineDisruptionBudgetList{} <nl> + listOptions := &client.ListOptions{Namespace: machine.Namespace} <nl> + err := c.List(context.TODO(), listOptions, list) <nl> + if err != nil { <nl> + return nil, err <nl> + } <nl> + <nl> + var mdbs []*healthcheckingv1alpha1.MachineDisruptionBudget <nl> + for i := range list.Items { <nl> + mdb := &list.Items[i] <nl> + selector, err := metav1.LabelSelectorAsSelector(mdb.Spec.Selector) <nl> + if err != nil { <nl> + glog.Warningf(\"invalid selector: %v\", err) <nl> + continue <nl> + } <nl> + <nl> + // If a mdb with a nil or empty selector creeps in, it should match nothing, not everything. <nl> + if selector.Empty() || !selector.Matches(labels.Set(machine.Labels)) { <nl> + continue <nl> + } <nl> + mdbs = append(mdbs, mdb) <nl> + } <nl> + <nl> + return mdbs, nil <nl> +} <nl> ", "msg": "Create machine utils file\nCurrenly contains only single method to decide if the machine\nobject has healthy state."}
{"diff_id": 4899, "repo": "openshift/machine-api-operator", "sha": "b2bca4edfc870eb9e307151a3d08a6bdacc2aa01", "time": "11.07.2019 17:13:47", "diff": "mmm a / pkg/controller/disruption/disruption.go <nl> ppp b / pkg/controller/disruption/disruption.go <nl>@@ -3,7 +3,6 @@ package disruption <nl> import ( <nl> \"context\" <nl> \"fmt\" <nl> - \"reflect\" <nl> \"time\" <nl> \"github.com/golang/glog\" <nl> @@ -11,6 +10,7 @@ import ( <nl> healthcheckingv1alpha1 \"github.com/openshift/machine-api-operator/pkg/apis/healthchecking/v1alpha1\" <nl> \"github.com/openshift/machine-api-operator/pkg/util\" <nl> machineutil \"github.com/openshift/machine-api-operator/pkg/util/machines\" <nl> + apiequality \"k8s.io/apimachinery/pkg/api/equality\" <nl> \"k8s.io/api/core/v1\" <nl> \"k8s.io/apimachinery/pkg/api/errors\" <nl> @@ -324,7 +324,7 @@ func (r *ReconcileMachineDisruption) updateMachineDisruptionBudgetStatus( <nl> mdb.Status.DesiredHealthy == desiredHealthy && <nl> mdb.Status.ExpectedMachines == expectedCount && <nl> mdb.Status.MachineDisruptionsAllowed == disruptionsAllowed && <nl> - reflect.DeepEqual(mdb.Status.DisruptedMachines, disruptedMachines) && <nl> + apiequality.Semantic.DeepEqual(mdb.Status.DisruptedMachines, disruptedMachines) && <nl> mdb.Status.ObservedGeneration == mdb.Generation { <nl> return nil <nl> } <nl> ", "msg": "Use semantic DeepEqual for map of disrupted machines comparison\nStandard reflect.DeepEqual sees nil map and empty map as not equal.\nPDB uses apiequality.Semantic.DeepEqual which properly accomodates\nthe case."}
{"diff_id": 4905, "repo": "openshift/machine-api-operator", "sha": "2bf8d07cb73d0fabc014addbefdf1b0ea386735b", "time": "14.08.2019 16:28:54", "diff": "mmm a / pkg/operator/baremetal_pod.go <nl> ppp b / pkg/operator/baremetal_pod.go <nl>@@ -232,7 +232,6 @@ func newMetal3Containers(config *OperatorConfig) []corev1.Container { <nl> containers = append(containers, createContainerMetal3Httpd(config)) <nl> containers = append(containers, createContainerMetal3IronicConductor(config)) <nl> containers = append(containers, createContainerMetal3IronicApi(config)) <nl> - containers = append(containers, createContainerMetal3IronicExporter(config)) <nl> containers = append(containers, createContainerMetal3IronicInspector(config)) <nl> containers = append(containers, createContainerMetal3StaticIpManager(config)) <nl> return containers <nl> @@ -335,26 +334,6 @@ func createContainerMetal3IronicApi(config *OperatorConfig) corev1.Container { <nl> return container <nl> } <nl> -func createContainerMetal3IronicExporter(config *OperatorConfig) corev1.Container { <nl> - <nl> - container := corev1.Container{ <nl> - Name: \"metal3-ironic-exporter\", <nl> - Image: config.BaremetalControllers.Ironic, <nl> - ImagePullPolicy: \"Always\", <nl> - SecurityContext: &corev1.SecurityContext{ <nl> - Privileged: pointer.BoolPtr(true), <nl> - }, <nl> - Command: []string{\"/bin/runironic-exporter\"}, <nl> - VolumeMounts: volumeMounts, <nl> - Env: []corev1.EnvVar{ <nl> - setMariadbPassword(), <nl> - setEnvVar(\"HTTP_PORT\", \"http_port\"), <nl> - setEnvVar(\"PROVISIONING_INTERFACE\", \"provisioning_interface\"), <nl> - }, <nl> - } <nl> - return container <nl> -} <nl> - <nl> func createContainerMetal3IronicInspector(config *OperatorConfig) corev1.Container { <nl> container := corev1.Container{ <nl> ", "msg": "baremetal: Drop ironic-exporter.\nThis component is not being used in this version of OpenShift, so drop\nit."}
{"diff_id": 4913, "repo": "openshift/machine-api-operator", "sha": "c7aa4ab8233f7c91e5fca34f0d73ca5fa747ae2b", "time": "13.12.2019 12:53:09", "diff": "mmm a / None <nl> ppp b / cmd/vsphere/main.go <nl>+package main <nl> + <nl> +import ( <nl> + \"flag\" <nl> + \"fmt\" <nl> + \"os\" <nl> + <nl> + \"github.com/openshift/machine-api-operator/pkg/apis/machine/v1beta1\" <nl> + vsphereapis \"github.com/openshift/machine-api-operator/pkg/apis/vsphereprovider\" <nl> + capimachine \"github.com/openshift/machine-api-operator/pkg/controller/machine\" <nl> + machine \"github.com/openshift/machine-api-operator/pkg/controller/vsphere\" <nl> + \"github.com/openshift/machine-api-operator/pkg/version\" <nl> + \"k8s.io/klog\" <nl> + \"sigs.k8s.io/controller-runtime/pkg/client/config\" <nl> + \"sigs.k8s.io/controller-runtime/pkg/manager\" <nl> + \"sigs.k8s.io/controller-runtime/pkg/runtime/signals\" <nl> +) <nl> + <nl> +func main() { <nl> + var printVersion bool <nl> + flag.BoolVar(&printVersion, \"version\", false, \"print version and exit\") <nl> + <nl> + klog.InitFlags(nil) <nl> + watchNamespace := flag.String(\"namespace\", \"\", \"Namespace that the controller watches to reconcile machine-api objects. If unspecified, the controller watches for machine-api objects across all namespaces.\") <nl> + flag.Set(\"logtostderr\", \"true\") <nl> + flag.Parse() <nl> + <nl> + if printVersion { <nl> + fmt.Println(version.String) <nl> + os.Exit(0) <nl> + } <nl> + <nl> + cfg := config.GetConfigOrDie() <nl> + <nl> + opts := manager.Options{ <nl> + // Disable metrics serving <nl> + MetricsBindAddress: \"0\", <nl> + } <nl> + if *watchNamespace != \"\" { <nl> + opts.Namespace = *watchNamespace <nl> + klog.Infof(\"Watching machine-api objects only in namespace %q for reconciliation.\", opts.Namespace) <nl> + } <nl> + <nl> + // Setup a Manager <nl> + mgr, err := manager.New(cfg, opts) <nl> + if err != nil { <nl> + klog.Fatalf(\"Failed to set up overall controller manager: %v\", err) <nl> + } <nl> + <nl> + // Initialize machine actuator. <nl> + machineActuator := machine.NewActuator(machine.ActuatorParams{ <nl> + Client: mgr.GetClient(), <nl> + EventRecorder: mgr.GetEventRecorderFor(\"vspherecontroller\"), <nl> + }) <nl> + <nl> + if err := vsphereapis.AddToScheme(mgr.GetScheme()); err != nil { <nl> + klog.Fatal(err) <nl> + } <nl> + <nl> + if err := v1beta1.AddToScheme(mgr.GetScheme()); err != nil { <nl> + klog.Fatal(err) <nl> + } <nl> + <nl> + capimachine.AddWithActuator(mgr, machineActuator) <nl> + <nl> + if err := mgr.Start(signals.SetupSignalHandler()); err != nil { <nl> + klog.Fatalf(\"Failed to run manager: %v\", err) <nl> + } <nl> +} <nl> ", "msg": "Add ability to build vsphere binary"}
{"diff_id": 4933, "repo": "openshift/machine-api-operator", "sha": "cc8116ddda3e4ed89dac0c7f51f623a21bee916d", "time": "24.03.2020 17:49:16", "diff": "mmm a / cmd/machine-api-operator/start.go <nl> ppp b / cmd/machine-api-operator/start.go <nl>@@ -13,9 +13,9 @@ import ( <nl> \"github.com/openshift/machine-api-operator/pkg/metrics\" <nl> \"github.com/openshift/machine-api-operator/pkg/operator\" <nl> \"github.com/openshift/machine-api-operator/pkg/version\" <nl> - \"github.com/spf13/cobra\" <nl> - <nl> \"github.com/prometheus/client_golang/prometheus\" <nl> + \"github.com/prometheus/client_golang/prometheus/promhttp\" <nl> + \"github.com/spf13/cobra\" <nl> v1 \"k8s.io/api/core/v1\" <nl> \"k8s.io/apimachinery/pkg/runtime\" <nl> \"k8s.io/client-go/kubernetes\" <nl> @@ -151,8 +151,7 @@ func startMetricsCollectionAndServer(ctx *ControllerContext) { <nl> func startHTTPMetricServer(metricsPort string) { <nl> mux := http.NewServeMux() <nl> - //TODO(vikasc): Use promhttp package for handler. This is Deprecated <nl> - mux.Handle(\"/metrics\", prometheus.Handler()) <nl> + mux.Handle(\"/metrics\", promhttp.Handler()) <nl> server := &http.Server{ <nl> Addr: metricsPort, <nl> ", "msg": "Update to use promhttp.Handler() as prometheus.Handler() was dropped support after adding kube 1.18 deps"}
{"diff_id": 4940, "repo": "openshift/machine-api-operator", "sha": "a973d4efb72a0028eb3782237b4321d569e62e20", "time": "23.04.2020 13:42:47", "diff": "mmm a / pkg/operator/baremetal_config.go <nl> ppp b / pkg/operator/baremetal_config.go <nl>@@ -132,8 +132,12 @@ func getIronicInspectorEndpoint(baremetalConfig BaremetalProvisioningConfig) *st <nl> } <nl> func getProvisioningDHCPRange(baremetalConfig BaremetalProvisioningConfig) *string { <nl> + // When the DHCP server is external, it is OK for the DHCP range in the CR <nl> + // to be empty. <nl> if baremetalConfig.ProvisioningDHCPRange != \"\" { <nl> return &(baremetalConfig.ProvisioningDHCPRange) <nl> + } else if baremetalConfig.ProvisioningDHCPExternal { <nl> + return &(baremetalConfig.ProvisioningDHCPRange) <nl> } <nl> return nil <nl> } <nl> ", "msg": "Fix External DHCP range values for Baremetal configuration\nAllow an empty string to be passed in as DHCP range when the DHCP\nserver is external in a Baremetal IPI installation."}
{"diff_id": 4946, "repo": "openshift/machine-api-operator", "sha": "c4b2e88e51707b0839c53c437c42cd17f727e753", "time": "27.05.2020 13:07:00", "diff": "mmm a / pkg/controller/vsphere/machine_scope_test.go <nl> ppp b / pkg/controller/vsphere/machine_scope_test.go <nl>@@ -4,18 +4,23 @@ import ( <nl> \"bytes\" <nl> \"context\" <nl> \"fmt\" <nl> + \"net\" <nl> + \"path/filepath\" <nl> \"testing\" <nl> + \"time\" <nl> + . \"github.com/onsi/gomega\" <nl> configv1 \"github.com/openshift/api/config/v1\" <nl> machinev1 \"github.com/openshift/machine-api-operator/pkg/apis/machine/v1beta1\" <nl> vspherev1 \"github.com/openshift/machine-api-operator/pkg/apis/vsphereprovider/v1beta1\" <nl> corev1 \"k8s.io/api/core/v1\" <nl> - \"k8s.io/apimachinery/pkg/api/equality\" <nl> metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" <nl> \"k8s.io/apimachinery/pkg/runtime\" <nl> + \"k8s.io/apimachinery/pkg/types\" <nl> \"k8s.io/client-go/kubernetes/scheme\" <nl> - runtimeclient \"sigs.k8s.io/controller-runtime/pkg/client\" <nl> + \"sigs.k8s.io/controller-runtime/pkg/client\" <nl> \"sigs.k8s.io/controller-runtime/pkg/client/fake\" <nl> + \"sigs.k8s.io/controller-runtime/pkg/envtest\" <nl> ) <nl> const TestNamespace = \"vsphere-test\" <nl> @@ -277,19 +282,68 @@ func TestGetCredentialsSecret(t *testing.T) { <nl> } <nl> func TestPatchMachine(t *testing.T) { <nl> + g := NewWithT(t) <nl> + ctx := context.Background() <nl> + <nl> + configv1.AddToScheme(scheme.Scheme) <nl> + machinev1.AddToScheme(scheme.Scheme) <nl> + <nl> + testEnv := &envtest.Environment{ <nl> + CRDDirectoryPaths: []string{ <nl> + filepath.Join(\"..\", \"..\", \"..\", \"install\"), <nl> + filepath.Join(\"..\", \"..\", \"..\", \"vendor\", \"github.com\", \"openshift\", \"api\", \"config\", \"v1\")}, <nl> + } <nl> + <nl> + cfg, err := testEnv.Start() <nl> + g.Expect(err).ToNot(HaveOccurred()) <nl> + g.Expect(cfg).ToNot(BeNil()) <nl> + defer func() { <nl> + g.Expect(testEnv.Stop()).To(Succeed()) <nl> + }() <nl> + <nl> + k8sClient, err := client.New(cfg, client.Options{}) <nl> + g.Expect(err).ToNot(HaveOccurred()) <nl> + <nl> model, _, server := initSimulator(t) <nl> defer model.Remove() <nl> defer server.Close() <nl> - credentialsSecretUsername := fmt.Sprintf(\"%s.username\", server.URL.Host) <nl> - credentialsSecretPassword := fmt.Sprintf(\"%s.password\", server.URL.Host) <nl> + host, port, err := net.SplitHostPort(server.URL.Host) <nl> + if err != nil { <nl> + t.Fatal(err) <nl> + } <nl> + <nl> + credentialsSecretUsername := fmt.Sprintf(\"%s.username\", host) <nl> + credentialsSecretPassword := fmt.Sprintf(\"%s.password\", host) <nl> // fake objects for newMachineScope() <nl> password, _ := server.URL.User.Password() <nl> - namespace := \"test\" <nl> + <nl> + configNamespace := &corev1.Namespace{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: openshiftConfigNamespace, <nl> + }, <nl> + } <nl> + g.Expect(k8sClient.Create(ctx, configNamespace)).To(Succeed()) <nl> + defer func() { <nl> + g.Expect(k8sClient.Delete(ctx, configNamespace)).To(Succeed()) <nl> + }() <nl> + <nl> + testNamespaceName := \"test\" <nl> + <nl> + testNamespace := &corev1.Namespace{ <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: testNamespaceName, <nl> + }, <nl> + } <nl> + g.Expect(k8sClient.Create(ctx, testNamespace)).To(Succeed()) <nl> + defer func() { <nl> + g.Expect(k8sClient.Delete(ctx, testNamespace)).To(Succeed()) <nl> + }() <nl> + <nl> credentialsSecret := &corev1.Secret{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> Name: \"test\", <nl> - Namespace: namespace, <nl> + Namespace: testNamespaceName, <nl> }, <nl> Data: map[string][]byte{ <nl> credentialsSecretUsername: []byte(server.URL.User.Username()), <nl> @@ -297,16 +351,25 @@ func TestPatchMachine(t *testing.T) { <nl> }, <nl> } <nl> - testConfig := fmt.Sprintf(testConfigFmt, \"\") <nl> + g.Expect(k8sClient.Create(ctx, credentialsSecret)).To(Succeed()) <nl> + defer func() { <nl> + g.Expect(k8sClient.Delete(ctx, credentialsSecret)).To(Succeed()) <nl> + }() <nl> + <nl> + testConfig := fmt.Sprintf(testConfigFmt, port) <nl> configMap := &corev1.ConfigMap{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> - Name: \"testName\", <nl> + Name: \"testname\", <nl> Namespace: openshiftConfigNamespace, <nl> }, <nl> Data: map[string]string{ <nl> - \"testKey\": testConfig, <nl> + \"testkey\": testConfig, <nl> }, <nl> } <nl> + g.Expect(k8sClient.Create(ctx, configMap)).To(Succeed()) <nl> + defer func() { <nl> + g.Expect(k8sClient.Delete(ctx, configMap)).To(Succeed()) <nl> + }() <nl> infra := &configv1.Infrastructure{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> @@ -314,12 +377,84 @@ func TestPatchMachine(t *testing.T) { <nl> }, <nl> Spec: configv1.InfrastructureSpec{ <nl> CloudConfig: configv1.ConfigMapFileReference{ <nl> - Name: \"testName\", <nl> - Key: \"testKey\", <nl> + Name: \"testname\", <nl> + Key: \"testkey\", <nl> + }, <nl> + }, <nl> + } <nl> + g.Expect(k8sClient.Create(ctx, infra)).To(Succeed()) <nl> + defer func() { <nl> + g.Expect(k8sClient.Delete(ctx, infra)).To(Succeed()) <nl> + }() <nl> + <nl> + failedPhase := \"Failed\" <nl> + <nl> + providerStatus := &vspherev1.VSphereMachineProviderStatus{} <nl> + <nl> + machineName := \"test\" <nl> + machineKey := types.NamespacedName{Namespace: testNamespaceName, Name: machineName} <nl> + <nl> + testCases := []struct { <nl> + name string <nl> + mutate func(*machinev1.Machine) <nl> + expect func(*machinev1.Machine) error <nl> + }{ <nl> + { <nl> + name: \"Test changing labels\", <nl> + mutate: func(m *machinev1.Machine) { <nl> + m.Labels[\"testlabel\"] = \"test\" <nl> + }, <nl> + expect: func(m *machinev1.Machine) error { <nl> + if m.Labels[\"testlabel\"] != \"test\" { <nl> + return fmt.Errorf(\"label \\\"testlabel\\\" %q not equal expected \\\"test\\\"\", m.ObjectMeta.Labels[\"test\"]) <nl> + } <nl> + return nil <nl> + }, <nl> + }, <nl> + { <nl> + name: \"Test setting phase\", <nl> + mutate: func(m *machinev1.Machine) { <nl> + m.Status.Phase = &failedPhase <nl> + }, <nl> + expect: func(m *machinev1.Machine) error { <nl> + if m.Status.Phase != nil && *m.Status.Phase == failedPhase { <nl> + return nil <nl> + } <nl> + return fmt.Errorf(\"phase is nil or not equal expected \\\"Failed\\\"\") <nl> + }, <nl> + }, <nl> + { <nl> + name: \"Test setting provider status\", <nl> + mutate: func(m *machinev1.Machine) { <nl> + instanceID := \"123\" <nl> + instanceState := \"running\" <nl> + providerStatus.InstanceID = &instanceID <nl> + providerStatus.InstanceState = &instanceState <nl> + }, <nl> + expect: func(m *machinev1.Machine) error { <nl> + providerStatus, err := vspherev1.ProviderStatusFromRawExtension(m.Status.ProviderStatus) <nl> + if err != nil { <nl> + return fmt.Errorf(\"unable to get provider status: %v\", err) <nl> + } <nl> + <nl> + if providerStatus.InstanceID == nil || *providerStatus.InstanceID != \"123\" { <nl> + return fmt.Errorf(\"instanceID is nil or not equal expected \\\"123\\\"\") <nl> + } <nl> + <nl> + if providerStatus.InstanceState == nil || *providerStatus.InstanceState != \"running\" { <nl> + return fmt.Errorf(\"instanceState is nil or not equal expected \\\"running\\\"\") <nl> + } <nl> + <nl> + return nil <nl> }, <nl> }, <nl> } <nl> + for _, tc := range testCases { <nl> + t.Run(tc.name, func(t *testing.T) { <nl> + gs := NewWithT(t) <nl> + timeout := 10 * time.Second <nl> + <nl> // original objects <nl> originalProviderSpec := vspherev1.VSphereMachineProviderSpec{ <nl> CredentialsSecret: &corev1.LocalObjectReference{ <nl> @@ -327,27 +462,23 @@ func TestPatchMachine(t *testing.T) { <nl> }, <nl> Workspace: &vspherev1.Workspace{ <nl> - Server: server.URL.Host, <nl> + Server: host, <nl> Folder: \"test\", <nl> }, <nl> } <nl> rawProviderSpec, err := vspherev1.RawExtensionFromProviderSpec(&originalProviderSpec) <nl> - if err != nil { <nl> - t.Fatal(err) <nl> - } <nl> - <nl> + gs.Expect(err).ToNot(HaveOccurred()) <nl> originalProviderStatus := &vspherev1.VSphereMachineProviderStatus{ <nl> TaskRef: \"test\", <nl> } <nl> rawProviderStatus, err := vspherev1.RawExtensionFromProviderStatus(originalProviderStatus) <nl> - if err != nil { <nl> - t.Fatal(err) <nl> - } <nl> + gs.Expect(err).ToNot(HaveOccurred()) <nl> - originalMachine := &machinev1.Machine{ <nl> + machine := &machinev1.Machine{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> - Name: \"test\", <nl> - Namespace: namespace, <nl> + Name: machineName, <nl> + Namespace: testNamespaceName, <nl> + Labels: map[string]string{}, <nl> }, <nl> TypeMeta: metav1.TypeMeta{ <nl> Kind: \"Machine\", <nl> @@ -363,59 +494,48 @@ func TestPatchMachine(t *testing.T) { <nl> }, <nl> } <nl> - // expected objects <nl> - expectedMachine := originalMachine.DeepCopy() <nl> - providerID := \"mutated\" <nl> - expectedMachine.Spec.ProviderID = &providerID <nl> - expectedMachine.Status.Addresses = []corev1.NodeAddress{ <nl> - { <nl> - Type: corev1.NodeInternalDNS, <nl> - Address: \"127.0.0.1\", <nl> - }, <nl> - } <nl> - expectedProviderStatus := &vspherev1.VSphereMachineProviderStatus{ <nl> - TaskRef: \"mutated\", <nl> - } <nl> - rawProviderStatus, err = vspherev1.RawExtensionFromProviderStatus(expectedProviderStatus) <nl> - if err != nil { <nl> - t.Fatal(err) <nl> - } <nl> - expectedMachine.Status.ProviderStatus = rawProviderStatus <nl> + // Create the machine <nl> + gs.Expect(k8sClient.Create(ctx, machine)).To(Succeed()) <nl> + defer func() { <nl> + gs.Expect(k8sClient.Delete(ctx, machine)).To(Succeed()) <nl> + }() <nl> - // machineScope <nl> - if err := machinev1.AddToScheme(scheme.Scheme); err != nil { <nl> - t.Fatal(err) <nl> + // Ensure the machine has synced to the cache <nl> + getMachine := func() error { <nl> + <nl> + return k8sClient.Get(ctx, machineKey, machine) <nl> } <nl> - fakeClient := fake.NewFakeClientWithScheme(scheme.Scheme, <nl> - credentialsSecret, <nl> - originalMachine, <nl> - configMap, <nl> - infra) <nl> + gs.Eventually(getMachine, timeout).Should(Succeed()) <nl> + <nl> machineScope, err := newMachineScope(machineScopeParams{ <nl> - client: fakeClient, <nl> - Context: context.TODO(), <nl> - machine: originalMachine, <nl> - apiReader: fakeClient, <nl> + client: k8sClient, <nl> + machine: machine, <nl> + apiReader: k8sClient, <nl> + Context: ctx, <nl> }) <nl> - if err != nil { <nl> - t.Fatal(err) <nl> - } <nl> - // mutations <nl> - machineScope.machine.Spec.ProviderID = expectedMachine.Spec.ProviderID <nl> - machineScope.machine.Status.Addresses = expectedMachine.Status.Addresses <nl> - machineScope.providerStatus = expectedProviderStatus <nl> + gs.Expect(err).ToNot(HaveOccurred()) <nl> + <nl> + tc.mutate(machineScope.machine) <nl> - if err := machineScope.PatchMachine(); err != nil { <nl> - t.Errorf(\"unexpected error\") <nl> + machineScope.providerStatus = providerStatus <nl> + <nl> + // Patch the machine and check the expectation from the test case <nl> + gs.Expect(machineScope.PatchMachine()).To(Succeed()) <nl> + checkExpectation := func() error { <nl> + if err := getMachine(); err != nil { <nl> + return err <nl> } <nl> - gotMachine := &machinev1.Machine{} <nl> - if err := machineScope.client.Get(context.TODO(), runtimeclient.ObjectKey{Name: \"test\", Namespace: namespace}, gotMachine); err != nil { <nl> - t.Fatal(err) <nl> + return tc.expect(machine) <nl> } <nl> + gs.Eventually(checkExpectation, timeout).Should(Succeed()) <nl> - expectedMachine.ResourceVersion = \"2\" <nl> - if !equality.Semantic.DeepEqual(gotMachine, expectedMachine) { <nl> - t.Errorf(\"expected: %+v, got: %+v\", expectedMachine, gotMachine) <nl> + // Check that resource version doesn't change if we call patchMachine() again <nl> + machineResourceVersion := machine.ResourceVersion <nl> + <nl> + gs.Expect(machineScope.PatchMachine()).To(Succeed()) <nl> + gs.Eventually(getMachine, timeout).Should(Succeed()) <nl> + gs.Expect(machine.ResourceVersion).To(Equal(machineResourceVersion)) <nl> + }) <nl> } <nl> } <nl> ", "msg": "[vSphere] Use envtest for testing machine patch"}
{"diff_id": 4953, "repo": "openshift/machine-api-operator", "sha": "5fa69d5ca176b3911a07a40fd4db453cb32525f8", "time": "02.07.2020 13:29:19", "diff": "mmm a / cmd/machine-healthcheck/main.go <nl> ppp b / cmd/machine-healthcheck/main.go <nl>@@ -3,6 +3,7 @@ package main <nl> import ( <nl> \"flag\" <nl> \"runtime\" <nl> + \"time\" <nl> \"github.com/openshift/machine-api-operator/pkg/controller/machinehealthcheck\" <nl> \"github.com/openshift/machine-api-operator/pkg/metrics\" <nl> @@ -44,6 +45,24 @@ func main() { <nl> \"The address for health checking.\", <nl> ) <nl> + leaderElectResourceNamespace := flag.String( <nl> + \"leader-elect-resource-namespace\", <nl> + \"\", <nl> + \"The namespace of resource object that is used for locking during leader election. If unspecified and running in cluster, defaults to the service account namespace for the controller. Required for leader-election outside of a cluster.\", <nl> + ) <nl> + <nl> + leaderElect := flag.Bool( <nl> + \"leader-elect\", <nl> + false, <nl> + \"Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability.\", <nl> + ) <nl> + <nl> + leaderElectLeaseDuration := flag.Duration( <nl> + \"leader-elect-lease-duration\", <nl> + 15*time.Second, <nl> + \"The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\", <nl> + ) <nl> + <nl> flag.Parse() <nl> printVersion() <nl> @@ -56,7 +75,12 @@ func main() { <nl> opts := manager.Options{ <nl> MetricsBindAddress: *metricsAddress, <nl> HealthProbeBindAddress: *healthAddr, <nl> + LeaderElection: *leaderElect, <nl> + LeaderElectionNamespace: *leaderElectResourceNamespace, <nl> + LeaderElectionID: \"cluster-api-provider-healthcheck-leader\", <nl> + LeaseDuration: leaderElectLeaseDuration, <nl> } <nl> + <nl> if *watchNamespace != \"\" { <nl> opts.Namespace = *watchNamespace <nl> glog.Infof(\"Watching machine-api objects only in namespace %q for reconciliation.\", opts.Namespace) <nl> ", "msg": "Add Leader election flags for machine health checks"}
{"diff_id": 4956, "repo": "openshift/machine-api-operator", "sha": "3d215b3cef92ff7d1b450a70c04db80cc5ea4db2", "time": "20.07.2020 12:56:21", "diff": "mmm a / cmd/vsphere/main.go <nl> ppp b / cmd/vsphere/main.go <nl>@@ -45,7 +45,7 @@ func main() { <nl> leaderElectLeaseDuration := flag.Duration( <nl> \"leader-elect-lease-duration\", <nl> - 15*time.Second, <nl> + 90*time.Second, <nl> \"The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\", <nl> ) <nl> ", "msg": "Increate leader election lease time for vsphere\nThe machine-api-controller components are refreshing their lease more\nthan all other components combined. Bringing this to 90s each, will\ndecrease etcd writes at idle."}
{"diff_id": 4958, "repo": "openshift/machine-api-operator", "sha": "aa7bd363b719c3f1f8213a57cd0277638b15f444", "time": "20.07.2020 12:57:45", "diff": "mmm a / cmd/machineset/main.go <nl> ppp b / cmd/machineset/main.go <nl>@@ -75,7 +75,7 @@ func main() { <nl> leaderElectLeaseDuration := flag.Duration( <nl> \"leader-elect-lease-duration\", <nl> - 15*time.Second, <nl> + 90*time.Second, <nl> \"The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\", <nl> ) <nl> ", "msg": "Increate leader election lease time for machineset\nThe machine-api-controller components are refreshing their lease more\nthan all other components combined. Bringing this to 90s each, will\ndecrease etcd writes at idle."}
{"diff_id": 4963, "repo": "openshift/machine-api-operator", "sha": "765b29ba00be5db02673bbb5e22e0229a94c43ff", "time": "03.08.2020 12:52:05", "diff": "mmm a / pkg/apis/machine/v1beta1/machine_webhook.go <nl> ppp b / pkg/apis/machine/v1beta1/machine_webhook.go <nl>@@ -95,7 +95,7 @@ const ( <nl> defaultWebhookServiceName = \"machine-api-operator-webhook\" <nl> defaultWebhookServiceNamespace = \"openshift-machine-api\" <nl> - defaultUserDataSecret = \"worker-user-data\" <nl> + defaultUserDataSecret = \"worker-user-data-managed\" <nl> defaultSecretNamespace = \"openshift-machine-api\" <nl> // AWS Defaults <nl> ", "msg": "Update default user data to be worker-user-data-managed\nThis changed the name of the userdata generated by the installer to satisfy MCO ign v2 -> v3 migration and let the new secret be under mco management."}
{"diff_id": 4966, "repo": "openshift/machine-api-operator", "sha": "a3f5dcb66b83645a248e237ad838a0f002e5bc5d", "time": "11.08.2020 16:02:20", "diff": "mmm a / pkg/operator/baremetal_pod.go <nl> ppp b / pkg/operator/baremetal_pod.go <nl>@@ -337,6 +337,7 @@ func createContainerMetal3Httpd(config *OperatorConfig, baremetalProvisioningCon <nl> VolumeMounts: volumeMounts, <nl> Env: []corev1.EnvVar{ <nl> buildEnvVar(\"HTTP_PORT\", baremetalProvisioningConfig), <nl> + buildEnvVar(\"PROVISIONING_IP\", baremetalProvisioningConfig), <nl> buildEnvVar(\"PROVISIONING_INTERFACE\", baremetalProvisioningConfig), <nl> }, <nl> } <nl> @@ -357,6 +358,7 @@ func createContainerMetal3IronicConductor(config *OperatorConfig, baremetalProvi <nl> Env: []corev1.EnvVar{ <nl> setMariadbPassword(), <nl> buildEnvVar(\"HTTP_PORT\", baremetalProvisioningConfig), <nl> + buildEnvVar(\"PROVISIONING_IP\", baremetalProvisioningConfig), <nl> buildEnvVar(\"PROVISIONING_INTERFACE\", baremetalProvisioningConfig), <nl> }, <nl> } <nl> @@ -377,6 +379,7 @@ func createContainerMetal3IronicApi(config *OperatorConfig, baremetalProvisionin <nl> Env: []corev1.EnvVar{ <nl> setMariadbPassword(), <nl> buildEnvVar(\"HTTP_PORT\", baremetalProvisioningConfig), <nl> + buildEnvVar(\"PROVISIONING_IP\", baremetalProvisioningConfig), <nl> buildEnvVar(\"PROVISIONING_INTERFACE\", baremetalProvisioningConfig), <nl> }, <nl> } <nl> @@ -394,6 +397,7 @@ func createContainerMetal3IronicInspector(config *OperatorConfig, baremetalProvi <nl> }, <nl> VolumeMounts: volumeMounts, <nl> Env: []corev1.EnvVar{ <nl> + buildEnvVar(\"PROVISIONING_IP\", baremetalProvisioningConfig), <nl> buildEnvVar(\"PROVISIONING_INTERFACE\", baremetalProvisioningConfig), <nl> }, <nl> } <nl> ", "msg": "baremetal: configure containers with provisioning_ip\nWhen the provisioning network is disabled, provisioning IP is on a\nmachine network, and we automatically detect the interface to use. We\nneed to pass the IP into these containers, as well as the interface (if\nany is set)."}
{"diff_id": 4968, "repo": "openshift/machine-api-operator", "sha": "c939c9208557c35a446d4217e5a4d3ea7de9caf7", "time": "12.08.2020 16:19:37", "diff": "mmm a / cmd/machineset/main.go <nl> ppp b / cmd/machineset/main.go <nl>@@ -39,6 +39,12 @@ const ( <nl> defaultWebhookCertdir = \"/etc/machine-api-operator/tls\" <nl> ) <nl> +// The default durations for the leader electrion operations. <nl> +var ( <nl> + retryPeriod = 30 * time.Second <nl> + renewDealine = 90 * time.Second <nl> +) <nl> + <nl> func main() { <nl> flag.Set(\"logtostderr\", \"true\") <nl> klog.InitFlags(nil) <nl> @@ -102,6 +108,9 @@ func main() { <nl> LeaderElectionNamespace: *leaderElectResourceNamespace, <nl> LeaderElectionID: \"cluster-api-provider-machineset-leader\", <nl> LeaseDuration: leaderElectLeaseDuration, <nl> + // Slow the default retry and renew election rate to reduce etcd writes at idle: BZ 1858400 <nl> + RetryPeriod: &retryPeriod, <nl> + RenewDeadline: &renewDealine, <nl> } <nl> mgr, err := manager.New(cfg, opts) <nl> ", "msg": "Slow the default lease retry and renew rate for MachineSet controller\nPrevent machine controllers from writing in etcd at idle too often\nby setting 30s retry and 90s deadline on all renewals.\nBZ"}
{"diff_id": 4969, "repo": "openshift/machine-api-operator", "sha": "5fdd6efbccb119d4805c373c959330242bbd6e67", "time": "12.08.2020 16:20:07", "diff": "mmm a / cmd/nodelink-controller/main.go <nl> ppp b / cmd/nodelink-controller/main.go <nl>@@ -15,6 +15,12 @@ import ( <nl> \"sigs.k8s.io/controller-runtime/pkg/runtime/signals\" <nl> ) <nl> +// The default durations for the leader electrion operations. <nl> +var ( <nl> + retryPeriod = 30 * time.Second <nl> + renewDealine = 90 * time.Second <nl> +) <nl> + <nl> func printVersion() { <nl> klog.Infof(\"Go Version: %s\", runtime.Version()) <nl> klog.Infof(\"Go OS/Arch: %s/%s\", runtime.GOOS, runtime.GOARCH) <nl> @@ -65,6 +71,9 @@ func main() { <nl> LeaderElectionNamespace: *leaderElectResourceNamespace, <nl> LeaderElectionID: \"cluster-api-provider-nodelink-leader\", <nl> LeaseDuration: leaderElectLeaseDuration, <nl> + // Slow the default retry and renew election rate to reduce etcd writes at idle: BZ 1858400 <nl> + RetryPeriod: &retryPeriod, <nl> + RenewDeadline: &renewDealine, <nl> } <nl> if *watchNamespace != \"\" { <nl> opts.Namespace = *watchNamespace <nl> ", "msg": "Slow the default lease retry and renew rate for nodelink controller\nPrevent machine controllers from writing in etcd at idle too often\nby setting 60s retry and delay on all renewals.\nBZ"}
{"diff_id": 4972, "repo": "openshift/machine-api-operator", "sha": "fa06cd54c45b1c1cc246c585436056a39442c13b", "time": "27.07.2020 17:03:28", "diff": "mmm a / pkg/operator/baremetal_pod.go <nl> ppp b / pkg/operator/baremetal_pod.go <nl>@@ -450,6 +450,11 @@ func createContainerMetal3IronicConductor(config *OperatorConfig, baremetalProvi <nl> Value: ironicUsername, <nl> }, <nl> setIronicPassword(\"IRONIC_HTTP_BASIC_PASSWORD\"), <nl> + { <nl> + Name: \"INSPECTOR_HTTP_BASIC_USERNAME\", <nl> + Value: ironicUsername, <nl> + }, <nl> + setIronicPassword(\"INSPECTOR_HTTP_BASIC_PASSWORD\"), <nl> }, <nl> } <nl> return container <nl> ", "msg": "baremetal: Pass Inspector credentials to Ironic pod\nThe ironic image doesn't yet support this, but we can pass them."}
{"diff_id": 4982, "repo": "openshift/machine-api-operator", "sha": "2f6dca2b8ba5980ab2e699af34f406f967e8c9e2", "time": "04.08.2020 15:12:06", "diff": "mmm a / pkg/operator/baremetal_pod.go <nl> ppp b / pkg/operator/baremetal_pod.go <nl>@@ -97,12 +97,11 @@ func buildEnvVar(name string, baremetalProvisioningConfig BaremetalProvisioningC <nl> Name: name, <nl> Value: *value, <nl> } <nl> - } else { <nl> + } <nl> return corev1.EnvVar{ <nl> Name: name, <nl> } <nl> } <nl> -} <nl> func setMariadbPassword() corev1.EnvVar { <nl> return corev1.EnvVar{ <nl> @@ -395,10 +394,8 @@ func createInitContainerStaticIpSet(config *OperatorConfig, baremetalProvisionin <nl> return initContainer <nl> } <nl> -func newMetal3Containers(config *OperatorConfig, baremetalProvisioningConfig BaremetalProvisioningConfig) []corev1.Container { <nl> - //Starting off with the metal3-baremetal-operator container <nl> - containers := []corev1.Container{ <nl> - { <nl> +func createContainerMetal3BareMetalOperator(config *OperatorConfig, baremetalProvisioningConfig BaremetalProvisioningConfig) corev1.Container { <nl> + return corev1.Container{ <nl> Name: \"metal3-baremetal-operator\", <nl> Image: config.BaremetalControllers.BaremetalOperator, <nl> Ports: []corev1.ContainerPort{ <nl> @@ -443,17 +440,23 @@ func newMetal3Containers(config *OperatorConfig, baremetalProvisioningConfig Bar <nl> Value: metal3AuthRootDir, <nl> }, <nl> }, <nl> - }, <nl> + } <nl> +} <nl> + <nl> +func newMetal3Containers(config *OperatorConfig, baremetalProvisioningConfig BaremetalProvisioningConfig) []corev1.Container { <nl> + containers := []corev1.Container{ <nl> + createContainerMetal3BareMetalOperator(config, baremetalProvisioningConfig), <nl> + createContainerMetal3Mariadb(config), <nl> + createContainerMetal3Httpd(config, baremetalProvisioningConfig), <nl> + createContainerMetal3IronicConductor(config, baremetalProvisioningConfig), <nl> + createContainerMetal3IronicApi(config, baremetalProvisioningConfig), <nl> + createContainerMetal3IronicInspector(config, baremetalProvisioningConfig), <nl> + createContainerMetal3StaticIpManager(config, baremetalProvisioningConfig), <nl> } <nl> if baremetalProvisioningConfig.ProvisioningNetwork != provisioningNetworkDisabled { <nl> containers = append(containers, createContainerMetal3Dnsmasq(config, baremetalProvisioningConfig)) <nl> } <nl> - containers = append(containers, createContainerMetal3Mariadb(config)) <nl> - containers = append(containers, createContainerMetal3Httpd(config, baremetalProvisioningConfig)) <nl> - containers = append(containers, createContainerMetal3IronicConductor(config, baremetalProvisioningConfig)) <nl> - containers = append(containers, createContainerMetal3IronicApi(config, baremetalProvisioningConfig)) <nl> - containers = append(containers, createContainerMetal3IronicInspector(config, baremetalProvisioningConfig)) <nl> - containers = append(containers, createContainerMetal3StaticIpManager(config, baremetalProvisioningConfig)) <nl> + <nl> return containers <nl> } <nl> ", "msg": "make metal3 container list construction more concise"}
{"diff_id": 4984, "repo": "openshift/machine-api-operator", "sha": "83647910b3352a27d50e6a56f74cc0c8c855a877", "time": "04.08.2020 16:14:36", "diff": "mmm a / pkg/operator/sync.go <nl> ppp b / pkg/operator/sync.go <nl>@@ -449,6 +449,45 @@ func newDeployment(config *OperatorConfig, features map[string]bool) *appsv1.Dep <nl> } <nl> } <nl> +// List of the volumes needed by newKubeProxyContainer <nl> +func newRBACConfigVolumes() []corev1.Volume { <nl> + var readOnly int32 = 420 <nl> + return []corev1.Volume{ <nl> + { <nl> + Name: kubeRBACConfigName, <nl> + VolumeSource: corev1.VolumeSource{ <nl> + ConfigMap: &corev1.ConfigMapVolumeSource{ <nl> + LocalObjectReference: corev1.LocalObjectReference{ <nl> + Name: \"kube-rbac-proxy\", <nl> + }, <nl> + DefaultMode: pointer.Int32Ptr(readOnly), <nl> + }, <nl> + }, <nl> + }, <nl> + { <nl> + Name: certStoreName, <nl> + VolumeSource: corev1.VolumeSource{ <nl> + Secret: &corev1.SecretVolumeSource{ <nl> + SecretName: \"machine-api-controllers-tls\", <nl> + DefaultMode: pointer.Int32Ptr(readOnly), <nl> + }, <nl> + }, <nl> + }, <nl> + { <nl> + Name: \"trusted-ca\", <nl> + VolumeSource: corev1.VolumeSource{ <nl> + ConfigMap: &corev1.ConfigMapVolumeSource{ <nl> + Items: []corev1.KeyToPath{{Key: \"ca-bundle.crt\", Path: \"tls-ca-bundle.pem\"}}, <nl> + LocalObjectReference: corev1.LocalObjectReference{ <nl> + Name: externalTrustBundleConfigMapName, <nl> + }, <nl> + Optional: pointer.BoolPtr(true), <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> +} <nl> + <nl> func newPodTemplateSpec(config *OperatorConfig, features map[string]bool) *corev1.PodTemplateSpec { <nl> containers := newContainers(config, features) <nl> proxyContainers := newKubeProxyContainers(config.Controllers.KubeRBACProxy) <nl> @@ -477,26 +516,6 @@ func newPodTemplateSpec(config *OperatorConfig, features map[string]bool) *corev <nl> var readOnly int32 = 420 <nl> volumes := []corev1.Volume{ <nl> - { <nl> - Name: kubeRBACConfigName, <nl> - VolumeSource: corev1.VolumeSource{ <nl> - ConfigMap: &corev1.ConfigMapVolumeSource{ <nl> - LocalObjectReference: corev1.LocalObjectReference{ <nl> - Name: \"kube-rbac-proxy\", <nl> - }, <nl> - DefaultMode: pointer.Int32Ptr(readOnly), <nl> - }, <nl> - }, <nl> - }, <nl> - { <nl> - Name: certStoreName, <nl> - VolumeSource: corev1.VolumeSource{ <nl> - Secret: &corev1.SecretVolumeSource{ <nl> - SecretName: \"machine-api-controllers-tls\", <nl> - DefaultMode: pointer.Int32Ptr(readOnly), <nl> - }, <nl> - }, <nl> - }, <nl> { <nl> Name: \"cert\", <nl> VolumeSource: corev1.VolumeSource{ <nl> @@ -516,19 +535,8 @@ func newPodTemplateSpec(config *OperatorConfig, features map[string]bool) *corev <nl> }, <nl> }, <nl> }, <nl> - { <nl> - Name: \"trusted-ca\", <nl> - VolumeSource: corev1.VolumeSource{ <nl> - ConfigMap: &corev1.ConfigMapVolumeSource{ <nl> - Items: []corev1.KeyToPath{{Key: \"ca-bundle.crt\", Path: \"tls-ca-bundle.pem\"}}, <nl> - LocalObjectReference: corev1.LocalObjectReference{ <nl> - Name: externalTrustBundleConfigMapName, <nl> - }, <nl> - Optional: pointer.BoolPtr(true), <nl> - }, <nl> - }, <nl> - }, <nl> } <nl> + volumes = append(volumes, newRBACConfigVolumes()...) <nl> return &corev1.PodTemplateSpec{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> ", "msg": "move proxy config volume settings to a reusable function"}
{"diff_id": 4985, "repo": "openshift/machine-api-operator", "sha": "a0fa5658defc8a2b51528a4dc5d6ae658fed1ffd", "time": "04.08.2020 15:13:59", "diff": "mmm a / pkg/operator/baremetal_pod.go <nl> ppp b / pkg/operator/baremetal_pod.go <nl>@@ -39,7 +39,7 @@ const ( <nl> htpasswdEnvVar = \"HTTP_BASIC_HTPASSWD\" <nl> ) <nl> -var volumes = []corev1.Volume{ <nl> +var defaultBaremetalVolumes = []corev1.Volume{ <nl> { <nl> Name: baremetalSharedVolume, <nl> VolumeSource: corev1.VolumeSource{ <nl> @@ -319,6 +319,13 @@ func newMetal3PodTemplateSpec(config *OperatorConfig, baremetalProvisioningConfi <nl> }, <nl> } <nl> + volumes := []corev1.Volume{} <nl> + volumes = append(volumes, defaultBaremetalVolumes...) <nl> + // Include the volumes needed by newKubeProxyContainer from <nl> + // sync.go, used to set up the proxy container for metric <nl> + // collection. <nl> + volumes = append(volumes, newRBACConfigVolumes()...) <nl> + <nl> return &corev1.PodTemplateSpec{ <nl> ObjectMeta: metav1.ObjectMeta{ <nl> Labels: map[string]string{ <nl> @@ -399,12 +406,6 @@ func createContainerMetal3BareMetalOperator(config *OperatorConfig, baremetalPro <nl> return corev1.Container{ <nl> Name: \"metal3-baremetal-operator\", <nl> Image: config.BaremetalControllers.BaremetalOperator, <nl> - Ports: []corev1.ContainerPort{ <nl> - { <nl> - Name: \"metrics\", <nl> - ContainerPort: metal3ExposeMetricsPort, <nl> - }, <nl> - }, <nl> Command: []string{\"/baremetal-operator\", <nl> \"-metrics-addr\", metrics.DefaultMetal3MetricsAddress}, <nl> ImagePullPolicy: \"IfNotPresent\", <nl> @@ -447,6 +448,8 @@ func createContainerMetal3BareMetalOperator(config *OperatorConfig, baremetalPro <nl> func newMetal3Containers(config *OperatorConfig, baremetalProvisioningConfig BaremetalProvisioningConfig) []corev1.Container { <nl> containers := []corev1.Container{ <nl> + newKubeProxyContainer(config.Controllers.KubeRBACProxy, \"metrics\", <nl> + metrics.DefaultMetal3MetricsAddress, metal3ExposeMetricsPort), <nl> createContainerMetal3BareMetalOperator(config, baremetalProvisioningConfig), <nl> createContainerMetal3Mariadb(config), <nl> createContainerMetal3Httpd(config, baremetalProvisioningConfig), <nl> ", "msg": "add a proxy container for metal3 metrics\ninclude the necessary volumes and drop the old ContainerPort setting\nthat exposed the metrics directly"}
{"diff_id": 4997, "repo": "openshift/machine-api-operator", "sha": "3c577db57f7e68bf3833b88f8f1bb7ae58089fab", "time": "04.02.2021 11:24:22", "diff": "mmm a / pkg/controller/vsphere/reconciler.go <nl> ppp b / pkg/controller/vsphere/reconciler.go <nl>@@ -475,9 +475,10 @@ func clone(s *machineScope) (string, error) { <nl> vmTemplate, err := s.GetSession().FindVM(*s, \"\", s.providerSpec.Template) <nl> if err != nil { <nl> + const multipleFoundMsg = \"multiple templates found, specify one in config\" <nl> const notFoundMsg = \"template not found, specify valid value\" <nl> defaultError := fmt.Errorf(\"unable to get template %q: %w\", s.providerSpec.Template, err) <nl> - return \"\", handleVSphereError(\"\", notFoundMsg, defaultError, err) <nl> + return \"\", handleVSphereError(multipleFoundMsg, notFoundMsg, defaultError, err) <nl> } <nl> var snapshotRef *types.ManagedObjectReference <nl> ", "msg": "Return a valid error message when multiple templates are found"}
{"diff_id": 5010, "repo": "openshift/machine-api-operator", "sha": "d4efca51e4823445daaa8f8050215b7edfeea196", "time": "28.04.2021 15:10:26", "diff": "mmm a / pkg/controller/machine/controller_test.go <nl> ppp b / pkg/controller/machine/controller_test.go <nl>@@ -43,6 +43,26 @@ var ( <nl> ) <nl> func TestReconcileRequest(t *testing.T) { <nl> + machineNoPhase := machinev1.Machine{ <nl> + TypeMeta: metav1.TypeMeta{ <nl> + Kind: \"Machine\", <nl> + }, <nl> + ObjectMeta: metav1.ObjectMeta{ <nl> + Name: \"emptyPhase\", <nl> + Namespace: \"default\", <nl> + Finalizers: []string{machinev1.MachineFinalizer, metav1.FinalizerDeleteDependents}, <nl> + Labels: map[string]string{ <nl> + machinev1.MachineClusterIDLabel: \"testcluster\", <nl> + }, <nl> + }, <nl> + Spec: machinev1.MachineSpec{ <nl> + ProviderSpec: machinev1.ProviderSpec{ <nl> + Value: &runtime.RawExtension{ <nl> + Raw: []byte(\"{}\"), <nl> + }, <nl> + }, <nl> + }, <nl> + } <nl> machineProvisioning := machinev1.Machine{ <nl> TypeMeta: metav1.TypeMeta{ <nl> Kind: \"Machine\", <nl> @@ -62,6 +82,9 @@ func TestReconcileRequest(t *testing.T) { <nl> }, <nl> }, <nl> }, <nl> + Status: machinev1.MachineStatus{ <nl> + Phase: pointer.StringPtr(phaseProvisioning), <nl> + }, <nl> } <nl> machineProvisioned := machinev1.Machine{ <nl> TypeMeta: metav1.TypeMeta{ <nl> @@ -189,6 +212,19 @@ func TestReconcileRequest(t *testing.T) { <nl> existsValue bool <nl> expected expected <nl> }{ <nl> + { <nl> + request: reconcile.Request{NamespacedName: types.NamespacedName{Name: machineNoPhase.Name, Namespace: machineNoPhase.Namespace}}, <nl> + existsValue: false, <nl> + expected: expected{ <nl> + createCallCount: 0, <nl> + existCallCount: 1, <nl> + updateCallCount: 0, <nl> + deleteCallCount: 0, <nl> + result: reconcile.Result{RequeueAfter: requeueAfter}, <nl> + error: false, <nl> + phase: phaseProvisioning, <nl> + }, <nl> + }, <nl> { <nl> request: reconcile.Request{NamespacedName: types.NamespacedName{Name: machineProvisioning.Name, Namespace: machineProvisioning.Namespace}}, <nl> existsValue: false, <nl> @@ -270,11 +306,13 @@ func TestReconcileRequest(t *testing.T) { <nl> } <nl> for _, tc := range testCases { <nl> + t.Run(tc.request.Name, func(t *testing.T) { <nl> act := newTestActuator() <nl> act.ExistsValue = tc.existsValue <nl> machinev1.AddToScheme(scheme.Scheme) <nl> r := &ReconcileMachine{ <nl> Client: fake.NewFakeClientWithScheme(scheme.Scheme, <nl> + &machineNoPhase, <nl> &machineProvisioning, <nl> &machineProvisioned, <nl> &machineDeleting, <nl> @@ -323,6 +361,7 @@ func TestReconcileRequest(t *testing.T) { <nl> if tc.expected.phase != stringPointerDeref(machine.Status.Phase) { <nl> t.Errorf(\"Case %s. Got: %v, expected: %v\", tc.request.Name, stringPointerDeref(machine.Status.Phase), tc.expected.phase) <nl> } <nl> + }) <nl> } <nl> } <nl> ", "msg": "Update tests to account for ensuring phase is always set"}
